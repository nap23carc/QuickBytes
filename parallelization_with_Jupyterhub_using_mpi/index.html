<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>JupyterHub Parallel Processing with MPI - CARC QuickBytes</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "JupyterHub Parallel Processing with MPI";
        var mkdocs_page_input_path = "parallelization_with_Jupyterhub_using_mpi.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> CARC QuickBytes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../workshop_slides/">Workshop Slides</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Systems</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../systems_information/">Systems Information</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../resource_limits/">Resource Limits</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../export_control/">Export Control</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Linux & HPC Introduction</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../linux_intro/">Linux Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../logging_in/">Logging in</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ssh_keygen_config/">SSH keys and Config file</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../transfer_data/">Transferring data</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../Intro_to_slurm/">Intro to Slurm</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../pbs2slurm/">Converting PBS to Slurm</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../checking_on_running_jobs/">Check running jobs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../module_management/">Managing modules</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../slurm_accounting/">Intro to Slurm accounting at CARC</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../GNU_Parallel/">GNU Parallel</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../X11_forwarding/">X11 Forwarding</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Applications Tutorials</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" >MATLAB</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../running_matlab_jobs/">Running MATLAB jobs at CARC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../ParallelMatlabServer/">Parallel MATLAB Server</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Parallel_MATLAB_profile_setup_and_batch_submission/">Parallel MATLAB batch submission</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Using_GPUs_on_Xena_with_MATLAB/">Using GPUs with MATLAB</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../MATLAB_Deep_Learning_on_Xena/">MATLAB Deep Learning on Xena</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >JupyterHub</a>
    <ul class="current">
                <li class="toctree-l2 current"><a class="reference internal current" href="#">JupyterHub Parallel Processing with MPI</a>
    <ul class="current">
    <li class="toctree-l3"><a class="reference internal" href="#create-a-pbs-profile-on-carc">Create a PBS profile on CARC</a>
    </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Conda_JupyterHub/">Conda python environments for JupyterHub</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../julia_with_jupyterhub/">Using Julia in JupyterHub</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Conda</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../anaconda_general_intro/">General intro to Conda</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../anaconda_intro/">Intro to Conda with example</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../anaconda_pip_channels/">Anaconda pip channels</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >R</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../R_usage/">R Programming in HPC</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://github.com/UNM-CARC/QuickBytes/tree/master/R_at_CARC">R at CARC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Parallel_R_with_Future.ipynb">Running R in Parallel with Future</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Gurobi_optimizer_with_R/">Gurobi optimizer with R</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Machine Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Tensorflow_documentation/">Tensorflow</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../PyTorch_1.9_Xena/">Installing PyTorch on Xena</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../PyTorch_Classifier_Xena.ipynb">Example PyTorch Image Classification on Xena</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../multiGPU_tensorflow_tutorial.ipynb">Tensorflow with multiple GPUs</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../parallel_jupyterhub_with_dask_and_scikit-learn/">Parallelization with JupyterHub using Dask and SciKit-learn</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Bioinformatics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../GATK_QuickByte/">Genomic variant calling with GATK</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../genome_evaluation/">Genome evaluation with QUAST and BUSCO</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../psmc_quickbyte/">Single genome demographic history with PSMC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Stacks_quickbyte/">Stacks for RAD-Seq Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Metabarcoding/">Metabarcoding with QIIME2, Mothur, and USEARCH</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Beast_at_CARC/">BEAST at CARC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../msprime_quickbyte/">Population genetic simulations with msprime (backwards time</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Computational Chemistry</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../orca_wheeler_taos/">Orca on Wheeler and Taos</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../alphafold/">Alphafold</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Computational Immunology</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../SimCov/">SimCov on Wheeler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Astronomy</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../mpiCASA/">CASA Radio Astronomy</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Paraview</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../paraview/">Paraview Wheeler</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Paraview_Hopper/">Paraview Hopper</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../singularity-markdown-version/">Docker and Singularity</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../haskell/">Haskell at CARC</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../spark/">Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../install_perl_libraries/">Installing Perl Libraries to Your Home Directory</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">CARC QuickBytes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Applications Tutorials</li>
          <li class="breadcrumb-item">JupyterHub</li>
      <li class="breadcrumb-item active">JupyterHub Parallel Processing with MPI</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="parallelization-with-jupyterhub-using-mpi">Parallelization with JupyterHub using MPI</h2>
<p>The following steps will show you the steps to use MPI through ipython's ipyparallel interface. </p>
<h3 id="create-a-pbs-profile-on-carc">Create a PBS profile on CARC</h3>
<p>Once you are logged in at carc run these steps:</p>
<pre><code class="language-console">cd /projects/systems/shared/ipython_cluster_profiles
cp -r profile_pbs ~/.ipython/
</code></pre>
<p>Then check the files copied. </p>
<pre><code class="language-console">cd ~/.ipython/profile_pbs
</code></pre>
<p>Now on JupyterHub go to the IPython Clusters tab (refresh if already open) and you should see a pbs profile now available to you. Click the JupyterHub icon in the upper left of your screen if you can't see the clusters tab.</p>
<p>You can start a job by setting number of engine in the 'pbs' cluster profile and clicking start under actions. For this example we will request 8 ipython compute engines.</p>
<p>[Optional] Since ipython's ipyparallel system is requesting compute nodes through the torque PBS system you will have to wait until the nodes are running before you can run 
code on them. Check that the job is running in terminal with </p>
<pre><code class="language-console">watch qstat -tn -u &lt;username&gt;


You should see something like the following:

Every 2.0s: qstat -t -n -u $USER     Wed Oct 23 09:15:14 2019                                                                                      
wheeler-sn.alliance.unm.edu:
                                                                                  Req'd       Req'd   Elap
Job ID                  Username    Queue    Jobname          SessID  NDS   TSK   Memory      Time    S   Time
----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - ---------
258370.wheeler-sn.alli  mfricke     default  jupyterhub        21730     1  1   --   08:00:00 R  00:06:45
   wheeler291/1
258371.wheeler-sn.alli  mfricke     default  ipython_controll  22553     1  1   --   01:00:00 R  00:06:11
   wheeler291/2
258372.wheeler-sn.alli  mfricke     default  ipython_engine 3213     2     16   --   01:00:00 R  00:06:11
   wheeler176/0-7+wheeler175/0-7
</code></pre>
<p>Notice the ipython engines are running with status 'R'. You can also check to see whether the compute engines are ready in your python notebook (see below).</p>
<p>To exit the watch command use control-C </p>
<p>To change the walltime of your profile, in the ~/.ipython/profile_pbs directory edit the pbs.engine.template and the pbs.controller.template to fit the requirments for your job. By editing these files you can also change from the default to debug queue as you are testing your program. </p>
<p>Now you can open a Jupyter notebook and follow the remainder of this tutorial.</p>
<h2 id="creating-an-example-function-that-uses-mpi">Creating an example function that uses MPI</h2>
<p>Create a new file in your home directory and name it psum.py. Enter the following into psum.py and save the file.</p>
<pre><code class="language-python">from mpi4py import MPI
import numpy as np

def psum(a):
    locsum = np.sum(a)
    rcvBuf = np.array(0.0,'d')
    MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE],
        [rcvBuf, MPI.DOUBLE],
        op=MPI.SUM)
    return rcvBuf
</code></pre>
<p>This function performs a distributed sum over all the nodes on the MPI communications group.</p>
<h2 id="create-a-jupyter-notebook-to-call-our-mpi-function">Create a Jupyter Notebook to Call Our MPI Function</h2>
<p>Create a new Python 3 notebook in Jupyterhub and name it mpi_test.ipynb. Enter the following into cells of your notebook. Many of the commands are run on the MPI cluster and so are asynchronous. To check whether an operation has completed we check the status with ".wait_interactive()". When the status reports "done" you can move on to the next step.</p>
<h2 id="load-required-packages-for-ipyparallel-and-mpi">Load required packages for ipyparallel and MPI</h2>
<pre><code class="language-python">import ipyparallel as ipp
from mpi4py import MPI
import numpy as np
</code></pre>
<h2 id="create-a-cluster-to-use-the-cpus-allocated-thrugh-pbs">Create a cluster to use the CPUs allocated thrugh PBS</h2>
<pre><code class="language-python">cluster = ipp.Client(profile='pbs')
</code></pre>
<h2 id="check-if-the-cluster-is-ready-we-are-looking-for-8-ids-since-we-asked-for-8-engines">Check if the cluster is ready. We are looking for 8 ids since we asked for 8 engines.</h2>
<p>Engines in ipparallel parlence are the same as processes or workers in other parallel systems.</p>
<pre><code class="language-python">cluster.ids
</code></pre>
<p>[0, 1, 2, 3, 4, 5, 6, 7]</p>
<pre><code class="language-python">len(cluster[:])
</code></pre>
<p>8</p>
<h2 id="assign-the-engines-to-a-variable-named-view-to-allow-us-to-interact-with-them">Assign the engines to a variable named "view" to allow us to interact with them</h2>
<pre><code class="language-python">view = cluster[:]
</code></pre>
<p>Enable ipython `magics´. These are ipython helper functions such as %</p>
<pre><code class="language-python">view.activate()
</code></pre>
<h2 id="check-to-see-if-the-mpi-communication-world-is-of-the-expected-size-it-should-be-size-8-since-we-have-8-engines">Check to see if the MPI communication world is of the expected size. It should be size 8 since we have 8 engines.</h2>
<p>Note we are running the Get_size command on each engine to make sure they all see the same MPI comm world. %px simply executes the code following it on each compute engine in parallel.</p>
<pre><code class="language-python">status_mpi_size=%px size = MPI.COMM_WORLD.Get_size()
</code></pre>
<pre><code class="language-python">status_mpi_size.wait_interactive()
</code></pre>
<p>done</p>
<p>The output of viewing the size variable should be an array with the same number of entries as engines, and each entry should be the number of engines requested.</p>
<pre><code class="language-python">view['size']
</code></pre>
<p>[8, 8, 8, 8, 8, 8, 8, 8]</p>
<h2 id="run-the-external-python-code-in-psumpy-on-all-the-engines">Run the external python code in ´psum.py´ on all the engines.</h2>
<p>Recall that psum.py just loads the MPI libraries and defines the distributed sum function, psum. We are not actually calling the psum function yet.</p>
<pre><code class="language-python">status_psum_run=view.run('psum.py')
</code></pre>
<pre><code class="language-python">status_psum_run.wait_interactive()
</code></pre>
<p>done</p>
<h2 id="send-data-to-all-nodes-to-by-summed">Send data to all nodes to by summed</h2>
<p>The scatter command sends 32 values from 0 to 31 to the 8 compute engines. Each compute engine gets 32/8=4 values. This is the ipyparallel scatter command, not an MPI scatter command.</p>
<pre><code class="language-python">status_scatter=view.scatter('a',np.arange(32,dtype='float'))
</code></pre>
<p>done</p>
<p>We can view the variable 'a' on all the compute engines. The value of 'a' for each compute engine is an element of the return array. In this case each value is itself an array.</p>
<pre><code class="language-python">view['a']
</code></pre>
<p>[array([0., 1., 2., 3.]),
     array([4., 5., 6., 7.]),
     array([ 8.,  9., 10., 11.]),
     array([12., 13., 14., 15.]),
     array([16., 17., 18., 19.]),
     array([20., 21., 22., 23.]),
     array([24., 25., 26., 27.]),
     array([28., 29., 30., 31.])]</p>
<h2 id="execute-the-psum-function-on-all-the-compute-engines-and-store-the-result-in-totalsum">Execute the psum function on all the compute engines and store the result in totalsum</h2>
<p>MPI code has to be executed on each compute engine so they can each perform the MPI reduce. This is accomplished by running calling the psum function on all the compute engines simultaniosly. MPI will allow them to communicate with each other to calculate the sum. </p>
<pre><code class="language-python">status_psum_call=%px totalsum = psum(a)
</code></pre>
<pre><code class="language-python">status_psum_call.wait_interactive()
</code></pre>
<p>done</p>
<h2 id="check-the-value-of-totalsum-on-each-node">Check the value of totalsum on each node</h2>
<p>Total should be equal to 31(31+1)/2=496</p>
<pre><code class="language-python">view['totalsum']
</code></pre>
<p>[array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.)]</p>
<p>Each compute engine calculated the sum of all the values. Since we ran this MPI function on all the compute engines they report the same value.</p>
<h2 id="defining-functions-in-the-notebook">Defining functions in the notebook</h2>
<p>Rather than loading psum from file we can define it in the notebook using the ipython function decorator '@'. </p>
<pre><code class="language-python">@view.remote(block = True)
def inlinesum():
    from mpi4py import MPI
    import numpy as np
    locsum = np.sum(a)
    rcvBuf = np.array(0.0,'d')
    MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE],
        [rcvBuf, MPI.DOUBLE],
        op=MPI.SUM)    
    return rcvBuf
</code></pre>
<p>Now we can call inlinesum and it is automatically run on every compute engine. The call is through ipyparallels but the computation is still using MPI.</p>
<pre><code class="language-python">inlinesum()
</code></pre>
<p>[array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.),
     array(496.)]</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../MATLAB_Deep_Learning_on_Xena/" class="btn btn-neutral float-left" title="MATLAB Deep Learning on Xena"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Conda_JupyterHub/" class="btn btn-neutral float-right" title="Conda python environments for JupyterHub">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../MATLAB_Deep_Learning_on_Xena/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Conda_JupyterHub/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
