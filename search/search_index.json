{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"QuickBytes Quickbytes are tutorials specifically designed to assist CARC users in navigating and utilizing our resources effectively. These tutorials aim to provide practical guidance on a variety of topics related to advanced research computing, making it easier for users to enhance their skills and workflows. In addition to the Quickbytes tutorials, this site also hosts workshop slides and other educational materials from training sessions offered by CARC. These resources are tailored to address common challenges and promote best practices in research computing. To explore the materials, please make use of the navigation menu or the search bar to locate a topic of interest. Whether you\u2019re looking for step-by-step instructions, troubleshooting tips, or advanced technical insights, you\u2019ll find valuable resources here. It\u2019s important to note that maintaining up-to-date documentation for an active research center is an ongoing effort. We continuously revise and expand our resources to reflect the latest developments and user needs. If you encounter any issues, such as broken links, outdated information, or functionality problems, please let us know. You can report these issues by sending an email to help@carc.unm.edu.","title":"Home"},{"location":"#quickbytes","text":"Quickbytes are tutorials specifically designed to assist CARC users in navigating and utilizing our resources effectively. These tutorials aim to provide practical guidance on a variety of topics related to advanced research computing, making it easier for users to enhance their skills and workflows. In addition to the Quickbytes tutorials, this site also hosts workshop slides and other educational materials from training sessions offered by CARC. These resources are tailored to address common challenges and promote best practices in research computing. To explore the materials, please make use of the navigation menu or the search bar to locate a topic of interest. Whether you\u2019re looking for step-by-step instructions, troubleshooting tips, or advanced technical insights, you\u2019ll find valuable resources here. It\u2019s important to note that maintaining up-to-date documentation for an active research center is an ongoing effort. We continuously revise and expand our resources to reflect the latest developments and user needs. If you encounter any issues, such as broken links, outdated information, or functionality problems, please let us know. You can report these issues by sending an email to help@carc.unm.edu.","title":"QuickBytes"},{"location":"Beast_at_CARC/","text":"BEAST at CARC Bayesian Evolutionary Analysis by Sampling Trees (BEAST) is a software package that performs phylogenetic tree analysis with user specified molecular clock models using the widely popular Bayesian Markov chain Monte Carlo (MCMC) methods. BEAST has its origins in modeling pathogen evolution in near real time but is also popular for other phylogenetic applications. BEAST is a well documented and flexible tool for modeling phylogenetics. Using BEAST at CARC offers more power for rigorous computations. Generating BEAST imput files: BEAUti BEAST uses .xml files which contain sequences and model parameters. Because BEAST is capable of incorporating a diverse range of meta data and specific time modeling parameters, the graphical user interface BEAUTi allows users to upload nexus files and create .xml files with ease. Make sure that the version of beast in the module you load matches the version of BEAUTi used to generate the .xml files. Running BEAST on Wheeler Once a .xml file is generated, beast can be easily run on CARC. An example .pbs script is as follows: #!/bin/bash #PBS -q default #PBS -N BEASTjob #PBS -l nodes=1:ppn=8 #PBS -l walltime=24:00:00 #PBS -j oe cd $PBS_O_WORKDIR module load beast2-2.5.2-intel-19.0.4-hcnoysj beast my_data.xml The output should be a job log (joined with any potential error file), and a .tree file for your downstream analysis. For more assistance with BEAST at CARC please email help@carc.unm.edu.","title":"BEAST at CARC"},{"location":"Beast_at_CARC/#beast-at-carc","text":"Bayesian Evolutionary Analysis by Sampling Trees (BEAST) is a software package that performs phylogenetic tree analysis with user specified molecular clock models using the widely popular Bayesian Markov chain Monte Carlo (MCMC) methods. BEAST has its origins in modeling pathogen evolution in near real time but is also popular for other phylogenetic applications. BEAST is a well documented and flexible tool for modeling phylogenetics. Using BEAST at CARC offers more power for rigorous computations.","title":"BEAST at CARC"},{"location":"Beast_at_CARC/#generating-beast-imput-files-beauti","text":"BEAST uses .xml files which contain sequences and model parameters. Because BEAST is capable of incorporating a diverse range of meta data and specific time modeling parameters, the graphical user interface BEAUTi allows users to upload nexus files and create .xml files with ease. Make sure that the version of beast in the module you load matches the version of BEAUTi used to generate the .xml files.","title":"Generating BEAST imput files: BEAUti"},{"location":"Beast_at_CARC/#running-beast-on-wheeler","text":"Once a .xml file is generated, beast can be easily run on CARC. An example .pbs script is as follows: #!/bin/bash #PBS -q default #PBS -N BEASTjob #PBS -l nodes=1:ppn=8 #PBS -l walltime=24:00:00 #PBS -j oe cd $PBS_O_WORKDIR module load beast2-2.5.2-intel-19.0.4-hcnoysj beast my_data.xml The output should be a job log (joined with any potential error file), and a .tree file for your downstream analysis. For more assistance with BEAST at CARC please email help@carc.unm.edu.","title":"Running BEAST on Wheeler"},{"location":"Conda_JupyterHub/","text":"Using Custom Conda Python Environments in JupyterHub Custom environments created by users can also be used on JupyterHub. This QuickByte shows you how to make python environments accessible on JupyterHub. In Terminal If you are creating a conda environment from scrach that you know you will want to use on JupyterHub, as you are creating the environment add the ipykernel to the packages you want included. For example, if you were making a natural language processing libraries environment, you could create an environment like this: module load miniconda3 conda create -n nltk nltk ipykernel Alternatively, if you already have an environment created and would like it to be available on JupyterHub, then add the ipykernal. source activate nltk conda install ipykernel Remember that you can also access the terminal though JupyterHub. To do this click Terminal under the New dropdown menu. On JupyterHub After your environments have been modified to include the ipykernal, you can open notebooks on JupyterHub by opening a New Notebook under File and selecting the environment.","title":"Conda python environments for JupyterHub"},{"location":"Conda_JupyterHub/#using-custom-conda-python-environments-in-jupyterhub","text":"Custom environments created by users can also be used on JupyterHub. This QuickByte shows you how to make python environments accessible on JupyterHub.","title":"Using Custom Conda Python Environments in JupyterHub"},{"location":"Conda_JupyterHub/#in-terminal","text":"If you are creating a conda environment from scrach that you know you will want to use on JupyterHub, as you are creating the environment add the ipykernel to the packages you want included. For example, if you were making a natural language processing libraries environment, you could create an environment like this: module load miniconda3 conda create -n nltk nltk ipykernel Alternatively, if you already have an environment created and would like it to be available on JupyterHub, then add the ipykernal. source activate nltk conda install ipykernel Remember that you can also access the terminal though JupyterHub. To do this click Terminal under the New dropdown menu.","title":"In Terminal"},{"location":"Conda_JupyterHub/#on-jupyterhub","text":"After your environments have been modified to include the ipykernal, you can open notebooks on JupyterHub by opening a New Notebook under File and selecting the environment.","title":"On JupyterHub"},{"location":"GATK_QuickByte/","text":"Parallel genomic variant calling with Genome Analysis Toolkit (GATK) This QuickByte outlines how to run a pipeline based on Genome Analysis Toolkit v4 (GATK4) best practices, a common pipeline for processing genomic data from Illumina platforms. Major modifications from \u201ctrue\u201d best practices are done to facilitate using this pipeline for both model and non-model organisms. Additionally, we show how to best parallelize these steps on CARC. Here we outline the steps for a single sample without parallelization, then with parallelization for specific steps, and finally provide an example of a fully parallelized script. Extensive documentation (including other Best Practices pipelines) can be found here . Specifically, the Best Practices informing this pipeline are the data pre-processing workflow and the germline short variant discovery workflow . We aim to give you sample commands to emulate these scripts workflows, which will also allow you to easily modify the pipeline. The goal of this pipeline is to output Single Nucleotide Polymorphisms (SNPs) and optionally indels for a given dataset. This same pipeline can be used for humans, model organisms, and non-model organisms. Spots that can leverage information from model organisms are noted, but those steps can be bypassed. Because sample size and depth of coverage are often lower in non-model organisms, filtering recommendations and memory requirements will vary. Note that this assumes you are using paired-end data and will differ slightly if you use unpaired. The basic steps are aligning and processing raw reads into binary alignment map (BAM) files, optionally getting descriptive metrics about the samples\u2019 sequencing and alignment, calling variants to produce genomic variant call format (GVCF) files, genotyping those GVCFs to produce VCFs, and filtering those variants for analysis. For CARC users, we have provided some test data to run this on from a paper on the conservation genomics of sagegrouse . It is two sets of gzipped fastq files per species (i.e. eight total, 4 read and 4 read 2), a file with adapter sequences to trim, and a reference genome. They are located at /projects/shared/tutorials/GATK/. Copy them into your space like \"cp /projects/shared/tutorials/quickbytes/GATK/* ~/path/to/directory\". A .pbs script for running the pipeline (seen below) is also included, but you may learn more by running each step individually. The whole process with the script with 4 nodes on wheeler takes about 5.5 hours. If you run this script, note that it should output a filtered VCF file that's ~350 Mb. Please note that you must cite any program you use in a paper. At the end of this, we have provided citations you would include for the programs we ran here. Table of Contents Preliminaries The Pipeline Trimming reads Alignment and pre-processing Summary statistics Calling Variants Consolidating and genotyping Selecting variants Scatter-gather parallel Sample Scatter-gather PBS script Trobuleshooting Citations Preliminary stuff Module and directories We will be using conda to make an environment to load within our PBS script. First, if you haven\u2019t already, set up conda as follows: module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik # can also use other conda modules conda init bash <exit and re-log or restart shell> This following line will create an environment and install the most recent versions of what we need. We assume you run this before starting up your job. conda create -n gatk-env -c bioconda -c conda-forge gatk4 bwa samtools picard trimmomatic Alternatively, you can load these as modules if you are on Wheeler (Xena only has Samtools now), but they may not be the most recent versions: module load bwa-0.7.17-intel-18.0.2-7jvpfu2 module load samtools-1.9-gcc-7.3.0-tnzvvzt module load picard-2.20.8-gcc-4.8.5-3yh2dzv module load gatk-4.1.4.1-gcc-4.8.5-python3-fqiavji module load trimmomatic-0.36-gcc-4.8.5-q3gx4rj If you are parallelizing (see \u201cScatter-gather Parallel\u201d and sample PBS script), you'll need this: module load parallel-20170322-gcc-4.8.5-2ycpx7e source $(which env_parallel.bash) The directories we will need (other than the home directory) are a raw_reads directory for the demultiplexed reads and the following for various intermediate files to go into. Alternatively, if you don\u2019t want to move around all your reads, just replace the path in the BWA call with that path. Note that a few of these are only used with scatter-gather parallelization (reccomended for larger datasets). mkdir clean_reads mkdir alignments # next three are only if you get optional metrics mkdir alignments/alignment_summary mkdir alignments/insert_metrics mkdir alignments/depth mkdir alignments/dedup_temp mkdir bams mkdir gvcfs mkdir combined_vcfs mkdir analysis_vcfs # scatter-gather only mkdir combined_vcfs/intervals mkdir gvcfs/combined_intervals We will be using a few variables throughout this that we can set now. These are shortcuts for the path to our working directory and reference. src=$PBS_O_WORKDIR reference=$src/reference Sample Names To keep our script short, and outputs easy to understand, we will use consistent sample names for each step, and keep the sample names in a file. We assume this file is named \u201csample_list\u201d. The file should have one sample name per line. with a single blank line at the end. The one for the tutorial dataset looks like: GRSG_JHWY140 GRSG_JHWY142 GUSG_GGS1 GUSG_GGS2 We will use this sample list in two ways. The first way is loops, and second is GNU parallel. You can see some examples in the PBS script at the end of the document. Here's a basic demonstration of how to us the list in a loop: while read sample; do RunTask -input ${sample} done < $src/sample_list And this is what GNU parallel looks like (note it's different for BWA, as we need to specify a specific number of jobs). Remember, we need to use env_parallel if we are using conda. cat $src/sample_list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'RunTask -input {}.file' For clarity, in most cases the commands are written as they would be for a for loop (i.e. with $sample instead of {}). Demultiplexing Because it is not covered by best practices, and is often done by the sequencing center, we will not go into the details of demultiplexing here. We recommend you use Illumina\u2019s software bcl2fastq if you have the data in .bcl format, and saber if it has already been converted to fastq format and it does not have dual combinatorial barcodes. We'll assume these reads will be in the raw_reads folder with the name SAMPLE_1.fastq.gz (or 2 for read 2). The Pipeline Trimming Reads Although not a part of GATK's best practices, it is common practice to trim your reads before using them in analyses. We'll use trimmomatic for this. Trimmomatic performs very poorly with its internal thread command, so we'll use GNU parallel to run it in the final script. Note that trimmomatic doesn't have many command line flags, so we'll name variables ahead of time to keep them straight: # note this assumes the provided fasta file is in your working directory. adapters=$src/TruSeq3-PE.fa read1=$src/raw_reads/${sample}_1.fastq.gz read2=$src/raw_reads/${sample}_2.fastq.gz paired_r1=$src/clean_reads/${sample}_paired_R1.fastq.gz paired_r2=$src/clean_reads/${sample}_paired_R2.fastq.gz unpaired_r1=$src/clean_reads/${sample}_unpaired_R1.fastq.gz unpaired_r2=$src/clean_reads/${sample}_unpaired_R2.fastq.gz # the minimum read length accepted, we do the liberal 30bp here min_length=30 trimmomatic PE -threads 1 \\ $read1 $read2 $paired_r1 $unpaired_r1 $paired_r2 $unpaired_r2 \\ ILLUMINACLIP:${adapters}:2:30:10:2:True \\ LEADING:3 TRAILING:3 MINLEN:${min_length} If you don't have access to the CARC directory with the adapters file, it can be found in the conda install/spack package. The exact path will vary, but they'll be something like this: # spack adapters=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-4.8.5/trimmomatic-0.36-q3gx4rjeruluf75uhcdfkjoaujqnjhzf/bin/TruSeq3-SE.fa # conda adapters=~/.conda/pkgs/trimmomatic-0.39-1/share/trimmomatic-0.39-1/adapters/TruSeq3-PE.fa If you're using the spack module, you call trimmomatic using java: java -jar /opt/spack/opt/spack/linux-centos7-x86_64/gcc-4.8.5/trimmomatic-0.36-q3gx4rjeruluf75uhcdfkjoaujqnjhzf/bin/trimmomatic-0.36.jar PE ... Alignment and Pre-processing This section prepares BAM files for variant calling. First, we need to index our reference and make a sequence dictionary. We'll index two ways, one for bwa and one for GATK: bwa index -p $reference ${reference}.fa samtools faidx ${reference}.fa -o ${reference}.fa.fai picard CreateSequenceDictionary \\ R=${reference}.fa \\ O=${reference}.dict Then, we need to align demultiplexed reads to a reference. For this step, we will use the Burrough-Wheeler Aligner\u2019s (BWA) mem algorithm. Another common option is Bowtie . One important flag here is the -R flag, which is the read group and sample ID for a given sample. We assume that these samples are in the same read group. We can get a node's worth of parallelization with the -t command (it can't work across nodes). Therefore, in the sample script at the end we will show you how to further parallelize BWA. The base command looks like this: bwa mem \\ -t [# threads] -M \\ -R \"@RG\\tID:${sample}\\tPL:ILLUMINA\\tLB:${sample}\\tSM:${sample}\" \\ $reference \\ $src/clean_reads/${sample}_paired_R1.fastq.gz \\ $src/clean_reads/${sample}_paired_R2.fastq.gz \\ > $src/alignments/${sample}.sam The next step is to mark PCR duplicates to remove bias, sort the file, and convert it to the smaller BAM format for downstream use. GATK\u2019s new MarkDuplicatesSpark performs all these tasks, but needs a temporary directory to store intermediate files. Note that although we aren't formally using Spark for parallelization, the line \" --conf 'spark.executor.cores=8'\" still speed it up, and makes (change the number of cores if the cores per node are higher than 8): gatk MarkDuplicatesSpark \\ -I $src/alignments/${sample}.sam \\ -M $src/bams/${sample}_dedup_metrics.txt \\ --tmp-dir $src/alignments/dedup_temp \\ -O $src/bams/${sample}_dedup.bam \\ --conf \"spark.executor.cores=8\" We recommend combining these steps per sample for efficiency and smoother troubleshooting. One issue is that we do not want large SAM files piling up. This can either be done by piping BWA output directly to MarkDuplicatesSpark or removing the SAM file after each loop. In case you want to save the SAM files, we did the latter (this isn\u2019t a bad idea if you have the space, in case there is a problem with generating BAM files). If you are doing base recalibration, you can also add \u201crm ${sample}_debup.bam\u201d to get rid of needless BAM files. Later in the pipeline, we assume you did base recalibration, so will use the {sample}_recal.bam file. If you did not use base recalibration, use {sample}_dedup.bam file in its place. Base Quality Score Recalibration (model organisms) An optional step (and one not taken in the tutorial) is to recalibrate base call scores. This applies machine learning to find where quality scores are over or underestimated based on things like read group and cycle number of a given base. This is recommended, but is rarely possible for non-model organisms, as a file of known polymorphisms is needed. Note, however, that it can take a strongly filtered VCF from the end of the pipeline, before running the entire pipeline again (but others haven\u2019t found much success with this method ). Here is how it looks, with the first line indexing the input VCF file if you haven't already. gatk IndexFeatureFile -I $src[name-of-known-sites].vcf gatk BaseRecalibrator \\ -I $src/bams/${sample}_dedup.bam \\ -R ${reference}.fa \\ --known-sites $src/[name-of-known-sites].vcf \\ -O $src/bams/${sample}_recal_data.table gatk ApplyBQSR \\ -I $src/bams/${sample}_dedup.bam \\ -R ${reference}.fa \\ --bqsr-recal-file $src/bams/${sample}_recal_data.table \\ -O $src/bams/${sample}_recal.bam Collect alignment and summary statistics (optional) This step is optional, and is not part of GATK\u2019s best practices, but is good to have. It will output important stats for assessing sample quality. Picard\u2019s \u201cCollectAlignmentSummaryMetrics\u201d gives several helpful statistics about the alignment for a given sample. Picard\u2019s \u201cCollectInsertSizeMetrics\u201d gives information about the distribution of insert sizes. Samtools\u2019s \u201cdepth\u201d gives information about the read depth of the sample. Note that \"depth\" has huge output files, so it may be best to skip it until needed. picard CollectAlignmentSummaryMetrics \\ R=${reference}.fa \\ I=$src/bams/${sample}_recal.bam \\ O=$src/alignments/alignment_summary/${sample}_alignment_summary.txt picard CollectInsertSizeMetrics \\ INPUT=$src/bams/${sample}_recal.bam \\ OUTPUT=$src/alignments/insert_metrics/${sample}_insert_size.txt \\ HISTOGRAM_FILE=$src/${sample}_insert_hist.pdf samtools depth \\ -a $src/bams/${sample}_recal.bam \\ > $src/alignments/depth/${sample}_depth.txt Calling variants with HaplotypeCaller The simplest way is individually going through BAM files and calling SNPs on them using HaplotypeCaller in GVCF mode (\u201c-ERC GVCF\u201d flag), resulting in GVCFs as output. gatk HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/${sample}_recal.bam \\ -O $src/gvcfs/${sample}_raw.g.vcf.gz \\ -ERC GVCF One issue with HaplotypeCaller is that it takes a long time, but is not programmed to be parallelized by default. We can use GNU parallel to solve that problem in two ways. If you have many small inputs and don't want to do scatter-gather parallel, you can run one instance of HaplotypeCaller per core. Note that we restrict the memory such that each job can only max out the core it's on (you'll want to change from 6g based on the machine you're running this on): cat $src/sample_list | env_parallel --sshloginfline $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/{}_recal.bam \\ -O $src/gvcfs/${}_raw.g.vcf.gz \\ -ERC GVCF' If you are dealing with large files, HaplotypeCaller may take longer than your walltime. The Scatter-gather Parallel section will outline how to fix that by breaking the job (and the next section) into multiple intervals. Consolidating and Genotyping This next step has two options, GenomicsDBImport and CombineGVCFs. GATK recommends GenomicsDBImport, as it is more efficient for large datasets, but it performs poorly on references with many contigs. CombineGVCFs can take a long time for large datasets, but is easier to use. Note that GenomicsDBImport must have intervals (generally corresponding to contigs or chromosomes) specified. GenomicsDBImport can take a file specifying GVCFs, but because CombineGVCFs cannot take this input, we just make a list of samples to combine programmatically and plug it in. Here is how we generate that command: gvcf_names=\"\" while read sample; do gvcf_names=\"${gvcf_names}-V ${src}/gvcfs/${sample}_raw.g.vcf.gz \" done < $src/sample_list If you do use GenomicsDBImport, or want to genotype contigs/chromosomes independently, we'll need intervals for it to work with (the same used for scatter-gather parallelization). Also, you'll need to pre-make a temp directory for holding files: mkdir gendb_temp cut -f 1 ${reference}.fa.fai > $src/intervals.list For GenomicsDBImport, you'll need to get rid of the directory you use for the database (here genomic_database) if you already made it: gatk GenomicsDBImport \\ ${gvcf_names} \\ --genomicsdb-workspace-path $src/genomic_database \\ --tmp-dir $src/gendb_temp \\ -L $src/intervals.list And an example for CombineGVCFs is: gatk CombineGVCFs \\ -R ${reference}.fa \\ ${gvcf_names} \\ -O $src/combined_vcfs/combined_gvcf.g.vcf.gz The next step is to genotype the combined (cohort) GVCF file. Here\u2019s a sample command for GenomicsDBImport: gatk GenotypeGVCFs \\ -R ${reference}.fa \\ -V gendb://$src/genomic_database \\ -O $src/combined_vcfs/combined_vcf.vcf.gz And one for CombineGVCFs: gatk GenotypeGVCFs \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_gvcf.g.vcf.gz \\ -O $src/combined_vcfs/combined_vcf.vcf.gz Selecting and filtering variants This first step is optional, but here we separate out indels and SNPs. Note that we don\u2019t use indels down the line, but similar filters can be applied. gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-yype SNP \\ -O $src/combined_vcfs/raw_snps.vcf.gz gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-type INDEL \\ -O $src/combined_vcfs/raw_indel.vcf.gz Here are some good sample filters. The \u201cDP_filter\u201d is depth of coverage (you will probably want to change this), \u201cQ_filter\u201d is quality score, \u201cQD_filter\u201d is quality by depth (avoids artificial inflation of calls), \"MQ_filter\" is a mapping quality filter, and \u201cFS_filter\u201d is a strand bias filter (higher value means higher bias). Note that DP is better for low depth samples, while QD is better for high depth. More info can be found on GATK\u2019s website . gatk VariantFiltration \\ -R ${reference}.fa \\ -V $src/combined_vcfs/raw_snps.vcf.gz \\ -O $src/analysis_vcfs/filtered_snps.vcf \\ -filter \"DP < 4\" --filter-name \"DP_filter\" \\ -filter \"QUAL < 30.0\" --filter-name \"Q_filter\" \\ -filter \"QD < 2.0\" --filter-name \"QD_filter\" \\ -filter \"FS > 60.0\" --filter-name \"FS_filter\" \\ -filter \"MQ < 40.0\" --filter-name \"MQ_filter\" This will give us our final VCF! Note that the filtered SNPs are still included, just with a filter tag. You can use something like SelectVariants' \"exclude-filtered\" flag or VCFtools\u2019 \u201c--remove-filtered-all\u201d flag to get rid of them. Scatter-gather Parallel Scatter-gather is the process of breaking a job into intervals (i.e. contigs or scaffolds in a reference) and running HaplotypeCaller, CombineGVCFs, and GenotypeGVCFs on each interval in parallel. Then, at the end, all the invervals are gathered together with GatherGVCFs. This results in a massive speed-up due to the parallelization. This is fully implemented in the sample script below, with each step outlined here. The output of GatherVcfs is the same as what comes from GenotypeGVCFs in the non-parallel version. Here is how we run HaplotypeCaller, note that this is only one sample, see the sample script for running this on all samples: # make our interval list cut -f 1 ${reference}.fa.fai > $src/intervals.list while read sample; do mkdir ${src}/gvcfs/${sample} cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/${sample}_recal.bam \\ -O $src/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \\ -L {} \\ -ERC GVCF' done < $src/sample_list You'll run then run CombineGVCFs. For each interval, you'll make a list of GVCF file paths for each sample you're including (the while loop below). cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'interval_list=\"\" # loop to generate list of sample-specific intervals to combine while read sample; do interval_list=\"${interval_list}-V ${src}/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \" done < $src/sample_list gatk --java-options \"-Xmx6g\" CombineGVCFs \\ -R ${reference}.fa \\ ${interval_list} \\ -O $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz' Next, you run GenotypeGVCFs to get VCFs to gather afterwards. No fancy lists needed! cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" GenotypeGVCFs \\ -R ${reference}.fa \\ -V $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz \\ -O $src/combined_vcfs/intervals/{}_genotyped.vcf.gz' If you have many samples, it may be best to use GenomicsDBImport. It is very similar, with both that step and the genotyping below. Note that the directory for --genomicsdb-workspace-path can't exist (unless you're updating it): cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'mkdir $src/gendb_temp/{} interval_list=\"\" # loop to generate list of sample-specific intervals while read sample; do interval_list=\"${interval_list}-V ${src}/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \" done < $src/sample_list # run make the genomics databases gatk --java-options \"-Xmx6g\" GenomicsDBImport \\ ${interval_list} \\ --genomicsdb-workspace-path $src/genomics_databases/{} \\ --tmp-dir $src/gendb_temp/{} \\ -L {}' cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" GenotypeGVCFs \\ -R ${reference}.fa \\ -V gendb://$src/genomics_databases/{} \\ -O $src/combined_vcfs/intervals/{}_genotyped.vcf.gz' The final (gather) step uses GatherVcfs, for which we'll make a file containing the paths to all input genotyped VCFs (generated in the while loop). Note the first line initializes a blank file for the gather list. After we make the gathered VCF, we need to index it for future analyses. > $src/combined_vcfs/gather_list while read interval; do echo ${src}/combined_vcfs/intervals/${interval}_genotyped.vcf.gz >> \\ $src/combined_vcfs/gather_list done < $src/chromosomes.list gatk GatherVcfs \\ -I $src/combined_vcfs/gather_list \\ -O combined_vcfs/combined_vcf.vcf.gz gatk IndexFeatureFile \\ -I $src/combined_vcfs/raw_snps.vcf.gz NOTE THAT THIS FILE STILL NEEDS TO HAVE VARIANTS SELECTED AND FILTERED, SEE \"Selecting and filtering variants\" ABOVE Sample PBS Script Here is a sample PBS script combining everything we have above, with as much parallelization as possible. One reason to break up steps like we did is for improved checkpointing (without having to write code checking if files are already present). Once you are finished running a block of code, you can just comment it out. Similarly, if you can only get part way through your sample list, you can copy it and remove samples that have already completed a given step. To convert this to Slurm, replace $PBS_O_WORKDIR with $SLURM_SUBMIT_DIR and refer to this conversion guide for the rest. #!/bin/bash #PBS -q default #PBS -l nodes=4:ppn=8 #PBS -l walltime=10:00:00 #PBS -N gatk_tutorial #PBS -m ae #PBS -M youremail@school.edu # the PBS lines are for the default queue, using 4 nodes, and has a conservative 10 hour wall time # it is named \"gatk_tutorial\" and sends an email to \"youremail@school.edu\" when done # load your conda environment module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik eval \"$(conda shell.bash hook)\" conda activate gatk-env # load GNU parallel, get env_parallel module load parallel-20170322-gcc-4.8.5-2ycpx7e source $(which env_parallel.bash) src=$PBS_O_WORKDIR # this is \"sagegrouse_reference\" in the tutorial reference=${src}/reference # indexing reference bwa index -p $reference ${reference}.fa samtools faidx ${reference}.fa -o ${reference}.fa.fai picard CreateSequenceDictionary \\ R=${reference}.fa \\ O=${reference}.dict # Trimming section adapters=~/.conda/pkgs/trimmomatic-0.39-1/share/trimmomatic-0.39-1/adapters/TruSeq3-PE.fa cat $src/sample_list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'read1=$src/raw_reads/{}_1.fastq.gz read2=$src/raw_reads/{}_2.fastq.gz paired_r1=$src/clean_reads/{}_paired_R1.fastq.gz paired_r2=$src/clean_reads/{}_paired_R2.fastq.gz unpaired_r1=$src/clean_reads/{}_unpaired_R1.fastq.gz unpaired_r2=$src/clean_reads/{}_unpaired_R2.fastq.gz # the minimum read length accepted, we do the liberal 30bp here min_length=30 trimmomatic PE -threads 1 \\ $read1 $read2 $paired_r1 $unpaired_r1 $paired_r2 $unpaired_r2 \\ ILLUMINACLIP:${adapters}:2:30:10:2:True \\ LEADING:3 TRAILING:3 MINLEN:${min_length}' # Section for alignment and marking duplicates. # Note we parallelize such that BWA uses exactly one node. # Then, we have a number of jobs equal to the number of nodes requested. cat $src/sample_list | env_parallel -j 1 --sshloginfile $PBS_NODEFILE \\ 'bwa mem \\ -t 8 -M \\ -R \"@RG\\tID:{}\\tPL:ILLUMINA\\tLB:{}\\tSM:{}\" \\ $reference \\ $src/clean_reads/{}_paired_R1.fastq.gz \\ $src/clean_reads/{}_paired_R2.fastq.gz \\ > $src/alignments/{}.sam gatk MarkDuplicatesSpark \\ -I $src/alignments/{}.sam \\ -M $src/bams/{}_dedup_metrics.txt \\ --tmp-dir $src/alignments/dedup_temp \\ -O $src/bams/{}_dedup.bam \\ --conf \"spark.executor.cores=8\" rm $src/alignments/{}.sam' # Collecting metrics in parallel # Remember to change from _recal to _dedup if you can\u2019t do base recalibration. # Also, depth will take A LOT of room up, so you may not want to run it until you know what to do with it. cat $src/sample_list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'picard CollectAlignmentSummaryMetrics \\ R=${reference}.fa \\ I=$src/bams/{}_dedup.bam \\ O=$src/alignments/alignment_summary/{}_alignment_summary.txt picard CollectInsertSizeMetrics \\ INPUT=$src/bams/{}_dedup.bam \\ OUTPUT=$src/alignments/insert_metrics/{}_insert_size.txt \\ HISTOGRAM_FILE=$src/alignments/insert_metrics/{}_insert_hist.pdf samtools depth \\ -a $src/bams/{}_dedup.bam \\ > $src/alignments/depth/{}_depth.txt' # Scatter-gather HaploType Caller, probably the most likely to need checkpoints. # This can take a lot of different forms, this one is best for large files. # Go to the HaplotypeCaller section for more info. cut -f 1 ${reference}.fa.fai > $src/intervals.list while read sample; do mkdir ${src}/gvcfs/${sample} cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/${sample}_dedup.bam \\ -O $src/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \\ -L {} \\ -ERC GVCF' done < $src/sample_list # Run CombineGVCFs per interval, each step combines all samples into one interval-specific GVCF cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'interval_list=\"\" # loop to generate list of sample-specific intervals to combine while read sample; do interval_list=\"${interval_list}-V ${src}/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \" done < $src/sample_list gatk --java-options \"-Xmx6g\" CombineGVCFs \\ -R ${reference}.fa \\ ${interval_list} \\ -O $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz' # Run GenotypeGVCFs on each interval GVCF cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" GenotypeGVCFs \\ -R ${reference}.fa \\ -V $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz \\ -O $src/combined_vcfs/intervals/{}_genotyped.vcf.gz' # Make a file with a list of paths for GatherVcfs to use > $src/combined_vcfs/gather_list while read interval; do echo ${src}/combined_vcfs/intervals/${interval}_genotyped.vcf.gz >> \\ $src/combined_vcfs/gather_list done < $src/intervals.list # Run GatherVcfs gatk GatherVcfs \\ -I $src/combined_vcfs/gather_list \\ -O $src/combined_vcfs/combined_vcf.vcf.gz # Index the gathered VCF gatk IndexFeatureFile \\ -I $src/combined_vcfs/combined_vcf.vcf.gz # Select and filter variants gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-type SNP \\ -O $src/combined_vcfs/raw_snps.vcf.gz gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-type INDEL \\ -O $src/combined_vcfs/raw_indel.vcf.gz gatk VariantFiltration \\ -R ${reference}.fa \\ -V $src/combined_vcfs/raw_snps.vcf.gz \\ -O $src/analysis_vcfs/filtered_snps.vcf \\ -filter \"DP < 4\" --filter-name \"DP_filter\" \\ -filter \"QUAL < 30.0\" --filter-name \"Q_filter\" \\ -filter \"QD < 2.0\" --filter-name \"QD_filter\" \\ -filter \"FS > 60.0\" --filter-name \"FS_filter\" \\ -filter \"MQ < 40.0\" --filter-name \"MQ_filter\" Troubleshooting If you need help troubleshooting an error, make sure to let us know the size of your dataset (number of individuals and approximate number of reads should suffice, unless coverage varies between individuals), GATK version, node details, and any error messages output. Citations Bolger, A. M., Lohse, M., & Usadel, B. (2014). Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics, 30(15), 2114\u20132120. https://doi.org/10.1093/bioinformatics/btu170 Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. Bioinformatics, 25(14), 1754\u20131760. https://doi.org/10.1093/bioinformatics/btp324 Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., \u2026 Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. Bioinformatics, 25(16), 2078\u20132079. https://doi.org/10.1093/bioinformatics/btp352 McKenna, A., Hanna, M., Banks, E., Sivachenko, A., Cibulskis, K., Kernytsky, A., \u2026 DePristo, M. A. (2010). The genome analysis toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data. Genome Research, 20(9), 1297\u20131303. https://doi.org/10.1101/gr.107524.110 Picard toolkit. (2019). Broad Institute, GitHub Repository. https://doi.org/http://broadinstitute.github.io/picard/ Tange, O. (2018). GNU Parallel 2018 [Computer software]. https://doi.org/10.5281/zenodo.1146014.","title":"Genomic variant calling with GATK"},{"location":"GATK_QuickByte/#parallel-genomic-variant-calling-with-genome-analysis-toolkit-gatk","text":"This QuickByte outlines how to run a pipeline based on Genome Analysis Toolkit v4 (GATK4) best practices, a common pipeline for processing genomic data from Illumina platforms. Major modifications from \u201ctrue\u201d best practices are done to facilitate using this pipeline for both model and non-model organisms. Additionally, we show how to best parallelize these steps on CARC. Here we outline the steps for a single sample without parallelization, then with parallelization for specific steps, and finally provide an example of a fully parallelized script. Extensive documentation (including other Best Practices pipelines) can be found here . Specifically, the Best Practices informing this pipeline are the data pre-processing workflow and the germline short variant discovery workflow . We aim to give you sample commands to emulate these scripts workflows, which will also allow you to easily modify the pipeline. The goal of this pipeline is to output Single Nucleotide Polymorphisms (SNPs) and optionally indels for a given dataset. This same pipeline can be used for humans, model organisms, and non-model organisms. Spots that can leverage information from model organisms are noted, but those steps can be bypassed. Because sample size and depth of coverage are often lower in non-model organisms, filtering recommendations and memory requirements will vary. Note that this assumes you are using paired-end data and will differ slightly if you use unpaired. The basic steps are aligning and processing raw reads into binary alignment map (BAM) files, optionally getting descriptive metrics about the samples\u2019 sequencing and alignment, calling variants to produce genomic variant call format (GVCF) files, genotyping those GVCFs to produce VCFs, and filtering those variants for analysis. For CARC users, we have provided some test data to run this on from a paper on the conservation genomics of sagegrouse . It is two sets of gzipped fastq files per species (i.e. eight total, 4 read and 4 read 2), a file with adapter sequences to trim, and a reference genome. They are located at /projects/shared/tutorials/GATK/. Copy them into your space like \"cp /projects/shared/tutorials/quickbytes/GATK/* ~/path/to/directory\". A .pbs script for running the pipeline (seen below) is also included, but you may learn more by running each step individually. The whole process with the script with 4 nodes on wheeler takes about 5.5 hours. If you run this script, note that it should output a filtered VCF file that's ~350 Mb. Please note that you must cite any program you use in a paper. At the end of this, we have provided citations you would include for the programs we ran here.","title":"Parallel genomic variant calling with Genome Analysis Toolkit (GATK)"},{"location":"GATK_QuickByte/#table-of-contents","text":"Preliminaries The Pipeline Trimming reads Alignment and pre-processing Summary statistics Calling Variants Consolidating and genotyping Selecting variants Scatter-gather parallel Sample Scatter-gather PBS script Trobuleshooting Citations","title":"Table of Contents"},{"location":"GATK_QuickByte/#preliminary-stuff","text":"","title":"Preliminary stuff"},{"location":"GATK_QuickByte/#module-and-directories","text":"We will be using conda to make an environment to load within our PBS script. First, if you haven\u2019t already, set up conda as follows: module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik # can also use other conda modules conda init bash <exit and re-log or restart shell> This following line will create an environment and install the most recent versions of what we need. We assume you run this before starting up your job. conda create -n gatk-env -c bioconda -c conda-forge gatk4 bwa samtools picard trimmomatic Alternatively, you can load these as modules if you are on Wheeler (Xena only has Samtools now), but they may not be the most recent versions: module load bwa-0.7.17-intel-18.0.2-7jvpfu2 module load samtools-1.9-gcc-7.3.0-tnzvvzt module load picard-2.20.8-gcc-4.8.5-3yh2dzv module load gatk-4.1.4.1-gcc-4.8.5-python3-fqiavji module load trimmomatic-0.36-gcc-4.8.5-q3gx4rj If you are parallelizing (see \u201cScatter-gather Parallel\u201d and sample PBS script), you'll need this: module load parallel-20170322-gcc-4.8.5-2ycpx7e source $(which env_parallel.bash) The directories we will need (other than the home directory) are a raw_reads directory for the demultiplexed reads and the following for various intermediate files to go into. Alternatively, if you don\u2019t want to move around all your reads, just replace the path in the BWA call with that path. Note that a few of these are only used with scatter-gather parallelization (reccomended for larger datasets). mkdir clean_reads mkdir alignments # next three are only if you get optional metrics mkdir alignments/alignment_summary mkdir alignments/insert_metrics mkdir alignments/depth mkdir alignments/dedup_temp mkdir bams mkdir gvcfs mkdir combined_vcfs mkdir analysis_vcfs # scatter-gather only mkdir combined_vcfs/intervals mkdir gvcfs/combined_intervals We will be using a few variables throughout this that we can set now. These are shortcuts for the path to our working directory and reference. src=$PBS_O_WORKDIR reference=$src/reference","title":"Module and directories"},{"location":"GATK_QuickByte/#sample-names","text":"To keep our script short, and outputs easy to understand, we will use consistent sample names for each step, and keep the sample names in a file. We assume this file is named \u201csample_list\u201d. The file should have one sample name per line. with a single blank line at the end. The one for the tutorial dataset looks like: GRSG_JHWY140 GRSG_JHWY142 GUSG_GGS1 GUSG_GGS2 We will use this sample list in two ways. The first way is loops, and second is GNU parallel. You can see some examples in the PBS script at the end of the document. Here's a basic demonstration of how to us the list in a loop: while read sample; do RunTask -input ${sample} done < $src/sample_list And this is what GNU parallel looks like (note it's different for BWA, as we need to specify a specific number of jobs). Remember, we need to use env_parallel if we are using conda. cat $src/sample_list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'RunTask -input {}.file' For clarity, in most cases the commands are written as they would be for a for loop (i.e. with $sample instead of {}).","title":"Sample Names"},{"location":"GATK_QuickByte/#demultiplexing","text":"Because it is not covered by best practices, and is often done by the sequencing center, we will not go into the details of demultiplexing here. We recommend you use Illumina\u2019s software bcl2fastq if you have the data in .bcl format, and saber if it has already been converted to fastq format and it does not have dual combinatorial barcodes. We'll assume these reads will be in the raw_reads folder with the name SAMPLE_1.fastq.gz (or 2 for read 2).","title":"Demultiplexing"},{"location":"GATK_QuickByte/#the-pipeline","text":"","title":"The Pipeline"},{"location":"GATK_QuickByte/#trimming-reads","text":"Although not a part of GATK's best practices, it is common practice to trim your reads before using them in analyses. We'll use trimmomatic for this. Trimmomatic performs very poorly with its internal thread command, so we'll use GNU parallel to run it in the final script. Note that trimmomatic doesn't have many command line flags, so we'll name variables ahead of time to keep them straight: # note this assumes the provided fasta file is in your working directory. adapters=$src/TruSeq3-PE.fa read1=$src/raw_reads/${sample}_1.fastq.gz read2=$src/raw_reads/${sample}_2.fastq.gz paired_r1=$src/clean_reads/${sample}_paired_R1.fastq.gz paired_r2=$src/clean_reads/${sample}_paired_R2.fastq.gz unpaired_r1=$src/clean_reads/${sample}_unpaired_R1.fastq.gz unpaired_r2=$src/clean_reads/${sample}_unpaired_R2.fastq.gz # the minimum read length accepted, we do the liberal 30bp here min_length=30 trimmomatic PE -threads 1 \\ $read1 $read2 $paired_r1 $unpaired_r1 $paired_r2 $unpaired_r2 \\ ILLUMINACLIP:${adapters}:2:30:10:2:True \\ LEADING:3 TRAILING:3 MINLEN:${min_length} If you don't have access to the CARC directory with the adapters file, it can be found in the conda install/spack package. The exact path will vary, but they'll be something like this: # spack adapters=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-4.8.5/trimmomatic-0.36-q3gx4rjeruluf75uhcdfkjoaujqnjhzf/bin/TruSeq3-SE.fa # conda adapters=~/.conda/pkgs/trimmomatic-0.39-1/share/trimmomatic-0.39-1/adapters/TruSeq3-PE.fa If you're using the spack module, you call trimmomatic using java: java -jar /opt/spack/opt/spack/linux-centos7-x86_64/gcc-4.8.5/trimmomatic-0.36-q3gx4rjeruluf75uhcdfkjoaujqnjhzf/bin/trimmomatic-0.36.jar PE ...","title":"Trimming Reads"},{"location":"GATK_QuickByte/#alignment-and-pre-processing","text":"This section prepares BAM files for variant calling. First, we need to index our reference and make a sequence dictionary. We'll index two ways, one for bwa and one for GATK: bwa index -p $reference ${reference}.fa samtools faidx ${reference}.fa -o ${reference}.fa.fai picard CreateSequenceDictionary \\ R=${reference}.fa \\ O=${reference}.dict Then, we need to align demultiplexed reads to a reference. For this step, we will use the Burrough-Wheeler Aligner\u2019s (BWA) mem algorithm. Another common option is Bowtie . One important flag here is the -R flag, which is the read group and sample ID for a given sample. We assume that these samples are in the same read group. We can get a node's worth of parallelization with the -t command (it can't work across nodes). Therefore, in the sample script at the end we will show you how to further parallelize BWA. The base command looks like this: bwa mem \\ -t [# threads] -M \\ -R \"@RG\\tID:${sample}\\tPL:ILLUMINA\\tLB:${sample}\\tSM:${sample}\" \\ $reference \\ $src/clean_reads/${sample}_paired_R1.fastq.gz \\ $src/clean_reads/${sample}_paired_R2.fastq.gz \\ > $src/alignments/${sample}.sam The next step is to mark PCR duplicates to remove bias, sort the file, and convert it to the smaller BAM format for downstream use. GATK\u2019s new MarkDuplicatesSpark performs all these tasks, but needs a temporary directory to store intermediate files. Note that although we aren't formally using Spark for parallelization, the line \" --conf 'spark.executor.cores=8'\" still speed it up, and makes (change the number of cores if the cores per node are higher than 8): gatk MarkDuplicatesSpark \\ -I $src/alignments/${sample}.sam \\ -M $src/bams/${sample}_dedup_metrics.txt \\ --tmp-dir $src/alignments/dedup_temp \\ -O $src/bams/${sample}_dedup.bam \\ --conf \"spark.executor.cores=8\" We recommend combining these steps per sample for efficiency and smoother troubleshooting. One issue is that we do not want large SAM files piling up. This can either be done by piping BWA output directly to MarkDuplicatesSpark or removing the SAM file after each loop. In case you want to save the SAM files, we did the latter (this isn\u2019t a bad idea if you have the space, in case there is a problem with generating BAM files). If you are doing base recalibration, you can also add \u201crm ${sample}_debup.bam\u201d to get rid of needless BAM files. Later in the pipeline, we assume you did base recalibration, so will use the {sample}_recal.bam file. If you did not use base recalibration, use {sample}_dedup.bam file in its place.","title":"Alignment and Pre-processing"},{"location":"GATK_QuickByte/#base-quality-score-recalibration-model-organisms","text":"An optional step (and one not taken in the tutorial) is to recalibrate base call scores. This applies machine learning to find where quality scores are over or underestimated based on things like read group and cycle number of a given base. This is recommended, but is rarely possible for non-model organisms, as a file of known polymorphisms is needed. Note, however, that it can take a strongly filtered VCF from the end of the pipeline, before running the entire pipeline again (but others haven\u2019t found much success with this method ). Here is how it looks, with the first line indexing the input VCF file if you haven't already. gatk IndexFeatureFile -I $src[name-of-known-sites].vcf gatk BaseRecalibrator \\ -I $src/bams/${sample}_dedup.bam \\ -R ${reference}.fa \\ --known-sites $src/[name-of-known-sites].vcf \\ -O $src/bams/${sample}_recal_data.table gatk ApplyBQSR \\ -I $src/bams/${sample}_dedup.bam \\ -R ${reference}.fa \\ --bqsr-recal-file $src/bams/${sample}_recal_data.table \\ -O $src/bams/${sample}_recal.bam","title":"Base Quality Score Recalibration (model organisms)"},{"location":"GATK_QuickByte/#collect-alignment-and-summary-statistics-optional","text":"This step is optional, and is not part of GATK\u2019s best practices, but is good to have. It will output important stats for assessing sample quality. Picard\u2019s \u201cCollectAlignmentSummaryMetrics\u201d gives several helpful statistics about the alignment for a given sample. Picard\u2019s \u201cCollectInsertSizeMetrics\u201d gives information about the distribution of insert sizes. Samtools\u2019s \u201cdepth\u201d gives information about the read depth of the sample. Note that \"depth\" has huge output files, so it may be best to skip it until needed. picard CollectAlignmentSummaryMetrics \\ R=${reference}.fa \\ I=$src/bams/${sample}_recal.bam \\ O=$src/alignments/alignment_summary/${sample}_alignment_summary.txt picard CollectInsertSizeMetrics \\ INPUT=$src/bams/${sample}_recal.bam \\ OUTPUT=$src/alignments/insert_metrics/${sample}_insert_size.txt \\ HISTOGRAM_FILE=$src/${sample}_insert_hist.pdf samtools depth \\ -a $src/bams/${sample}_recal.bam \\ > $src/alignments/depth/${sample}_depth.txt","title":"Collect alignment and summary statistics (optional)"},{"location":"GATK_QuickByte/#calling-variants-with-haplotypecaller","text":"The simplest way is individually going through BAM files and calling SNPs on them using HaplotypeCaller in GVCF mode (\u201c-ERC GVCF\u201d flag), resulting in GVCFs as output. gatk HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/${sample}_recal.bam \\ -O $src/gvcfs/${sample}_raw.g.vcf.gz \\ -ERC GVCF One issue with HaplotypeCaller is that it takes a long time, but is not programmed to be parallelized by default. We can use GNU parallel to solve that problem in two ways. If you have many small inputs and don't want to do scatter-gather parallel, you can run one instance of HaplotypeCaller per core. Note that we restrict the memory such that each job can only max out the core it's on (you'll want to change from 6g based on the machine you're running this on): cat $src/sample_list | env_parallel --sshloginfline $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/{}_recal.bam \\ -O $src/gvcfs/${}_raw.g.vcf.gz \\ -ERC GVCF' If you are dealing with large files, HaplotypeCaller may take longer than your walltime. The Scatter-gather Parallel section will outline how to fix that by breaking the job (and the next section) into multiple intervals.","title":"Calling variants with HaplotypeCaller"},{"location":"GATK_QuickByte/#consolidating-and-genotyping","text":"This next step has two options, GenomicsDBImport and CombineGVCFs. GATK recommends GenomicsDBImport, as it is more efficient for large datasets, but it performs poorly on references with many contigs. CombineGVCFs can take a long time for large datasets, but is easier to use. Note that GenomicsDBImport must have intervals (generally corresponding to contigs or chromosomes) specified. GenomicsDBImport can take a file specifying GVCFs, but because CombineGVCFs cannot take this input, we just make a list of samples to combine programmatically and plug it in. Here is how we generate that command: gvcf_names=\"\" while read sample; do gvcf_names=\"${gvcf_names}-V ${src}/gvcfs/${sample}_raw.g.vcf.gz \" done < $src/sample_list If you do use GenomicsDBImport, or want to genotype contigs/chromosomes independently, we'll need intervals for it to work with (the same used for scatter-gather parallelization). Also, you'll need to pre-make a temp directory for holding files: mkdir gendb_temp cut -f 1 ${reference}.fa.fai > $src/intervals.list For GenomicsDBImport, you'll need to get rid of the directory you use for the database (here genomic_database) if you already made it: gatk GenomicsDBImport \\ ${gvcf_names} \\ --genomicsdb-workspace-path $src/genomic_database \\ --tmp-dir $src/gendb_temp \\ -L $src/intervals.list And an example for CombineGVCFs is: gatk CombineGVCFs \\ -R ${reference}.fa \\ ${gvcf_names} \\ -O $src/combined_vcfs/combined_gvcf.g.vcf.gz The next step is to genotype the combined (cohort) GVCF file. Here\u2019s a sample command for GenomicsDBImport: gatk GenotypeGVCFs \\ -R ${reference}.fa \\ -V gendb://$src/genomic_database \\ -O $src/combined_vcfs/combined_vcf.vcf.gz And one for CombineGVCFs: gatk GenotypeGVCFs \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_gvcf.g.vcf.gz \\ -O $src/combined_vcfs/combined_vcf.vcf.gz","title":"Consolidating and Genotyping"},{"location":"GATK_QuickByte/#selecting-and-filtering-variants","text":"This first step is optional, but here we separate out indels and SNPs. Note that we don\u2019t use indels down the line, but similar filters can be applied. gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-yype SNP \\ -O $src/combined_vcfs/raw_snps.vcf.gz gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-type INDEL \\ -O $src/combined_vcfs/raw_indel.vcf.gz Here are some good sample filters. The \u201cDP_filter\u201d is depth of coverage (you will probably want to change this), \u201cQ_filter\u201d is quality score, \u201cQD_filter\u201d is quality by depth (avoids artificial inflation of calls), \"MQ_filter\" is a mapping quality filter, and \u201cFS_filter\u201d is a strand bias filter (higher value means higher bias). Note that DP is better for low depth samples, while QD is better for high depth. More info can be found on GATK\u2019s website . gatk VariantFiltration \\ -R ${reference}.fa \\ -V $src/combined_vcfs/raw_snps.vcf.gz \\ -O $src/analysis_vcfs/filtered_snps.vcf \\ -filter \"DP < 4\" --filter-name \"DP_filter\" \\ -filter \"QUAL < 30.0\" --filter-name \"Q_filter\" \\ -filter \"QD < 2.0\" --filter-name \"QD_filter\" \\ -filter \"FS > 60.0\" --filter-name \"FS_filter\" \\ -filter \"MQ < 40.0\" --filter-name \"MQ_filter\" This will give us our final VCF! Note that the filtered SNPs are still included, just with a filter tag. You can use something like SelectVariants' \"exclude-filtered\" flag or VCFtools\u2019 \u201c--remove-filtered-all\u201d flag to get rid of them.","title":"Selecting and filtering variants"},{"location":"GATK_QuickByte/#scatter-gather-parallel","text":"Scatter-gather is the process of breaking a job into intervals (i.e. contigs or scaffolds in a reference) and running HaplotypeCaller, CombineGVCFs, and GenotypeGVCFs on each interval in parallel. Then, at the end, all the invervals are gathered together with GatherGVCFs. This results in a massive speed-up due to the parallelization. This is fully implemented in the sample script below, with each step outlined here. The output of GatherVcfs is the same as what comes from GenotypeGVCFs in the non-parallel version. Here is how we run HaplotypeCaller, note that this is only one sample, see the sample script for running this on all samples: # make our interval list cut -f 1 ${reference}.fa.fai > $src/intervals.list while read sample; do mkdir ${src}/gvcfs/${sample} cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/${sample}_recal.bam \\ -O $src/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \\ -L {} \\ -ERC GVCF' done < $src/sample_list You'll run then run CombineGVCFs. For each interval, you'll make a list of GVCF file paths for each sample you're including (the while loop below). cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'interval_list=\"\" # loop to generate list of sample-specific intervals to combine while read sample; do interval_list=\"${interval_list}-V ${src}/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \" done < $src/sample_list gatk --java-options \"-Xmx6g\" CombineGVCFs \\ -R ${reference}.fa \\ ${interval_list} \\ -O $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz' Next, you run GenotypeGVCFs to get VCFs to gather afterwards. No fancy lists needed! cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" GenotypeGVCFs \\ -R ${reference}.fa \\ -V $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz \\ -O $src/combined_vcfs/intervals/{}_genotyped.vcf.gz' If you have many samples, it may be best to use GenomicsDBImport. It is very similar, with both that step and the genotyping below. Note that the directory for --genomicsdb-workspace-path can't exist (unless you're updating it): cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'mkdir $src/gendb_temp/{} interval_list=\"\" # loop to generate list of sample-specific intervals while read sample; do interval_list=\"${interval_list}-V ${src}/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \" done < $src/sample_list # run make the genomics databases gatk --java-options \"-Xmx6g\" GenomicsDBImport \\ ${interval_list} \\ --genomicsdb-workspace-path $src/genomics_databases/{} \\ --tmp-dir $src/gendb_temp/{} \\ -L {}' cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" GenotypeGVCFs \\ -R ${reference}.fa \\ -V gendb://$src/genomics_databases/{} \\ -O $src/combined_vcfs/intervals/{}_genotyped.vcf.gz' The final (gather) step uses GatherVcfs, for which we'll make a file containing the paths to all input genotyped VCFs (generated in the while loop). Note the first line initializes a blank file for the gather list. After we make the gathered VCF, we need to index it for future analyses. > $src/combined_vcfs/gather_list while read interval; do echo ${src}/combined_vcfs/intervals/${interval}_genotyped.vcf.gz >> \\ $src/combined_vcfs/gather_list done < $src/chromosomes.list gatk GatherVcfs \\ -I $src/combined_vcfs/gather_list \\ -O combined_vcfs/combined_vcf.vcf.gz gatk IndexFeatureFile \\ -I $src/combined_vcfs/raw_snps.vcf.gz NOTE THAT THIS FILE STILL NEEDS TO HAVE VARIANTS SELECTED AND FILTERED, SEE \"Selecting and filtering variants\" ABOVE","title":"Scatter-gather Parallel"},{"location":"GATK_QuickByte/#sample-pbs-script","text":"Here is a sample PBS script combining everything we have above, with as much parallelization as possible. One reason to break up steps like we did is for improved checkpointing (without having to write code checking if files are already present). Once you are finished running a block of code, you can just comment it out. Similarly, if you can only get part way through your sample list, you can copy it and remove samples that have already completed a given step. To convert this to Slurm, replace $PBS_O_WORKDIR with $SLURM_SUBMIT_DIR and refer to this conversion guide for the rest. #!/bin/bash #PBS -q default #PBS -l nodes=4:ppn=8 #PBS -l walltime=10:00:00 #PBS -N gatk_tutorial #PBS -m ae #PBS -M youremail@school.edu # the PBS lines are for the default queue, using 4 nodes, and has a conservative 10 hour wall time # it is named \"gatk_tutorial\" and sends an email to \"youremail@school.edu\" when done # load your conda environment module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik eval \"$(conda shell.bash hook)\" conda activate gatk-env # load GNU parallel, get env_parallel module load parallel-20170322-gcc-4.8.5-2ycpx7e source $(which env_parallel.bash) src=$PBS_O_WORKDIR # this is \"sagegrouse_reference\" in the tutorial reference=${src}/reference # indexing reference bwa index -p $reference ${reference}.fa samtools faidx ${reference}.fa -o ${reference}.fa.fai picard CreateSequenceDictionary \\ R=${reference}.fa \\ O=${reference}.dict # Trimming section adapters=~/.conda/pkgs/trimmomatic-0.39-1/share/trimmomatic-0.39-1/adapters/TruSeq3-PE.fa cat $src/sample_list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'read1=$src/raw_reads/{}_1.fastq.gz read2=$src/raw_reads/{}_2.fastq.gz paired_r1=$src/clean_reads/{}_paired_R1.fastq.gz paired_r2=$src/clean_reads/{}_paired_R2.fastq.gz unpaired_r1=$src/clean_reads/{}_unpaired_R1.fastq.gz unpaired_r2=$src/clean_reads/{}_unpaired_R2.fastq.gz # the minimum read length accepted, we do the liberal 30bp here min_length=30 trimmomatic PE -threads 1 \\ $read1 $read2 $paired_r1 $unpaired_r1 $paired_r2 $unpaired_r2 \\ ILLUMINACLIP:${adapters}:2:30:10:2:True \\ LEADING:3 TRAILING:3 MINLEN:${min_length}' # Section for alignment and marking duplicates. # Note we parallelize such that BWA uses exactly one node. # Then, we have a number of jobs equal to the number of nodes requested. cat $src/sample_list | env_parallel -j 1 --sshloginfile $PBS_NODEFILE \\ 'bwa mem \\ -t 8 -M \\ -R \"@RG\\tID:{}\\tPL:ILLUMINA\\tLB:{}\\tSM:{}\" \\ $reference \\ $src/clean_reads/{}_paired_R1.fastq.gz \\ $src/clean_reads/{}_paired_R2.fastq.gz \\ > $src/alignments/{}.sam gatk MarkDuplicatesSpark \\ -I $src/alignments/{}.sam \\ -M $src/bams/{}_dedup_metrics.txt \\ --tmp-dir $src/alignments/dedup_temp \\ -O $src/bams/{}_dedup.bam \\ --conf \"spark.executor.cores=8\" rm $src/alignments/{}.sam' # Collecting metrics in parallel # Remember to change from _recal to _dedup if you can\u2019t do base recalibration. # Also, depth will take A LOT of room up, so you may not want to run it until you know what to do with it. cat $src/sample_list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'picard CollectAlignmentSummaryMetrics \\ R=${reference}.fa \\ I=$src/bams/{}_dedup.bam \\ O=$src/alignments/alignment_summary/{}_alignment_summary.txt picard CollectInsertSizeMetrics \\ INPUT=$src/bams/{}_dedup.bam \\ OUTPUT=$src/alignments/insert_metrics/{}_insert_size.txt \\ HISTOGRAM_FILE=$src/alignments/insert_metrics/{}_insert_hist.pdf samtools depth \\ -a $src/bams/{}_dedup.bam \\ > $src/alignments/depth/{}_depth.txt' # Scatter-gather HaploType Caller, probably the most likely to need checkpoints. # This can take a lot of different forms, this one is best for large files. # Go to the HaplotypeCaller section for more info. cut -f 1 ${reference}.fa.fai > $src/intervals.list while read sample; do mkdir ${src}/gvcfs/${sample} cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" HaplotypeCaller \\ -R ${reference}.fa \\ -I $src/bams/${sample}_dedup.bam \\ -O $src/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \\ -L {} \\ -ERC GVCF' done < $src/sample_list # Run CombineGVCFs per interval, each step combines all samples into one interval-specific GVCF cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'interval_list=\"\" # loop to generate list of sample-specific intervals to combine while read sample; do interval_list=\"${interval_list}-V ${src}/gvcfs/${sample}/${sample}_{}_raw.g.vcf.gz \" done < $src/sample_list gatk --java-options \"-Xmx6g\" CombineGVCFs \\ -R ${reference}.fa \\ ${interval_list} \\ -O $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz' # Run GenotypeGVCFs on each interval GVCF cat $src/intervals.list | env_parallel --sshloginfile $PBS_NODEFILE \\ 'gatk --java-options \"-Xmx6g\" GenotypeGVCFs \\ -R ${reference}.fa \\ -V $src/gvcfs/combined_intervals/{}_raw.g.vcf.gz \\ -O $src/combined_vcfs/intervals/{}_genotyped.vcf.gz' # Make a file with a list of paths for GatherVcfs to use > $src/combined_vcfs/gather_list while read interval; do echo ${src}/combined_vcfs/intervals/${interval}_genotyped.vcf.gz >> \\ $src/combined_vcfs/gather_list done < $src/intervals.list # Run GatherVcfs gatk GatherVcfs \\ -I $src/combined_vcfs/gather_list \\ -O $src/combined_vcfs/combined_vcf.vcf.gz # Index the gathered VCF gatk IndexFeatureFile \\ -I $src/combined_vcfs/combined_vcf.vcf.gz # Select and filter variants gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-type SNP \\ -O $src/combined_vcfs/raw_snps.vcf.gz gatk SelectVariants \\ -R ${reference}.fa \\ -V $src/combined_vcfs/combined_vcf.vcf.gz \\ -select-type INDEL \\ -O $src/combined_vcfs/raw_indel.vcf.gz gatk VariantFiltration \\ -R ${reference}.fa \\ -V $src/combined_vcfs/raw_snps.vcf.gz \\ -O $src/analysis_vcfs/filtered_snps.vcf \\ -filter \"DP < 4\" --filter-name \"DP_filter\" \\ -filter \"QUAL < 30.0\" --filter-name \"Q_filter\" \\ -filter \"QD < 2.0\" --filter-name \"QD_filter\" \\ -filter \"FS > 60.0\" --filter-name \"FS_filter\" \\ -filter \"MQ < 40.0\" --filter-name \"MQ_filter\"","title":"Sample PBS Script"},{"location":"GATK_QuickByte/#troubleshooting","text":"If you need help troubleshooting an error, make sure to let us know the size of your dataset (number of individuals and approximate number of reads should suffice, unless coverage varies between individuals), GATK version, node details, and any error messages output.","title":"Troubleshooting"},{"location":"GATK_QuickByte/#citations","text":"Bolger, A. M., Lohse, M., & Usadel, B. (2014). Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics, 30(15), 2114\u20132120. https://doi.org/10.1093/bioinformatics/btu170 Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. Bioinformatics, 25(14), 1754\u20131760. https://doi.org/10.1093/bioinformatics/btp324 Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., \u2026 Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. Bioinformatics, 25(16), 2078\u20132079. https://doi.org/10.1093/bioinformatics/btp352 McKenna, A., Hanna, M., Banks, E., Sivachenko, A., Cibulskis, K., Kernytsky, A., \u2026 DePristo, M. A. (2010). The genome analysis toolkit: A MapReduce framework for analyzing next-generation DNA sequencing data. Genome Research, 20(9), 1297\u20131303. https://doi.org/10.1101/gr.107524.110 Picard toolkit. (2019). Broad Institute, GitHub Repository. https://doi.org/http://broadinstitute.github.io/picard/ Tange, O. (2018). GNU Parallel 2018 [Computer software]. https://doi.org/10.5281/zenodo.1146014.","title":"Citations"},{"location":"GNU_Parallel/","text":"GNU Parallel GNU parallel enables us to run as many jobs in parallel instead of sequentially thus saving lots of time. Unlike creating a queue for exectution of processes in a sequential run, GNU parallel tends to maximally parallize the execution over available processors in an embarassingly parallel fashion. For example, a parallel command to change formats of all files with csv extension to .txt using gnu-parallel is presented. module load image-magick-7.0.5-9-gcc-4.8.5-python2-xzyy5cz $find . -name \"*csv\" | parallel -I% --max-args 1 convert % %.txt Here the command finds all the files with .csv extension first and parallely converts them to txt format . Similarly, parallel can also used to compressed as many files and decompress them. $parallel gzip ::: *.txt $parallel gunzip ::: *.gz Tha above two commands can be used to zip and unzip image files with .jpg exptesion over given path. Matlab Implementation of GNU Parallel GNU Parallel takes many different arguments, but here we will use only two, --arg-file and {}. --arg-file precedes an input file name, \u201cmsizes\u201d, and {} is replaced with each line of the input file. A different copy of Matlab is run simultaneously, with each line of the input file replacing {} in each copy. This is Matlab in high-throughput mode (on a single node). Here is the new \u201cparallel matlab\u201d command: parallel --arg-file msizes \u2018matlab \u2013nojvm \u2013nodisplay \u2013r \u201cmsize={};program\u201d' >/dev/null Single quotes are required around the Matlab portion of the parallel Matlab command. The actual Matlab code is enclosed in double quotes. The contents of the input file, msizes, is: 1 2 3 4 This creates four files: 1.csv, 2.csv, 3.csv, and 4.csv. The contents of the files are, e.g., 1.csv: 0.81472 3.csv: 0.81472,0.91338,0.2785 0.90579,0.63236,0.54688 0.12699,0.09754,0.95751 So far, the command has run Matlab in high-throughput mode on only a single node, a node that you are currently logged into. This is not how Matlab should typically be run at CARC, since it is not taking advantage of the Center's large-scale resources. To run Matlab in high-throughput mode on massively-parallel or cluster machines, it is necessary to use a PBS batch script on the head node of a CARC machine, and have the script run Matlab for you. Use of a PBS batch script allows you to run on many nodes at the same time. Before we use a PBS batch script we will need to make some changes to the program.m and parallel Matlab command. The program.m now reads: %generate a matrix of random numbers of dimension msize x msize rmatrix=rand(msize); %create an output file name based on msize and write the random matrix to it fname=num2str(msize); process=num2str(pid); fname=strcat(process,'.',fname,'.csv'); csvwrite(fname,rmatrix); quit The parent process id is now prepended to the file name. This is helpful because there will now be multiple output files, and they must all have unique names. The command to run Matlab is now: parallel -j0 --arg-file msizes 'matlab -nojvm -nodisplay -r \"msize={};pid=$PPID;program\"' >/dev/null The $PPID (parent process id#) is input to program.m as the pid variable. The -j0 flag ensures that as many cores as possible on the node are used. Running this command (just as an example, without the PBS batch file) would produce output files: 18778.1.csv 18778.2.csv 18778.3.csv 18778.4.csv. The contents are the same as before. The command (with some PBS specific modifications) can now be inserted into a PBS batch script in order to run on more than one node, i.e. in high throughput mode. High-Throughput Mode To generalize the command to run on CARC machines and allow the use of multiple nodes on those machines, a PBS batch script is necessary. This will allocate nodes and the cores on each node to execute the parallel Matlab command. A sample PBS script reads: #PBS -l nodes=4:ppn=2 #PBS -l walltime=00:10:00 #PBS -N matlab_test #PBS -S /bin/bash cd $PBS_O_WORKDIR source /etc/profile.d/modules.sh module load matlab module load gnu-parallel PROCS=$(cat $PBS_NODEFILE | wc -l) nname=`hostname -s` echo starting Matlab jobs at `date` parallel -j0 --sshloginfile $PBS_NODEFILE --workdir $PBS_O_WORKDIR --env PATH --arg-file msizes 'matlab -nojvm -nodisplay -r \"msize={};pid=$$;program\" >/dev/null' echo finished Matlab jobs at `date` The essential portion for running Matlab in high-throughput mode is embodied in the first line, \u201cnodes=4:ppn=2\u201d. It is very important to remember to allocate the correct number of cores with this line. Use this equation to calculate the number of cores you are allocating: nodes x ppn = number of cores allocated. But since you must also match the number of lines of input in msizes (which determines the number of Matlab processes run) to the number of cores allocated, the equation should really be: nodes x ppn = number of cores allocated = number of lines of input = number of Matlab processes run. If you run more Matlab processes than the number of cores you allocate, the job will run much more slowly, e.g. allocating 8 cores and running 9 Matlab jobs will cause the overall job to take twice as long as when running 8 Matlab jobs. Running fewer Matlab jobs than the number of cores allocated leaves cores unused (and unusable by other users for the duration of your job) and so wastes resources. You always want the number of cores allocated to equal the number of Matlab jobs run (which is the number of lines in the file msizes). The scripts can be modified of course, but this principle should always be kept in mind. Also keep in mind also that the \u201cppn\u201d in the first line of the PBS script will vary from machine to machine, and you will get an error if it is incorrectly specified, i.e. some machines have more cores per node than others. For a list of CARC resources and information on numbers of processors/node, see Systems. Here are the contents of program.m: %generate a matrix of random numbers of dimension msize x msize rmatrix=rand(msize); %create a unique output name based on the node hostname, process id#, %and msize and write the random matrix to it [~,hname]=system('hostname'); fname=num2str(msize); process=num2str(pid); fname=strcat(hname,'.',process,'.',fname,'.csv'); csvwrite(fname,rmatrix); quit Here are the contents of msizes: 1 2 3 4 5 6 7 8 The PBS script above allocates 8 cores from a machine with 2 cores per node. You\u2019ll notice there have been some changes to the parallel Matlab command now that it is being used in a PBS script. It now reads: parallel -j0 --sshloginfile $PBS_NODEFILE --workdir $PBS_O_WORKDIR --env PATH --arg-file msizes 'matlab -nojvm -nodisplay -r \"msize={};pid=$$;program\" >/dev/null' You\u2019ll notice that a number of flags have been added. The flag \u201c--sshloginfile $PBS_NODEFILE\u201d gives the parallel command the hostnames of all the nodes that the PBS script has allocated for the job. (The names of the nodes are contained in the PBS environment variable $PBS_NODEFILE.) The flag \u201c--workdir $PBS_O_WORKDIR\u201d uses another PBS environment variable to force Matlab to run in the directory where the PBS script was run. The flag \u201c--env PATH\u201d makes sure all the Matlab processes have the same environment variables as the PBS script has when it is running, e.g. those variables loaded with \u201cmodule\u201d commands. Notice how this variable does not get a dollar sign ($) in front of it. GNU parallel understands this particular flag without the dollar sign. The rest of the command remains the same. Submitting the Job To run this job, save the PBS script to a file, e.g. mtest_pbs. Submit the job to the PBS batch scheduler by typing: qsub mtests_pbs. You should get, in this example, 8 output files containing random numbers. Each file will have a matrix of random numbers of size 1x1, 2x2, 3x3, \u2026, and 8x8. There should also be a .o and .e file, e.g. matlab_test.e27704. This will contain the standard output and standard error of your job, including messages from both Matlab and the PBS queuing system. Due to the way the software works, sometimes Matlab\u2019s error output will actually go into the .o file. The error output of PBS will always go to the .e file. Example of running embarassingly parallel jobs in python with parallel The usage of gnu-parallel for python tasks is similar to that of matlab. Following are the sample codes showing implementation of gnu-parallel for python tasks. Code for PBS script #!/bin/bash #PBS -N Gnu_parallel_test #PBS -l nodes=2:ppn=4 #PBS -l walltime=01:00:00 # load GNU-parallel module module load parallel-20170322-intel-18.0.2-4pa2ap6 # load anaconda python module load anaconda # activate python environment source activate numpy_py3 #you need to have numpy_py3 environment with numpy installed. Refer to Anaconda quickbytes. # change to directory PBS script was submitted. cd $PBS_O_WORKDIR # set jobs per node, which is core per node for galles JOBSPERNODE=8 /usr/bin/time -o time.log parallel --joblog logfile --wd $PBS_O_WORKDIR -j $JOBSPERNODE --sshloginfile $PBS_NODEFILE --env PATH -a mat_in python matrix_inv.py code for matrix_inv.py file # Import Libraries import numpy from numpy import linspace from numpy.random import rand from numpy.linalg import inv import timeit import argparse # define command line inputs parser = argparse.ArgumentParser() parser.add_argument(\"matrix\", type=int, help='provide single integer for matrix') args = parser.parse_args() #define our matrix inverse function def matinv(x): mat = rand(x, x) b = inv(mat) return b out = matinv(args.matrix) numpy.savetxt(\"%d.csv\" % args.matrix, out, delimiter=\",\") The Contents of mat_in looks like 1000 2000 3000 4000 5000 6000 7000 8000","title":"GNU Parallel"},{"location":"GNU_Parallel/#gnu-parallel","text":"GNU parallel enables us to run as many jobs in parallel instead of sequentially thus saving lots of time. Unlike creating a queue for exectution of processes in a sequential run, GNU parallel tends to maximally parallize the execution over available processors in an embarassingly parallel fashion. For example, a parallel command to change formats of all files with csv extension to .txt using gnu-parallel is presented. module load image-magick-7.0.5-9-gcc-4.8.5-python2-xzyy5cz $find . -name \"*csv\" | parallel -I% --max-args 1 convert % %.txt Here the command finds all the files with .csv extension first and parallely converts them to txt format . Similarly, parallel can also used to compressed as many files and decompress them. $parallel gzip ::: *.txt $parallel gunzip ::: *.gz Tha above two commands can be used to zip and unzip image files with .jpg exptesion over given path.","title":"GNU Parallel"},{"location":"GNU_Parallel/#matlab-implementation-of-gnu-parallel","text":"GNU Parallel takes many different arguments, but here we will use only two, --arg-file and {}. --arg-file precedes an input file name, \u201cmsizes\u201d, and {} is replaced with each line of the input file. A different copy of Matlab is run simultaneously, with each line of the input file replacing {} in each copy. This is Matlab in high-throughput mode (on a single node). Here is the new \u201cparallel matlab\u201d command: parallel --arg-file msizes \u2018matlab \u2013nojvm \u2013nodisplay \u2013r \u201cmsize={};program\u201d' >/dev/null Single quotes are required around the Matlab portion of the parallel Matlab command. The actual Matlab code is enclosed in double quotes. The contents of the input file, msizes, is: 1 2 3 4 This creates four files: 1.csv, 2.csv, 3.csv, and 4.csv. The contents of the files are, e.g., 1.csv: 0.81472 3.csv: 0.81472,0.91338,0.2785 0.90579,0.63236,0.54688 0.12699,0.09754,0.95751 So far, the command has run Matlab in high-throughput mode on only a single node, a node that you are currently logged into. This is not how Matlab should typically be run at CARC, since it is not taking advantage of the Center's large-scale resources. To run Matlab in high-throughput mode on massively-parallel or cluster machines, it is necessary to use a PBS batch script on the head node of a CARC machine, and have the script run Matlab for you. Use of a PBS batch script allows you to run on many nodes at the same time. Before we use a PBS batch script we will need to make some changes to the program.m and parallel Matlab command. The program.m now reads: %generate a matrix of random numbers of dimension msize x msize rmatrix=rand(msize); %create an output file name based on msize and write the random matrix to it fname=num2str(msize); process=num2str(pid); fname=strcat(process,'.',fname,'.csv'); csvwrite(fname,rmatrix); quit The parent process id is now prepended to the file name. This is helpful because there will now be multiple output files, and they must all have unique names. The command to run Matlab is now: parallel -j0 --arg-file msizes 'matlab -nojvm -nodisplay -r \"msize={};pid=$PPID;program\"' >/dev/null The $PPID (parent process id#) is input to program.m as the pid variable. The -j0 flag ensures that as many cores as possible on the node are used. Running this command (just as an example, without the PBS batch file) would produce output files: 18778.1.csv 18778.2.csv 18778.3.csv 18778.4.csv. The contents are the same as before. The command (with some PBS specific modifications) can now be inserted into a PBS batch script in order to run on more than one node, i.e. in high throughput mode. High-Throughput Mode To generalize the command to run on CARC machines and allow the use of multiple nodes on those machines, a PBS batch script is necessary. This will allocate nodes and the cores on each node to execute the parallel Matlab command. A sample PBS script reads: #PBS -l nodes=4:ppn=2 #PBS -l walltime=00:10:00 #PBS -N matlab_test #PBS -S /bin/bash cd $PBS_O_WORKDIR source /etc/profile.d/modules.sh module load matlab module load gnu-parallel PROCS=$(cat $PBS_NODEFILE | wc -l) nname=`hostname -s` echo starting Matlab jobs at `date` parallel -j0 --sshloginfile $PBS_NODEFILE --workdir $PBS_O_WORKDIR --env PATH --arg-file msizes 'matlab -nojvm -nodisplay -r \"msize={};pid=$$;program\" >/dev/null' echo finished Matlab jobs at `date` The essential portion for running Matlab in high-throughput mode is embodied in the first line, \u201cnodes=4:ppn=2\u201d. It is very important to remember to allocate the correct number of cores with this line. Use this equation to calculate the number of cores you are allocating: nodes x ppn = number of cores allocated. But since you must also match the number of lines of input in msizes (which determines the number of Matlab processes run) to the number of cores allocated, the equation should really be: nodes x ppn = number of cores allocated = number of lines of input = number of Matlab processes run. If you run more Matlab processes than the number of cores you allocate, the job will run much more slowly, e.g. allocating 8 cores and running 9 Matlab jobs will cause the overall job to take twice as long as when running 8 Matlab jobs. Running fewer Matlab jobs than the number of cores allocated leaves cores unused (and unusable by other users for the duration of your job) and so wastes resources. You always want the number of cores allocated to equal the number of Matlab jobs run (which is the number of lines in the file msizes). The scripts can be modified of course, but this principle should always be kept in mind. Also keep in mind also that the \u201cppn\u201d in the first line of the PBS script will vary from machine to machine, and you will get an error if it is incorrectly specified, i.e. some machines have more cores per node than others. For a list of CARC resources and information on numbers of processors/node, see Systems. Here are the contents of program.m: %generate a matrix of random numbers of dimension msize x msize rmatrix=rand(msize); %create a unique output name based on the node hostname, process id#, %and msize and write the random matrix to it [~,hname]=system('hostname'); fname=num2str(msize); process=num2str(pid); fname=strcat(hname,'.',process,'.',fname,'.csv'); csvwrite(fname,rmatrix); quit Here are the contents of msizes: 1 2 3 4 5 6 7 8 The PBS script above allocates 8 cores from a machine with 2 cores per node. You\u2019ll notice there have been some changes to the parallel Matlab command now that it is being used in a PBS script. It now reads: parallel -j0 --sshloginfile $PBS_NODEFILE --workdir $PBS_O_WORKDIR --env PATH --arg-file msizes 'matlab -nojvm -nodisplay -r \"msize={};pid=$$;program\" >/dev/null' You\u2019ll notice that a number of flags have been added. The flag \u201c--sshloginfile $PBS_NODEFILE\u201d gives the parallel command the hostnames of all the nodes that the PBS script has allocated for the job. (The names of the nodes are contained in the PBS environment variable $PBS_NODEFILE.) The flag \u201c--workdir $PBS_O_WORKDIR\u201d uses another PBS environment variable to force Matlab to run in the directory where the PBS script was run. The flag \u201c--env PATH\u201d makes sure all the Matlab processes have the same environment variables as the PBS script has when it is running, e.g. those variables loaded with \u201cmodule\u201d commands. Notice how this variable does not get a dollar sign ($) in front of it. GNU parallel understands this particular flag without the dollar sign. The rest of the command remains the same. Submitting the Job To run this job, save the PBS script to a file, e.g. mtest_pbs. Submit the job to the PBS batch scheduler by typing: qsub mtests_pbs. You should get, in this example, 8 output files containing random numbers. Each file will have a matrix of random numbers of size 1x1, 2x2, 3x3, \u2026, and 8x8. There should also be a .o and .e file, e.g. matlab_test.e27704. This will contain the standard output and standard error of your job, including messages from both Matlab and the PBS queuing system. Due to the way the software works, sometimes Matlab\u2019s error output will actually go into the .o file. The error output of PBS will always go to the .e file.","title":"Matlab Implementation of GNU Parallel"},{"location":"GNU_Parallel/#example-of-running-embarassingly-parallel-jobs-in-python-with-parallel","text":"The usage of gnu-parallel for python tasks is similar to that of matlab. Following are the sample codes showing implementation of gnu-parallel for python tasks.","title":"Example of running embarassingly parallel jobs in python with parallel"},{"location":"GNU_Parallel/#code-for-pbs-script","text":"#!/bin/bash #PBS -N Gnu_parallel_test #PBS -l nodes=2:ppn=4 #PBS -l walltime=01:00:00 # load GNU-parallel module module load parallel-20170322-intel-18.0.2-4pa2ap6 # load anaconda python module load anaconda # activate python environment source activate numpy_py3 #you need to have numpy_py3 environment with numpy installed. Refer to Anaconda quickbytes. # change to directory PBS script was submitted. cd $PBS_O_WORKDIR # set jobs per node, which is core per node for galles JOBSPERNODE=8 /usr/bin/time -o time.log parallel --joblog logfile --wd $PBS_O_WORKDIR -j $JOBSPERNODE --sshloginfile $PBS_NODEFILE --env PATH -a mat_in python matrix_inv.py","title":"Code for PBS script"},{"location":"GNU_Parallel/#code-for-matrix_invpy-file","text":"# Import Libraries import numpy from numpy import linspace from numpy.random import rand from numpy.linalg import inv import timeit import argparse # define command line inputs parser = argparse.ArgumentParser() parser.add_argument(\"matrix\", type=int, help='provide single integer for matrix') args = parser.parse_args() #define our matrix inverse function def matinv(x): mat = rand(x, x) b = inv(mat) return b out = matinv(args.matrix) numpy.savetxt(\"%d.csv\" % args.matrix, out, delimiter=\",\")","title":"code for matrix_inv.py file"},{"location":"GNU_Parallel/#the-contents-of-mat_in-looks-like","text":"1000 2000 3000 4000 5000 6000 7000 8000","title":"The Contents of mat_in looks like"},{"location":"Gurobi_optimizer_with_R/","text":"Gurobi optimizer with R Gurobi optimizer is a problem solving software that can be used within R. It can solve integer, linear, and quadratic programming optimizations. These techniques can help to find the answers to complex models. Example of running Gurobi optimizer with R at CARC There are modules for both Gurobi and R on the wheeler cluster. All you need to do is load them, and then start an R session. This command is for version 8.1.0, however there are other versions of gurobi available (enter module avail gurobi to see a full list). username@wheeler-sn:~$ module load gurobi/8.1.0 username@wheeler-sn:~$ module load r-3.6.0-gcc-7.3.0-python2-7akol5t username@wheeler-sn:~$ R Once you have started an R session, you can install packages just as you would in R. If you ever run into issues loading packages in R at CARC, you can reach out for assistance by emailling help@carc.unm.edu. One piece of advice if you are using JupyterHub to run an R notebook at CARC is you may need to install packages from ther terminal window on JupyterHub because the notebook will not let you interactiively answer questions installs may need. Start by installing the gurobi package: > install.packages('/opt/local/gurobi/8.1.0/linux64/R/gurobi_8.1-0_R_3.5.0.tar.gz') Installing package into '/users/username/R/x86_64-pc-linux-gnu-library/3.6' * installing *binary* package 'gurobi' ... * DONE (gurobi) You should now be able to load the gurobi library in an R session: > library(gurobi) Loading required package: slam Note that if you get an error regarding slam, you can install it using the command: install.packages(\"slam\", repos = \"https://cloud.r-project.org\") Now let's runs a quick model as an example of what Gurobi can do and to see if everything is working properly: > model <- list() > model$A <- matrix(c(1,2,3,1,1,0), nrow=2, ncol=3, byrow=T) > model$obj <- c(1,1,2) > model$modelsense <- 'max' > model$rhs <- c(4,1) > model$sense <- c('<', '>') > model$vtype <- 'B' > params <- list(OutputFlag=0) > result <- gurobi(model, params) > print('Solution:') [1] \"Solution:\" > print(result$objval) [1] 3 > print(result$x) [1] 1 0 1","title":"Gurobi optimizer with R"},{"location":"Gurobi_optimizer_with_R/#gurobi-optimizer-with-r","text":"Gurobi optimizer is a problem solving software that can be used within R. It can solve integer, linear, and quadratic programming optimizations. These techniques can help to find the answers to complex models.","title":"Gurobi optimizer with R"},{"location":"Gurobi_optimizer_with_R/#example-of-running-gurobi-optimizer-with-r-at-carc","text":"There are modules for both Gurobi and R on the wheeler cluster. All you need to do is load them, and then start an R session. This command is for version 8.1.0, however there are other versions of gurobi available (enter module avail gurobi to see a full list). username@wheeler-sn:~$ module load gurobi/8.1.0 username@wheeler-sn:~$ module load r-3.6.0-gcc-7.3.0-python2-7akol5t username@wheeler-sn:~$ R Once you have started an R session, you can install packages just as you would in R. If you ever run into issues loading packages in R at CARC, you can reach out for assistance by emailling help@carc.unm.edu. One piece of advice if you are using JupyterHub to run an R notebook at CARC is you may need to install packages from ther terminal window on JupyterHub because the notebook will not let you interactiively answer questions installs may need. Start by installing the gurobi package: > install.packages('/opt/local/gurobi/8.1.0/linux64/R/gurobi_8.1-0_R_3.5.0.tar.gz') Installing package into '/users/username/R/x86_64-pc-linux-gnu-library/3.6' * installing *binary* package 'gurobi' ... * DONE (gurobi) You should now be able to load the gurobi library in an R session: > library(gurobi) Loading required package: slam Note that if you get an error regarding slam, you can install it using the command: install.packages(\"slam\", repos = \"https://cloud.r-project.org\") Now let's runs a quick model as an example of what Gurobi can do and to see if everything is working properly: > model <- list() > model$A <- matrix(c(1,2,3,1,1,0), nrow=2, ncol=3, byrow=T) > model$obj <- c(1,1,2) > model$modelsense <- 'max' > model$rhs <- c(4,1) > model$sense <- c('<', '>') > model$vtype <- 'B' > params <- list(OutputFlag=0) > result <- gurobi(model, params) > print('Solution:') [1] \"Solution:\" > print(result$objval) [1] 3 > print(result$x) [1] 1 0 1","title":"Example of running Gurobi optimizer with R at CARC"},{"location":"Install_deep_learning_packages/","text":"Installing Deep Learning Library in Xena This step by step tutorial will guide you through installing deep learning and Machine learning tools in the Xena Server. Setting up the conda environment for installation Firstly load the Anaconda module to use the conda command. module load anaconda3 Create the Conda Environment with a name. conda create --name <env_name> python==3.6 Verify the installation of the environment. conda info --envs Load the environment. source activate <env_name> Install the deep learning Packages (you can install one of these or as per your need) a. Tensorflow : Install GPU version of the tensorflow for better performance conda install -c anaconda tensorflow-gpu b. Keras GPU Version conda install -c anaconda keras-gpu c. Pytorch Non-GPU Version conda install pytorch torchvision -c pytorch d. Pytorch GPU Version First make sure that you have Python 3.7 installed in your current environment. conda create -n <env_name> python==3.7 Next, activate your environment as per step 4 above. source activate <env_name> Finally, install the following packages: conda install /projects/shared/pytorch/PyTorch1.5-K40-Compatible/pytorch-1.5.0-py3.7_cuda10.1.243_cudnn7.6.3_0.tar.bz2 conda install cudatoolkit=10.1.243 To verify that the K40s are available to your pytorch run the following python code: import torch from torch import nn, tensor from torch.cuda import device_count device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') x=torch.rand(5,3) print(x) print(\"Is GPU Available?\",torch.cuda.is_available(),\" CUDA device count:\", torch.cuda.device_count(), \"current_device:\",torch.cuda.current_device()) x = torch.tensor([1, 2, 3], device=device) y = torch.tensor([1,4,9]).to(device) print(x,y) print(x+y) Expected Output: tensor([[0.3220, 0.2174, 0.1226], [0.7249, 0.8111, 0.8414], [0.5974, 0.5169, 0.5242], [0.1436, 0.5150, 0.5688], [0.3298, 0.1289, 0.5349]]) Is GPU Available? True CUDA device count: 1 current_device: 0 tensor([1, 2, 3], device='cuda:0') tensor([1, 4, 9], device='cuda:0') tensor([ 2, 6, 12], device='cuda:0') Install additional Machine Learning packages i. OpenCV conda install -c conda-forge opencv ii. numpy,pandas, matpotlib , scikit-learn conda install numpy pandas matplotlib scikit-learn","title":"Install deep learning packages"},{"location":"Install_deep_learning_packages/#installing-deep-learning-library-in-xena","text":"This step by step tutorial will guide you through installing deep learning and Machine learning tools in the Xena Server.","title":"Installing Deep Learning Library in Xena"},{"location":"Install_deep_learning_packages/#setting-up-the-conda-environment-for-installation","text":"Firstly load the Anaconda module to use the conda command. module load anaconda3 Create the Conda Environment with a name. conda create --name <env_name> python==3.6 Verify the installation of the environment. conda info --envs Load the environment. source activate <env_name> Install the deep learning Packages (you can install one of these or as per your need) a. Tensorflow : Install GPU version of the tensorflow for better performance conda install -c anaconda tensorflow-gpu b. Keras GPU Version conda install -c anaconda keras-gpu c. Pytorch Non-GPU Version conda install pytorch torchvision -c pytorch d. Pytorch GPU Version First make sure that you have Python 3.7 installed in your current environment. conda create -n <env_name> python==3.7 Next, activate your environment as per step 4 above. source activate <env_name> Finally, install the following packages: conda install /projects/shared/pytorch/PyTorch1.5-K40-Compatible/pytorch-1.5.0-py3.7_cuda10.1.243_cudnn7.6.3_0.tar.bz2 conda install cudatoolkit=10.1.243 To verify that the K40s are available to your pytorch run the following python code: import torch from torch import nn, tensor from torch.cuda import device_count device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') x=torch.rand(5,3) print(x) print(\"Is GPU Available?\",torch.cuda.is_available(),\" CUDA device count:\", torch.cuda.device_count(), \"current_device:\",torch.cuda.current_device()) x = torch.tensor([1, 2, 3], device=device) y = torch.tensor([1,4,9]).to(device) print(x,y) print(x+y) Expected Output: tensor([[0.3220, 0.2174, 0.1226], [0.7249, 0.8111, 0.8414], [0.5974, 0.5169, 0.5242], [0.1436, 0.5150, 0.5688], [0.3298, 0.1289, 0.5349]]) Is GPU Available? True CUDA device count: 1 current_device: 0 tensor([1, 2, 3], device='cuda:0') tensor([1, 4, 9], device='cuda:0') tensor([ 2, 6, 12], device='cuda:0') Install additional Machine Learning packages i. OpenCV conda install -c conda-forge opencv ii. numpy,pandas, matpotlib , scikit-learn conda install numpy pandas matplotlib scikit-learn","title":"Setting up the conda environment for installation"},{"location":"Intro_to_slurm/","text":"Slurm Workload Manager Slurm is a resource manager and job scheduler designed for scheduling and allocating resources as per user job requirements. Slurm is an open source software originally created by the Livermore Computing Center. Slurm Commands sinfo provides information regarding resources that are available from server. Example : user@taos:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST normal* up 2-00:00:00 2 mix taos[01,09] normal* up 2-00:00:00 1 alloc taos02 normal* up 2-00:00:00 6 idle taos[03-08] From the output above we can see that one node (taos02) is allocated under a normal partition. Similarly, we can see that two nodes (taos01 and taos09) are in a mixed state meaning multiple users have resources allocated on the same node. The final line in the output shows that all other nodes (taos03-08) are currently idle. sinfo \u2013N \u2013l provides more detailed information about individuals nodes including CPU count, memory, temporary disk space and so on. user@taos:~$ sinfo -N -l Tue Feb 19 19:08:16 2019 NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON taos01 1 normal* mixed 80 2:20:2 386868 690861 10 (null) none taos02 1 normal* allocated 40 2:10:2 64181 309479 1 (null) none taos03 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos04 1 normal* idle 40 2:10:2 64181 358607 1 (null) none taos05 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos06 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos07 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos08 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos09 1 normal* mixed 40 2:10:2 103198 6864803 20 (null) none More information regarding sinfo can be found by typing man sinfo at the command prompt while logged in to Taos. squeue provides information regarding currently running jobs and the resources allocated to those jobs. user@taos:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 22632 normal BinPacke username PD 0:00 1 (Resources) 22548 normal minia username R 1-07:30:18 1 taos09 22562 normal unicycle username R 22:34:59 1 taos02 22567 normal sspace username R 22:00:50 1 taos01 22576 normal megahit username R 7:22:53 1 taos09 The output from squeue shows you the JobID, the type of partition, the name of the job, which user owns the job, the total elapsed time of the job, how many nodes are allocated to that job, and which nodes those are. To cancel a job, you can use scancel <JOBID> where <JOBID> refers to the JobID assigned to your job by Slurm. Slurm Job Submission To submit a job in slurm you do so by submitting a shell script that outlines the resources you are requesting from the scheduler, the software needed for your job, and the commands you wish to run. The beginning of your submission scrip usually contains the #Hashbang specifying which interpreter should be used for the rest of the script, in this case we are using a bash shell as indicated by the code #!/bin/bash . The next portion of your submission script tells Slurm what resources you are requesting and is always preceeded by #SBATCH followed by flags for various parameters detailed below. Example of a Slurm submission script : slurm_submission.sh #!/bin/bash # #SBATCH --job-name=demo #SBATCH --output=result.txt # #SBATCH --ntasks=4 #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=100 #SBATCH --partition=partition_name srun hostname srun sleep The above script is requesting from the scheduler an allocation of 4 nodes for 10 minutes with 100MB of ram per CPU. Note that we are requesting resources for four tasks, --ntasks=4 , but not four nodes specifically. The default behavior of the scheduler is to provide one node per task, but this can be changed with the --cpus-per-task flag. Once the scheduler allocates the requested resources the job starts to run and the commands not preceeded by #SBATCH are interpreted and executed. The script first executes the srun hostname followed by srun sleep command. The arguments \u2013-job-name and \u2013-output correspond to name of the job you are submitting and the name of the output file where the any output not defined by the program being executed is saved. For example, anything printed to stdout will be saved in your --output file. Of note here is the --partition=partition_name (or -p partition_name ) command. This command specifies which partition, or queue, to submit your job to. If you are a member of a specific partition you likely are aware of the name of your partition, however you can see which partition you have access to with the sinfo command. If you leave this blank you will be submitted to the default or community partition. To submit the job you execute the sbatch command followed by the name of your submission script, for example: sbatch submission.sh Once you execute the above command the job is queued until the requested resources are available for to be allocated to your job. Once your job is submitted you can use sstat command to see information about memory usage, CPU usage, and other metrics related to the jobs you own. Below is an example of a Slurm submission script that runs a small python program that takes an integer as an argument, creates a random number matrix with the dimensions defined by the integer you provided, then inverts that matrix and writes it to a CSV file. Below is our small python program named demo.py that we will be invoking. # Import Libraries import numpy from numpy import linspace from numpy.random import rand from numpy.linalg import inv import argparse # define command line inputs parser = argparse.ArgumentParser() parser.add_argument(\"matrix\", type=int, help='provide single integer for matrix') args = parser.parse_args() #define our matrix inverse function def matinv(x): mat = rand(x, x) b = inv(mat) return b out = matinv(args.matrix) numpy.savetxt(\"%d.csv\" % args.matrix, out, delimiter=\",\") Below is the Slurm submission script to submit our python program named submission_python.sh . This job can be submitted by typing sbatch submission_python.sh at the command prompt. Note the module load command that loads the software environment that contains the numpy package necessary to run our program. #!/bin/bash # #SBATCH --job-name=demo #SBATCH --output=result.txt # #SBATCH --ntasks=4 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --partition=ceti module load anaconda3 python demo.py 34 This brief tutorial should provide the basics necessary for submitting jobs to the Slurm Workload Manager on CARC machines.","title":"Intro to Slurm"},{"location":"Intro_to_slurm/#slurm-workload-manager","text":"Slurm is a resource manager and job scheduler designed for scheduling and allocating resources as per user job requirements. Slurm is an open source software originally created by the Livermore Computing Center.","title":"Slurm Workload Manager"},{"location":"Intro_to_slurm/#slurm-commands","text":"sinfo provides information regarding resources that are available from server. Example : user@taos:~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST normal* up 2-00:00:00 2 mix taos[01,09] normal* up 2-00:00:00 1 alloc taos02 normal* up 2-00:00:00 6 idle taos[03-08] From the output above we can see that one node (taos02) is allocated under a normal partition. Similarly, we can see that two nodes (taos01 and taos09) are in a mixed state meaning multiple users have resources allocated on the same node. The final line in the output shows that all other nodes (taos03-08) are currently idle. sinfo \u2013N \u2013l provides more detailed information about individuals nodes including CPU count, memory, temporary disk space and so on. user@taos:~$ sinfo -N -l Tue Feb 19 19:08:16 2019 NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON taos01 1 normal* mixed 80 2:20:2 386868 690861 10 (null) none taos02 1 normal* allocated 40 2:10:2 64181 309479 1 (null) none taos03 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos04 1 normal* idle 40 2:10:2 64181 358607 1 (null) none taos05 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos06 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos07 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos08 1 normal* idle 40 2:10:2 64181 309479 1 (null) none taos09 1 normal* mixed 40 2:10:2 103198 6864803 20 (null) none More information regarding sinfo can be found by typing man sinfo at the command prompt while logged in to Taos. squeue provides information regarding currently running jobs and the resources allocated to those jobs. user@taos:~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 22632 normal BinPacke username PD 0:00 1 (Resources) 22548 normal minia username R 1-07:30:18 1 taos09 22562 normal unicycle username R 22:34:59 1 taos02 22567 normal sspace username R 22:00:50 1 taos01 22576 normal megahit username R 7:22:53 1 taos09 The output from squeue shows you the JobID, the type of partition, the name of the job, which user owns the job, the total elapsed time of the job, how many nodes are allocated to that job, and which nodes those are. To cancel a job, you can use scancel <JOBID> where <JOBID> refers to the JobID assigned to your job by Slurm.","title":"Slurm Commands"},{"location":"Intro_to_slurm/#slurm-job-submission","text":"To submit a job in slurm you do so by submitting a shell script that outlines the resources you are requesting from the scheduler, the software needed for your job, and the commands you wish to run. The beginning of your submission scrip usually contains the #Hashbang specifying which interpreter should be used for the rest of the script, in this case we are using a bash shell as indicated by the code #!/bin/bash . The next portion of your submission script tells Slurm what resources you are requesting and is always preceeded by #SBATCH followed by flags for various parameters detailed below. Example of a Slurm submission script : slurm_submission.sh #!/bin/bash # #SBATCH --job-name=demo #SBATCH --output=result.txt # #SBATCH --ntasks=4 #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=100 #SBATCH --partition=partition_name srun hostname srun sleep The above script is requesting from the scheduler an allocation of 4 nodes for 10 minutes with 100MB of ram per CPU. Note that we are requesting resources for four tasks, --ntasks=4 , but not four nodes specifically. The default behavior of the scheduler is to provide one node per task, but this can be changed with the --cpus-per-task flag. Once the scheduler allocates the requested resources the job starts to run and the commands not preceeded by #SBATCH are interpreted and executed. The script first executes the srun hostname followed by srun sleep command. The arguments \u2013-job-name and \u2013-output correspond to name of the job you are submitting and the name of the output file where the any output not defined by the program being executed is saved. For example, anything printed to stdout will be saved in your --output file. Of note here is the --partition=partition_name (or -p partition_name ) command. This command specifies which partition, or queue, to submit your job to. If you are a member of a specific partition you likely are aware of the name of your partition, however you can see which partition you have access to with the sinfo command. If you leave this blank you will be submitted to the default or community partition. To submit the job you execute the sbatch command followed by the name of your submission script, for example: sbatch submission.sh Once you execute the above command the job is queued until the requested resources are available for to be allocated to your job. Once your job is submitted you can use sstat command to see information about memory usage, CPU usage, and other metrics related to the jobs you own. Below is an example of a Slurm submission script that runs a small python program that takes an integer as an argument, creates a random number matrix with the dimensions defined by the integer you provided, then inverts that matrix and writes it to a CSV file. Below is our small python program named demo.py that we will be invoking. # Import Libraries import numpy from numpy import linspace from numpy.random import rand from numpy.linalg import inv import argparse # define command line inputs parser = argparse.ArgumentParser() parser.add_argument(\"matrix\", type=int, help='provide single integer for matrix') args = parser.parse_args() #define our matrix inverse function def matinv(x): mat = rand(x, x) b = inv(mat) return b out = matinv(args.matrix) numpy.savetxt(\"%d.csv\" % args.matrix, out, delimiter=\",\") Below is the Slurm submission script to submit our python program named submission_python.sh . This job can be submitted by typing sbatch submission_python.sh at the command prompt. Note the module load command that loads the software environment that contains the numpy package necessary to run our program. #!/bin/bash # #SBATCH --job-name=demo #SBATCH --output=result.txt # #SBATCH --ntasks=4 #SBATCH --time=10:00 #SBATCH --mem-per-cpu=100 #SBATCH --partition=ceti module load anaconda3 python demo.py 34 This brief tutorial should provide the basics necessary for submitting jobs to the Slurm Workload Manager on CARC machines.","title":"Slurm Job Submission"},{"location":"MATLAB_Deep_Learning_on_Xena/","text":"MATLAB Deep Learning on Xena MATLAB has great tools for deep learning and convolutional neural networks (CNNs). These tools can make use of GPUs, which are available for use on the Xena cluster. This Quickbytes tutorial will mimic the official Mathworks tutorial on using deep learning for JPEG Image Deblocking. To see that tutorial, follow this link. Before we begin, create a directory named 'deepLearningExample' from within your home directory. xena:~$ cd ~ xena:~$ mkdir deepLearningExample Table of Contents Train CNN Interactively Open MATLAB Get Required Example Functions Locate them interactively in MATLAB GUI Locate them using the terminal Create MATLAB script Subsequent uses of this Script Train Interactively Train CNN via Scheduled Job Slurm Scripts Single GPU Dual GPU Submit Script View Results Test Model MATLAB Script Changes you can make to the test_model.m script Different test image Zoom in on specific ROI Train CNN Interactively It is possible to train the the example CNN in an interactive MATLAB session and track the progress. This requires X11 fowarding. For a full guide on how to do that, please view our Quickbytes youtube tutorial. . Please note that you should not fully train a CNN interactively as the plotting requires large amounts of memory. It is alright to use the interactive plotting to verify that a model is training. In order to fully train a model, please schedule a job using a slurm script and disable the plotting. Open MATLAB It is highly reccommended to use the dualGPU partition and request two GPUs. The commands below will show you how to use both the single and dual GPU partitions. Once logged into Xena with X11 fowarding, you can begin an interactive job on a compute node. xena:~$ srun --partition singleGPU --x11 --mem 0 --ntasks 1 --cpus-per-task 16 -G 1 --pty bash This requests a single node and gpu with X11 forwarding. To use a machine with two gpus, use this command instead: xena:~$ srun --partition dualGPU --x11 --mem 0 --ntasks 1 --cpus-per-task 16 -G 2 --pty bash Once you are assigned a compute node, cd into our new directory, then start an interactive session of MATLAB: xena-01:~$ cd deepLearningExample/ xena-01:~$ module load matlab xena-01:~$ matlab This should bring up the MATLAB graphical user interface (GUI). Use the file brower on the left side of the window to move into your 'deepLearningExample' directory within MATLAB. Get Required Example Functions Before continuing, you must move the given MathWorks MATLAB functions (.m files) into yourly created 'deepLearningExample` directory. There are two ways to locate these files. Locate them interactively in MATLAB GUI. Locate them using the terminal. Locate them interactively in MATLAB GUI Follow these steps to locate the files from with the MATLAB GUI opened in the above step. Attempt to use an example function with blank arguments in the MATLAB Command Window. >> downloadIAPRTC12Data('','') That will cause an error and provide links to the examples MATLAB thinks you are using. Click on the JPEG Image Deblocking Using Deep Learning link. This will move MATLAB's current wortking directory to that of the Example code. In the file browser on the left side of the window, select all of the files in the current directory. Right click the selected files and hit 'copy'. Use the file brower on the left side to navigate back to your 'deepLearningExample' directory. Right click in the file brower and select paste to place all the files in this directory. Locate them using the terminal Follow these steps to copy the required example code into your new directory. Ssh into the compute node assigned to you (make sure the MATLAB module is loaded). xena:~$ ssh xena-01 Move into the MATLAB Examples directory. xena-01:~$ cd /tmp/Examples/R2021a/deeplearning_shared/JPEGImageDeblockingDeepLearningExample Copy all the needed files. xena-01:~$ cp *.m ~/deepLearningExample (Optional) Copy the pretrained example CNN. xena-01:~$ cp pretrianedJPEGDnCNN.mat ~/deepLearningExample Create MATLAB script Now, create the following MATLAB script with the name 'deep_learning_example.m' and ensure it lives within the directory created above ('~/deepLearningExample/'). dataDir = '~/deepLearningExample/data'; % Ensure the data is downloaded first isDownloaded = false; if ~isDownloaded downloadIAPRTC12Data('http://www-i6.informatik.rwth-aachen.de/imageclef/resources/iaprtc12.tgz',dataDir); end % Grab Subset of data for training trainImagesDir = fullfile(dataDir, \"iaprtc12\",\"images\",\"00\"); exts = [\".jpg\",\".bmp\",\".png\"]; imdsPristine = imageDatastore(trainImagesDir,FileExtensions=exts); % Prepare Training Data by compressing at various levels of quality JPEGQuality = [5:5:40 50 60 70 80]; isCompressed = false; if ~isCompressed [compressedDirName,residualDirName] = createJPEGDeblockingTrainingSet(imdsPristine,JPEGQuality); else compressedDirName = fullfile(dataDir,\"iaprtc12\",\"images\",\"00\",\"compressedImages\"); residualDirName = fullfile(dataDir,\"iaprtc12\",\"images\",\"00\",\"residualImages\"); end % Create Random Patch Extraction Datastore for Training imdsCompressed = imageDatastore(compressedDirName,FileExtensions=\".mat\",ReadFcn=@matRead); imdsResidual = imageDatastore(residualDirName,FileExtensions=\".mat\",ReadFcn=@matRead); augmenter = imageDataAugmenter(... RandRotation=@()randi([0,1],1)*90,... RandXReflection=true); patchSize = 50; patchesPerImage = 128; % Prepare dataset and setup CNN options dsTrain = randomPatchExtractionDatastore(imdsCompressed,imdsResidual,patchSize, ... PatchesPerImage=patchesPerImage, ... DataAugmentation=augmenter); dsTrain.MiniBatchSize = patchesPerImage; inputBatch = preview(dsTrain); disp(inputBatch) layers = dnCNNLayers maxEpochs = 10; initLearningRate = 0.1; l2reg = 0.0001; batchSize = 64; options = trainingOptions(\"sgdm\", ... Momentum=0.9, ... InitialLearnRate=initLearningRate, ... LearnRateSchedule=\"piecewise\", ... GradientThresholdMethod=\"absolute-value\", ... GradientThreshold=0.005, ... L2Regularization=l2reg, ... MiniBatchSize=batchSize, ... MaxEpochs=maxEpochs, ... % WARNING - turning on plots uses massive amounts of RAM. Do not run for more than 3 or 4 epochs. %Plots=\"training-progress\", ... Plots=\"none\", ... ExecutionEnvironment='multi-gpu',... Verbose=true); % Train the network doTraining = true; if doTraining [net,info] = trainNetwork(dsTrain,layers,options); modelDateTime = string(datetime(\"now\",Format=\"yyyy-MM-dd-HH-mm-ss\")); save(\"trainedJPEGDnCNN-\"+modelDateTime+\".mat\",\"net\"); end return If you are using a single GPU machine, change this line: ExecutionEnvironment='multi-gpu' into this: ExecutionEnvironment='gpu' Subsequent uses of this Script The script uses the following booleans to allow you to skip various steps. Once you have downloaded the data, change this line: isDownloaded = false; to this: isDownloaded = true; Once you have compressed the data, change this line: isCompressed = false; to this: isCompressed = true; Train Interactively To train the network interactively, run the script from within the interactive MATLAB session's Command Window >> deep_learning_example In the MATLAB script, you can change the 'Plotting' option to view a plot of the training status in real time. However, this uses massive amounts of RAM and should not be run for more than 3 or 4 epochs. You can use the plotting as verification that a model is actually training, but you should not attempt to fully train a model with the plotting enabled. Downloading and compressing the images can take quite a few minutes. Training the model takes up to 9 hours to complete 10 epochs on the dual GPU machines. Due to the way MATLAB trains a network with this depth (20 convolutional layers by default), these machines run out memory when training the network past 12 epochs with the rest of the settings left unchanged. One way to reduce memory usage and training time is top use fewer compressed images in the training set. Another option is to use a different layer setup with fewer hidden layers. Train CNN via Scheduled Job Since training this CNN can take many hours, it is a good idea to take advantage of the Slurm scheduler. Interactive jobs can be stopped and interrupted due to internet connection issues. In the options of the CNN itself we set Verbose=true , which will allow us to see the status of the model being trained in an ouput file that is updated in real time. Slurm Scripts It is reccomended that you use the dual GPU machines. When using either of the scripts below, ensure the ExecutionEnvironment option in the above MATLAB script matches your choice of machine. Single GPU To request a job with a single GPU, create the following script with the name 'dncnn_single_gpu.sh': #!/bin/bash #SBATCH --job-name DnCNN_singleGPU #SBATCH --mail-user jmccullough12@unm.edu #SBATCH --mail-type ALL #SBATCH --output dncnn_single_gpu.out #SBATCH --error dncnn_single_gpu.err #SBATCH --time 48:00:00 #SBATCH --partition singleGPU #SBATCH --ntasks 1 #SBATCH --mem 0 #SBATCH --cpus-per-task 16 #SBATCH -G 1 cd ~/deepLearningExample module load matlab matlab -nodisplay -r deep_learning_example > dncnn_single_training.out Dual GPU To request a job with dual GPUs, create the following script with the name 'dncnn_dual_gpu.sh': #!/bin/bash #SBATCH --job-name DnCNN_DualGPU #SBATCH --mail-user <YOUR EMAIL> #SBATCH --mail-type ALL #SBATCH --output dncnn_dual_gpu.out #SBATCH --error dncnn_dual_gpu.err #SBATCH --time 48:00:00 #SBATCH --partition dualGPU #SBATCH --ntasks 1 #SBATCH --mem 0 #SBATCH --cpus-per-task 16 #SBATCH -G 2 cd ~/deepLearningExample module load matlab matlab -nodisplay -r deep_learning_example > dncnn_dual_training.out Submit Script To submit a job request using the single GPU script, use the following command: xena:~$ sbatch dncnn_single_gpu.sh To submit a job request using the dual GPU script, use the following command: xena:~$ sbatch dncnn_dual_gpu.sh View Results While a network is being trained, you can see the results in real time with the cat command. If you used the single GPU slurm script, use this command to view the output: xena:~$ cat ~/deepLearningExamples/dncnn_single_training.out If you used the dual GPU slurm script, use this command to view the output: xena:~$ cat ~/deepLearningExamples/dncnn_dual_training.out Test the Model Once a model has finished training, it will be saved to a .mat file with a name like: trainedJPEGDnCNNyyyy-MM-dd-HH-mm-ss.mat You can also use the pretrained example model (see the above instructions to get the example MATALB function files). MATLAB Script Use the following MATLAB Script to the results of a trianed model. To specify which model to use, replace the <MODEL FILE> with the name of your model. If the script is run interactively with X11 fowarding (see above instructions), an image will appear that shows the predictions made on a test image,. The resulting comparisons are also saved to an imaged titled 'results.tif' which can be viewed with your preferred image viewer. Create the following script with the name 'test_model.m' and ensure it lives in the 'deepLearningExample' directory. % Open and test results of trained CNN model for JPEG Denoising % Load the model from file load(\"trainedJPEGDnCNN-2022-05-23-00-37-23.mat\"); % Open test images fileNames = [\"sherlock.jpg\",\"peacock.jpg\",\"fabric.png\",\"greens.jpg\", ... \"hands1.jpg\",\"kobi.png\",\"lighthouse.png\",\"office_4.jpg\", ... \"onion.png\",\"pears.png\",\"yellowlily.jpg\",\"indiancorn.jpg\", ... \"flamingos.jpg\",\"sevilla.jpg\",\"llama.jpg\",\"parkavenue.jpg\", ... \"strawberries.jpg\",\"trailer.jpg\",\"wagon.jpg\",\"football.jpg\"]; filePath = fullfile(matlabroot,\"toolbox\",\"images\",\"imdata\")+filesep; filePathNames = strcat(filePath,fileNames); testImages = imageDatastore(filePathNames); % Select an image to view - choose an image name from the above list. testImage = \"lighthouse.png\"; Ireference = imread(testImage); % Compress the test image in three different levels of quality imwrite(Ireference,fullfile(tempdir,\"testQuality10.jpg\"),\"Quality\",10); imwrite(Ireference,fullfile(tempdir,\"testQuality20.jpg\"),\"Quality\",20); imwrite(Ireference,fullfile(tempdir,\"testQuality50.jpg\"),\"Quality\",50); I10 = imread(fullfile(tempdir,\"testQuality10.jpg\")); I20 = imread(fullfile(tempdir,\"testQuality20.jpg\")); I50 = imread(fullfile(tempdir,\"testQuality50.jpg\")); I10ycbcr = rgb2ycbcr(I10); I20ycbcr = rgb2ycbcr(I20); I50ycbcr = rgb2ycbcr(I50); % Apply network to compressed test images I10y_predicted = denoiseImage(I10ycbcr(:,:,1),net); I20y_predicted = denoiseImage(I20ycbcr(:,:,1),net); I50y_predicted = denoiseImage(I50ycbcr(:,:,1),net); I10ycbcr_predicted = cat(3,I10y_predicted,I10ycbcr(:,:,2:3)); I20ycbcr_predicted = cat(3,I20y_predicted,I20ycbcr(:,:,2:3)); I50ycbcr_predicted = cat(3,I50y_predicted,I50ycbcr(:,:,2:3)); I10_predicted = ycbcr2rgb(I10ycbcr_predicted); I20_predicted = ycbcr2rgb(I20ycbcr_predicted); I50_predicted = ycbcr2rgb(I50ycbcr_predicted); % View and save results montage({I50,I20,I10,I50_predicted,I20_predicted,I10_predicted},Size=[2 3]) title(\"Compressed Images (above) Compared to Deblocked Images (below) with Quality Factor 50, 20 and 10 (Left to Right)\") imwrite(getframe(gca).cdata,'results.tif','tif'); % Change the following boolean to look closer at a specific region of % interes in the test image doROI = false; if doROI roi = [30 440 100 80]; i10 = imcrop(I10,roi); i20 = imcrop(I20,roi); i50 = imcrop(I50,roi); i10predicted = imcrop(I10_predicted,roi); i20predicted = imcrop(I20_predicted,roi); i50predicted = imcrop(I50_predicted,roi); montage({i50,i20,i10,i50predicted,i20predicted,i10predicted},Size=[2 3]) title(\"Compressed Images ROI (above) Compared to Deblocked Images ROI (below) with Quality Factor 50, 20 and 10 (Left to Right)\") imwrite(getframe(gca).cdata,'results_roi.tif','tif'); end Changes you can make to the test_model.m script The above script can be modified for two different kinds of functionality: Use a different test image Zoom in on a specific region of interest (ROI) Different test image You can change the test image by changing the line: testImage = \"lighthouse.png\"; to any of the images in the list of filenames, directly above in the script. Zoom in on specific ROI To zoom in on a specific region of interest, change the following line: doROI = false; to doROI = true; Then ROI can be changed by modifying this line: roi = [30 440 100 80]; This region works well when using lighthouse.png as your test image. Running the script with the boolean changed will display and save a image of the results zoomed in on the ROI. If you are using X11 forwarding, the image should appear on your display. The results are also saved to an image ('results_roi.tif') that can be viewed in your choice of image viewer.","title":"MATLAB Deep Learning on Xena"},{"location":"MATLAB_Deep_Learning_on_Xena/#matlab-deep-learning-on-xena","text":"MATLAB has great tools for deep learning and convolutional neural networks (CNNs). These tools can make use of GPUs, which are available for use on the Xena cluster. This Quickbytes tutorial will mimic the official Mathworks tutorial on using deep learning for JPEG Image Deblocking. To see that tutorial, follow this link. Before we begin, create a directory named 'deepLearningExample' from within your home directory. xena:~$ cd ~ xena:~$ mkdir deepLearningExample","title":"MATLAB Deep Learning on Xena"},{"location":"MATLAB_Deep_Learning_on_Xena/#table-of-contents","text":"Train CNN Interactively Open MATLAB Get Required Example Functions Locate them interactively in MATLAB GUI Locate them using the terminal Create MATLAB script Subsequent uses of this Script Train Interactively Train CNN via Scheduled Job Slurm Scripts Single GPU Dual GPU Submit Script View Results Test Model MATLAB Script Changes you can make to the test_model.m script Different test image Zoom in on specific ROI","title":"Table of Contents"},{"location":"MATLAB_Deep_Learning_on_Xena/#train-cnn-interactively","text":"It is possible to train the the example CNN in an interactive MATLAB session and track the progress. This requires X11 fowarding. For a full guide on how to do that, please view our Quickbytes youtube tutorial. . Please note that you should not fully train a CNN interactively as the plotting requires large amounts of memory. It is alright to use the interactive plotting to verify that a model is training. In order to fully train a model, please schedule a job using a slurm script and disable the plotting.","title":"Train CNN Interactively"},{"location":"MATLAB_Deep_Learning_on_Xena/#open-matlab","text":"It is highly reccommended to use the dualGPU partition and request two GPUs. The commands below will show you how to use both the single and dual GPU partitions. Once logged into Xena with X11 fowarding, you can begin an interactive job on a compute node. xena:~$ srun --partition singleGPU --x11 --mem 0 --ntasks 1 --cpus-per-task 16 -G 1 --pty bash This requests a single node and gpu with X11 forwarding. To use a machine with two gpus, use this command instead: xena:~$ srun --partition dualGPU --x11 --mem 0 --ntasks 1 --cpus-per-task 16 -G 2 --pty bash Once you are assigned a compute node, cd into our new directory, then start an interactive session of MATLAB: xena-01:~$ cd deepLearningExample/ xena-01:~$ module load matlab xena-01:~$ matlab This should bring up the MATLAB graphical user interface (GUI). Use the file brower on the left side of the window to move into your 'deepLearningExample' directory within MATLAB.","title":"Open MATLAB"},{"location":"MATLAB_Deep_Learning_on_Xena/#get-required-example-functions","text":"Before continuing, you must move the given MathWorks MATLAB functions (.m files) into yourly created 'deepLearningExample` directory. There are two ways to locate these files. Locate them interactively in MATLAB GUI. Locate them using the terminal.","title":"Get Required Example Functions"},{"location":"MATLAB_Deep_Learning_on_Xena/#locate-them-interactively-in-matlab-gui","text":"Follow these steps to locate the files from with the MATLAB GUI opened in the above step. Attempt to use an example function with blank arguments in the MATLAB Command Window. >> downloadIAPRTC12Data('','') That will cause an error and provide links to the examples MATLAB thinks you are using. Click on the JPEG Image Deblocking Using Deep Learning link. This will move MATLAB's current wortking directory to that of the Example code. In the file browser on the left side of the window, select all of the files in the current directory. Right click the selected files and hit 'copy'. Use the file brower on the left side to navigate back to your 'deepLearningExample' directory. Right click in the file brower and select paste to place all the files in this directory.","title":"Locate them interactively in MATLAB GUI"},{"location":"MATLAB_Deep_Learning_on_Xena/#locate-them-using-the-terminal","text":"Follow these steps to copy the required example code into your new directory. Ssh into the compute node assigned to you (make sure the MATLAB module is loaded). xena:~$ ssh xena-01 Move into the MATLAB Examples directory. xena-01:~$ cd /tmp/Examples/R2021a/deeplearning_shared/JPEGImageDeblockingDeepLearningExample Copy all the needed files. xena-01:~$ cp *.m ~/deepLearningExample (Optional) Copy the pretrained example CNN. xena-01:~$ cp pretrianedJPEGDnCNN.mat ~/deepLearningExample","title":"Locate them using the terminal"},{"location":"MATLAB_Deep_Learning_on_Xena/#create-matlab-script","text":"Now, create the following MATLAB script with the name 'deep_learning_example.m' and ensure it lives within the directory created above ('~/deepLearningExample/'). dataDir = '~/deepLearningExample/data'; % Ensure the data is downloaded first isDownloaded = false; if ~isDownloaded downloadIAPRTC12Data('http://www-i6.informatik.rwth-aachen.de/imageclef/resources/iaprtc12.tgz',dataDir); end % Grab Subset of data for training trainImagesDir = fullfile(dataDir, \"iaprtc12\",\"images\",\"00\"); exts = [\".jpg\",\".bmp\",\".png\"]; imdsPristine = imageDatastore(trainImagesDir,FileExtensions=exts); % Prepare Training Data by compressing at various levels of quality JPEGQuality = [5:5:40 50 60 70 80]; isCompressed = false; if ~isCompressed [compressedDirName,residualDirName] = createJPEGDeblockingTrainingSet(imdsPristine,JPEGQuality); else compressedDirName = fullfile(dataDir,\"iaprtc12\",\"images\",\"00\",\"compressedImages\"); residualDirName = fullfile(dataDir,\"iaprtc12\",\"images\",\"00\",\"residualImages\"); end % Create Random Patch Extraction Datastore for Training imdsCompressed = imageDatastore(compressedDirName,FileExtensions=\".mat\",ReadFcn=@matRead); imdsResidual = imageDatastore(residualDirName,FileExtensions=\".mat\",ReadFcn=@matRead); augmenter = imageDataAugmenter(... RandRotation=@()randi([0,1],1)*90,... RandXReflection=true); patchSize = 50; patchesPerImage = 128; % Prepare dataset and setup CNN options dsTrain = randomPatchExtractionDatastore(imdsCompressed,imdsResidual,patchSize, ... PatchesPerImage=patchesPerImage, ... DataAugmentation=augmenter); dsTrain.MiniBatchSize = patchesPerImage; inputBatch = preview(dsTrain); disp(inputBatch) layers = dnCNNLayers maxEpochs = 10; initLearningRate = 0.1; l2reg = 0.0001; batchSize = 64; options = trainingOptions(\"sgdm\", ... Momentum=0.9, ... InitialLearnRate=initLearningRate, ... LearnRateSchedule=\"piecewise\", ... GradientThresholdMethod=\"absolute-value\", ... GradientThreshold=0.005, ... L2Regularization=l2reg, ... MiniBatchSize=batchSize, ... MaxEpochs=maxEpochs, ... % WARNING - turning on plots uses massive amounts of RAM. Do not run for more than 3 or 4 epochs. %Plots=\"training-progress\", ... Plots=\"none\", ... ExecutionEnvironment='multi-gpu',... Verbose=true); % Train the network doTraining = true; if doTraining [net,info] = trainNetwork(dsTrain,layers,options); modelDateTime = string(datetime(\"now\",Format=\"yyyy-MM-dd-HH-mm-ss\")); save(\"trainedJPEGDnCNN-\"+modelDateTime+\".mat\",\"net\"); end return If you are using a single GPU machine, change this line: ExecutionEnvironment='multi-gpu' into this: ExecutionEnvironment='gpu'","title":"Create MATLAB script"},{"location":"MATLAB_Deep_Learning_on_Xena/#subsequent-uses-of-this-script","text":"The script uses the following booleans to allow you to skip various steps. Once you have downloaded the data, change this line: isDownloaded = false; to this: isDownloaded = true; Once you have compressed the data, change this line: isCompressed = false; to this: isCompressed = true;","title":"Subsequent uses of this Script"},{"location":"MATLAB_Deep_Learning_on_Xena/#train-interactively","text":"To train the network interactively, run the script from within the interactive MATLAB session's Command Window >> deep_learning_example In the MATLAB script, you can change the 'Plotting' option to view a plot of the training status in real time. However, this uses massive amounts of RAM and should not be run for more than 3 or 4 epochs. You can use the plotting as verification that a model is actually training, but you should not attempt to fully train a model with the plotting enabled. Downloading and compressing the images can take quite a few minutes. Training the model takes up to 9 hours to complete 10 epochs on the dual GPU machines. Due to the way MATLAB trains a network with this depth (20 convolutional layers by default), these machines run out memory when training the network past 12 epochs with the rest of the settings left unchanged. One way to reduce memory usage and training time is top use fewer compressed images in the training set. Another option is to use a different layer setup with fewer hidden layers.","title":"Train Interactively"},{"location":"MATLAB_Deep_Learning_on_Xena/#train-cnn-via-scheduled-job","text":"Since training this CNN can take many hours, it is a good idea to take advantage of the Slurm scheduler. Interactive jobs can be stopped and interrupted due to internet connection issues. In the options of the CNN itself we set Verbose=true , which will allow us to see the status of the model being trained in an ouput file that is updated in real time.","title":"Train CNN via Scheduled Job"},{"location":"MATLAB_Deep_Learning_on_Xena/#slurm-scripts","text":"It is reccomended that you use the dual GPU machines. When using either of the scripts below, ensure the ExecutionEnvironment option in the above MATLAB script matches your choice of machine.","title":"Slurm Scripts"},{"location":"MATLAB_Deep_Learning_on_Xena/#single-gpu","text":"To request a job with a single GPU, create the following script with the name 'dncnn_single_gpu.sh': #!/bin/bash #SBATCH --job-name DnCNN_singleGPU #SBATCH --mail-user jmccullough12@unm.edu #SBATCH --mail-type ALL #SBATCH --output dncnn_single_gpu.out #SBATCH --error dncnn_single_gpu.err #SBATCH --time 48:00:00 #SBATCH --partition singleGPU #SBATCH --ntasks 1 #SBATCH --mem 0 #SBATCH --cpus-per-task 16 #SBATCH -G 1 cd ~/deepLearningExample module load matlab matlab -nodisplay -r deep_learning_example > dncnn_single_training.out","title":"Single GPU"},{"location":"MATLAB_Deep_Learning_on_Xena/#dual-gpu","text":"To request a job with dual GPUs, create the following script with the name 'dncnn_dual_gpu.sh': #!/bin/bash #SBATCH --job-name DnCNN_DualGPU #SBATCH --mail-user <YOUR EMAIL> #SBATCH --mail-type ALL #SBATCH --output dncnn_dual_gpu.out #SBATCH --error dncnn_dual_gpu.err #SBATCH --time 48:00:00 #SBATCH --partition dualGPU #SBATCH --ntasks 1 #SBATCH --mem 0 #SBATCH --cpus-per-task 16 #SBATCH -G 2 cd ~/deepLearningExample module load matlab matlab -nodisplay -r deep_learning_example > dncnn_dual_training.out","title":"Dual GPU"},{"location":"MATLAB_Deep_Learning_on_Xena/#submit-script","text":"To submit a job request using the single GPU script, use the following command: xena:~$ sbatch dncnn_single_gpu.sh To submit a job request using the dual GPU script, use the following command: xena:~$ sbatch dncnn_dual_gpu.sh","title":"Submit Script"},{"location":"MATLAB_Deep_Learning_on_Xena/#view-results","text":"While a network is being trained, you can see the results in real time with the cat command. If you used the single GPU slurm script, use this command to view the output: xena:~$ cat ~/deepLearningExamples/dncnn_single_training.out If you used the dual GPU slurm script, use this command to view the output: xena:~$ cat ~/deepLearningExamples/dncnn_dual_training.out","title":"View Results"},{"location":"MATLAB_Deep_Learning_on_Xena/#test-the-model","text":"Once a model has finished training, it will be saved to a .mat file with a name like: trainedJPEGDnCNNyyyy-MM-dd-HH-mm-ss.mat You can also use the pretrained example model (see the above instructions to get the example MATALB function files).","title":"Test the Model"},{"location":"MATLAB_Deep_Learning_on_Xena/#matlab-script","text":"Use the following MATLAB Script to the results of a trianed model. To specify which model to use, replace the <MODEL FILE> with the name of your model. If the script is run interactively with X11 fowarding (see above instructions), an image will appear that shows the predictions made on a test image,. The resulting comparisons are also saved to an imaged titled 'results.tif' which can be viewed with your preferred image viewer. Create the following script with the name 'test_model.m' and ensure it lives in the 'deepLearningExample' directory. % Open and test results of trained CNN model for JPEG Denoising % Load the model from file load(\"trainedJPEGDnCNN-2022-05-23-00-37-23.mat\"); % Open test images fileNames = [\"sherlock.jpg\",\"peacock.jpg\",\"fabric.png\",\"greens.jpg\", ... \"hands1.jpg\",\"kobi.png\",\"lighthouse.png\",\"office_4.jpg\", ... \"onion.png\",\"pears.png\",\"yellowlily.jpg\",\"indiancorn.jpg\", ... \"flamingos.jpg\",\"sevilla.jpg\",\"llama.jpg\",\"parkavenue.jpg\", ... \"strawberries.jpg\",\"trailer.jpg\",\"wagon.jpg\",\"football.jpg\"]; filePath = fullfile(matlabroot,\"toolbox\",\"images\",\"imdata\")+filesep; filePathNames = strcat(filePath,fileNames); testImages = imageDatastore(filePathNames); % Select an image to view - choose an image name from the above list. testImage = \"lighthouse.png\"; Ireference = imread(testImage); % Compress the test image in three different levels of quality imwrite(Ireference,fullfile(tempdir,\"testQuality10.jpg\"),\"Quality\",10); imwrite(Ireference,fullfile(tempdir,\"testQuality20.jpg\"),\"Quality\",20); imwrite(Ireference,fullfile(tempdir,\"testQuality50.jpg\"),\"Quality\",50); I10 = imread(fullfile(tempdir,\"testQuality10.jpg\")); I20 = imread(fullfile(tempdir,\"testQuality20.jpg\")); I50 = imread(fullfile(tempdir,\"testQuality50.jpg\")); I10ycbcr = rgb2ycbcr(I10); I20ycbcr = rgb2ycbcr(I20); I50ycbcr = rgb2ycbcr(I50); % Apply network to compressed test images I10y_predicted = denoiseImage(I10ycbcr(:,:,1),net); I20y_predicted = denoiseImage(I20ycbcr(:,:,1),net); I50y_predicted = denoiseImage(I50ycbcr(:,:,1),net); I10ycbcr_predicted = cat(3,I10y_predicted,I10ycbcr(:,:,2:3)); I20ycbcr_predicted = cat(3,I20y_predicted,I20ycbcr(:,:,2:3)); I50ycbcr_predicted = cat(3,I50y_predicted,I50ycbcr(:,:,2:3)); I10_predicted = ycbcr2rgb(I10ycbcr_predicted); I20_predicted = ycbcr2rgb(I20ycbcr_predicted); I50_predicted = ycbcr2rgb(I50ycbcr_predicted); % View and save results montage({I50,I20,I10,I50_predicted,I20_predicted,I10_predicted},Size=[2 3]) title(\"Compressed Images (above) Compared to Deblocked Images (below) with Quality Factor 50, 20 and 10 (Left to Right)\") imwrite(getframe(gca).cdata,'results.tif','tif'); % Change the following boolean to look closer at a specific region of % interes in the test image doROI = false; if doROI roi = [30 440 100 80]; i10 = imcrop(I10,roi); i20 = imcrop(I20,roi); i50 = imcrop(I50,roi); i10predicted = imcrop(I10_predicted,roi); i20predicted = imcrop(I20_predicted,roi); i50predicted = imcrop(I50_predicted,roi); montage({i50,i20,i10,i50predicted,i20predicted,i10predicted},Size=[2 3]) title(\"Compressed Images ROI (above) Compared to Deblocked Images ROI (below) with Quality Factor 50, 20 and 10 (Left to Right)\") imwrite(getframe(gca).cdata,'results_roi.tif','tif'); end","title":"MATLAB Script"},{"location":"MATLAB_Deep_Learning_on_Xena/#changes-you-can-make-to-the-test_modelm-script","text":"The above script can be modified for two different kinds of functionality: Use a different test image Zoom in on a specific region of interest (ROI)","title":"Changes you can make to the test_model.m script"},{"location":"MATLAB_Deep_Learning_on_Xena/#different-test-image","text":"You can change the test image by changing the line: testImage = \"lighthouse.png\"; to any of the images in the list of filenames, directly above in the script.","title":"Different test image"},{"location":"MATLAB_Deep_Learning_on_Xena/#zoom-in-on-specific-roi","text":"To zoom in on a specific region of interest, change the following line: doROI = false; to doROI = true; Then ROI can be changed by modifying this line: roi = [30 440 100 80]; This region works well when using lighthouse.png as your test image. Running the script with the boolean changed will display and save a image of the results zoomed in on the ROI. If you are using X11 forwarding, the image should appear on your display. The results are also saved to an image ('results_roi.tif') that can be viewed in your choice of image viewer.","title":"Zoom in on specific ROI"},{"location":"Metabarcoding/","text":"Metabarcoding Tutorial Metabarcoding is the process of using next-generation sequencing platforms (Illumina, PacBio, or Oxford Nanopore) to sequence amplicons and determine the ecological community that is present. The most common applications are microbiome analyses to study the community of bacteria or fungi. Given that metabarcoding relies upon PCR, biases do occur (e.g. primer biases, variation in loci copy number, incomplete lineage sorting, etc.). However, other techniques such as metagenomic sequencing cannot fully assemble larger genomes such as fungal genomes and fail to capture all of the species present in high diversity samples. Thus, metabarcoding remains the best option to characterize microbial communities. This tutorial is designed to give you an example of how to take the sequences you get from the sequencing facility and generate: a fasta file of all of the unique sequences known as the representative sequence file (abbreviated to rep-seq) a table with the abundance of each of the representative sequences for every sample. Often referred to as an OTU/ASV table. One point of contention for metabarcoding projects is how to define which sequences are unique. The traditional view was to cluster any sequences that diverged by less than 3% of sequence similarity into a species known as an OTU (operational taxon unit). However, other people reject the clustering step as being arbitrary and define any divergence as noteworthy. This approach is known as ASVs, (amplicon sequence variants). While the debate between clustering to OTUs or using ASVs remains contentious, for most community analyses, each will give you the same biological answer. The right choice will depend on your question and your system. For example, fungal metabarcoding studies use a highly variable region called the internal transcribed spacer (ITS) which is known to diverge by 3% within an individual (they have many copies of the ITS region) and among members of the same species. Thus, clustering to OTUs is more logical for fungal taxa to avoid oversplitting species. However, if you were using a conserved gene such as the 18S, ASV's might give you a better approximation of species. Final note: make sure you are runing the commands in a pbs script or on an interactive node! Different pipelines There are many different pipelines to process metabarcoding samples. For this tutorial we will focus on the three main ones: 1. QIIME2 (using DADA2) 2. Mothur 3. USEARCH Steps for each pipeline For eachof them, they will follow these key steps: 1. install * how to set up the environments. 3. join forward and reverse reads * Illumina sequencing produces forward and reverse reads for every sequence. 5. filter reads (remove chimeras) * The merged reads will need to be trimmed of extraneous sequence, poor quality sequences will need be removed, and chimera, the generation of DNA sequences from disparate organisms due to errors in the PCR process, will also need to be removed. 7. create OTUs/ASVs * The unique sequences will be determined based on the chosen algorithm. 9. Creation of OTU/ASV table * The abundances of the OTUs or ASVs will be tabulated per sample to create the table. 11. determine Taxonomy for OTUs/ASVs * The taxonomy of each of the OTUs/ASVs will be inferred by comparing the sequences against commonly used databases. Data for tutorial For each of the pipelines we will use 16S bacterial of the V4 region provided by the Mothur pipeline. create folder structure mkdir metabarcoding cd metabarcoding src=~/metabarcoding mkdir data cd data # dataset is called miseqsopdata wget https://mothur.s3.us-east-2.amazonaws.com/wiki/miseqsopdata.zip unzip miseqsopdata.zip cd miseqsopdata # gzip all fastq files to save space. gzip *fa # move back to main folder cd $src QIIME2 (using DADA2) pipeline For QIIME2, every file created is either uses a .qsv or .qsa extension. the .qsv is a zip file that contains data and the metadata. The .qsv are visualizations that can be viewed by uploading the file to https://view.qiime2.org/. QIIME2 prefers to create ASVs using the DADA2 method, so this tutorial will do that. install We will create a conda environment called qiime2-2021.4. # To install QIIME2 on the Wheeler, Xena, or Hopper clusters, use the following command: module load miniconda3 # To do a similar installation on the Taos cluster, use the following command instead: module load miniconda3-4.10.3-gcc-10.2.0-gu6ytpa #download the yml wget https://data.qiime2.org/distro/core/qiime2-2021.4-py38-linux-conda.yml # create conda environment called qiime2-2020.8 conda env create -n qiime2-2021.4 --file qiime2-2021.4-py38-linux-conda.yml # delete yml rm qiime2-2020.8-py36-linux-conda.yml join forward and reverse reads conda activate qiime2-2021.4 cd $scr mkdir qiime2_tutorial cd qiime2_tutorial # merge reads qiime tools import \\ --type 'SampleData[PairedEndSequencesWithQuality]' \\ --input-path $src/data/MiSeq_SOP/fastqs/ \\ --input-format CasavaOneEightSingleLanePerSampleDirFmt \\ --output-path $src/qiime2_tutorial/demux-paired-end.qza # summary figures online # open visual at this link: https://view.qiime2.org/ qiime demux summarize \\ --i-data $src/qiime2_tutorial/demux-paired-end.qza \\ --o-visualization $src/qiime2_tutorial/demux.qzv filter reads (remove chimeras) and create ASVs This step does all of the filtering and creation of ASVs at once. # following what we see in the visualization we will trim the reads and denoise the reads. # this will also create the rep seq qiime file wiht the ASVs. # This method denoises paired-end sequences, dereplicates them, and filters chimeras. qiime dada2 denoise-paired \\ --i-demultiplexed-seqs $src/qiime2_tutorial/demux-paired-end.qza \\ --p-trunc-len-f 240 \\ --p-trunc-len-r 160 \\ --o-representative-sequences $src/qiime2_tutorial/rep-seqs-dada2.qza \\ --o-table $src/qiime2_tutorial/table-dada2.qza \\ --o-denoising-stats $src/qiime2_tutorial/stats-dada2.qza \\ --p-n-threads 0 # use all available cores # create rep-seq file. qiime feature-table tabulate-seqs \\ --i-data $src/qiime2_tutorial/rep-seqs-dada2.qza \\ --o-visualization $src/qiime2_tutorial/rep-seqs.qzv # create summary table qiime feature-table summarize \\ --i-table $src/qiime2_tutorial/table-dada2.qza \\ --o-visualization $src/qiime2_tutorial/table.qzv Creation of ASV table # export rep seq sequences. # it is exported in the rep-seqs wiht qiime tools export \\ --input-path $src/qiime2_tutorial/rep-seqs-dada2.qza \\ --output-path $src/qiime2_tutorial/rep-seqs # creates OTU table qiime tools export \\ --input-path $src/qiime2_tutorial/table-dada2.qza \\ --output-path $src/qiime2_tutorial/exported-feature-table Mothur pipeline Mothur uses a unique syntax in which each command begins is structured liek this: mothur \"#command here(parameters_here=X)\". Mothur can do ASVs but the preference of the its creator is to use OTUS so we will do that here. install # load miniconda module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik conda env create -n mothur conda install -n mothur -c bioconda mothur join forward and reverse reads conda activate mothur cd $src mkdir mothur_tutorial cd mothur_tutorial # copy files into mothur folder scp $src/data/MiSeq_SOP/fastqs/*gz . # create a list of the files # output # 1. stability.files mothur \"#make.file(inputdir=., type=gz, prefix=stability)\" # join forward and reverse reads. Gives you a count of reads of how many reads are assembled for each sample # output # 1. stability.trim.contigs.fasta # 2. stability.scrap.contigs.fasta # 3. stability.contigs.report # 4. stability.contigs.groups mothur \"#make.contigs(file=stability.files, processors=8)\" # summary stats on the merged reads mothur \"#summary.seqs(fasta=stability.trim.contigs.fasta)\" filter reads, remove primer region, and remove chimeras # removes reads that are longer than 275 bases. Likely to be errors # output files # 1. stability.contigs.pick.groups # 2. stability.trim.contigs.good.fasta # 3. stability.trim.contigs.bad.accnos # 4. stability.contigs.good.groups mothur \"#screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275)\" # find unique sequences in dataset. This is to reduce the size of the dataset and reduce redundancies. # output files # 1.stability.trim.contigs.good.names 2. stability.trim.contigs.good.unique.fasta mothur \"#unique.seqs(fasta=stability.trim.contigs.good.fasta)\" # count up how many reads are match the good names. # output file # 1. stability.trim.contigs.good.count_table mothur \"#count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups)\" # now going to trim unique reads to silva dataset. # download reference database to trim reads wget https://mothur.s3.us-east-2.amazonaws.com/wiki/silva.bacteria.zip unzip silva.bacteria.zip # trim to the V4 variable region of the silva bacteria dataset. This is the reigon we sequenced # output file # 1. silva.bacteria/silva.bacteria.pcr.fasta mothur \"#pcr.seqs(fasta=silva.bacteria/silva.bacteria.fasta, start=11894, end=25319, keepdots=F, processors=8)\" # align unique reads to Silva reference # output files. # 1. stability.trim.contigs.good.unique.align # 2. stability.trim.contigs.good.unique.align.report mothur \"#align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.bacteria/silva.bacteria.pcr.fasta)\" # summary of where the sequences align # output file # 1. stability.trim.contigs.good.unique.summary mothur \"#summary.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table)\" # trim sequences that extend beyond the Silva alignment (overhangs) and remove gap only columns. # output file # 1. stability.filter # 2. stability.trim.contigs.good.unique.filter.fasta mothur \"#filter.seqs(fasta=stability.trim.contigs.good.unique.align, vertical=T, trump=.)\" # find all unique sequences again in case there are some sequences that are now identical. # creates: # 1. stability.trim.contigs.good.unique.filter.count_table # 2. stability.trim.contigs.good.unique.filter.unique.fasta mothur \"#unique.seqs(fasta=stability.trim.contigs.good.unique.filter.fasta, count=stability.trim.contigs.good.count_table)\" # pre-clustering to clean up sequencing errors. Differences of two nucleotides will be clustered. # creates an output file for each sample so it can create a lot of files. # output files # 1. stability.trim.contigs.good.unique.filter.unique.precluster.fasta # 2. stability.trim.contigs.good.unique.filter.unique.precluster.count_table # 3. stability.trim.contigs.good.unique.filter.unique.precluster.F3D0.map # 4. stability.trim.contigs.good.unique.filter.unique.precluster.F3D1.map # 5. stability.trim.contigs.good.unique.filter.unique.precluster.F3D141.map # 6. stability.trim.contigs.good.unique.filter.unique.precluster.F3D142.map # 7. stability.trim.contigs.good.unique.filter.unique.precluster.F3D143.map # 8. stability.trim.contigs.good.unique.filter.unique.precluster.F3D144.map # 9. stability.trim.contigs.good.unique.filter.unique.precluster.F3D145.map # 10. stability.trim.contigs.good.unique.filter.unique.precluster.F3D146.map # 11. stability.trim.contigs.good.unique.filter.unique.precluster.F3D147.map # 12. stability.trim.contigs.good.unique.filter.unique.precluster.F3D148.map # 13. stability.trim.contigs.good.unique.filter.unique.precluster.F3D149.map # 14. stability.trim.contigs.good.unique.filter.unique.precluster.F3D150.map # 15. stability.trim.contigs.good.unique.filter.unique.precluster.F3D2.map # 16. stability.trim.contigs.good.unique.filter.unique.precluster.F3D3.map # 17. stability.trim.contigs.good.unique.filter.unique.precluster.F3D5.map # 18. stability.trim.contigs.good.unique.filter.unique.precluster.F3D6.map # 19. stability.trim.contigs.good.unique.filter.unique.precluster.F3D7.map # 20. stability.trim.contigs.good.unique.filter.unique.precluster.F3D8.map # 21. stability.trim.contigs.good.unique.filter.unique.precluster.F3D9.map # 22. stability.trim.contigs.good.unique.filter.unique.precluster.Mock.map mothur \"#pre.cluster(fasta=stability.trim.contigs.good.unique.filter.unique.fasta, count=stability.trim.contigs.good.unique.filter.count_table, diffs=2)\" # remove chimeras Output files: # 1. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table # 2. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.chimeras # 3. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.accnos mothur \"#chimera.vsearch(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.fasta, count=stability.trim.contigs.good.unique.filter.unique.precluster.count_table, dereplicate=t)\" # remove chimeras from fasta # output file # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta mothur \"#remove.seqs(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.fasta, accnos=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.accnos)\" # downloading files to classifiy sequences and remove non-bacterial reads wget https://mothur.s3.us-east-2.amazonaws.com/wiki/trainset18_062020.pds.tgz tar zxvf trainset18_062020.pds.tgz # remove sequences that are not bacteria. # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.tax.summary mothur \"#classify.seqs(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, reference=trainset18_062020.pds/trainset18_062020.pds.fasta, taxonomy=trainset18_062020.pds/trainset18_062020.pds.tax, cutoff=80)\" # removing lineages here # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.pick.taxonomy # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.accnos # 3. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table # 4. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.fasta mothur \"#remove.lineage(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, taxonomy=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota)\" # get summary of taxonomy by counts mothur \"#summary.tax(taxonomy=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.pick.taxonomy, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table)\" create OTUs # create file of distances between potential OTUs # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.dist mothur \"#dist.seqs(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.fasta, cutoff=0.03)\" # create OTUs at 97% similarity. # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.steps # 3. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.sensspec mothur \"#cluster(column=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.dist, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table)\" # Creates OTU table with only OTUs # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.shared mothur \"#make.shared(list=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table, label=0.03)\" # create taxonomy for each OTU # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.cons.taxonomy # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.cons.tax.summary mothur \"#classify.otu(list=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.pick.taxonomy, label=0.03)\" # get representative sequences files # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.rep.names # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.rep.fasta mothur \"#get.oturep(column=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.dist, list=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list, name=stability.trim.contigs.good.names, fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta)\" USEARCH USEARCH is a proprietary software developed by Robert Edgar. For this tutorial, we will use the free, 32-bit version. However, if you have multiple sequencing runs, you will likely need to upgrade to the paid, 64-bit version. USEARCH can do OTUs or ASVs. For this example I stuck with OTUs. Many of the commands can easily be changed to VSEARCH, an alternative that is open-source. install cd $src mkdir usearch_tutorial cd usearch_tutorial # download the program wget https://www.drive5.com/downloads/usearch11.0.667_i86linux32.gz gunzip usearch11.0.667_i86linux32 # change permissions so you can run it chmod +x usearch11.0.667_i86linux32 # rename so its shorter mv usearch11.0.667_i86linux32 usearch11.0.667 ``` ### join forward and reverse reads ### ``` # need to unzip all the files first cd $src/data/MiSeq_SOP/fastqs gunzip *gz cd $src/usearch_tutorial # merges forward and reverse reads. ./usearch11.0.667 -fastq_mergepairs $src/data/MiSeq_SOP/fastqs/*R1*.fastq -fastqout merged.fq -relabel @ # we can use this step to guide how we will filter the reads. # this will tell us the size of the reads and the error rate. # given the error rate is well below 0.5 and most reaads are above 250 bp, i will use that and 250 as the shortest size for length. ./usearch11.0.667 -fastx_info merged.fq -secs 5 -output reads_info.txt # filter reads based off the suggestions above. This creates a fasta file which can be used for clustering ./usearch11.0.667 -fastq_filter merged.fq -fastaout reads.fasta -fastq_maxee 0.5 -fastq_minlen 250 Create OTUs #### # dereplicate sequences so we only have unique reads. # creates file with uniques.fasta which each read relabeled as uniq ./usearch11.0.667 -fastx_uniques reads.fasta -fastaout uniques.fasta -sizeout -relabel Uniq # creates otus at 97% and removes chimeras via UPARSE # warning: this will throw out singletons so change -minsize to 1. # creates uparse.txt - log of how the clustering went for each unique read. # creates otus.fasta - representative OTUs. ./usearch11.0.667 -cluster_otus uniques.fasta -otus otus.fasta -uparseout uparse.txt -relabel Otu -minsize 2 create OTU table ``` map back reads and create OTU table the otutab.txt is the OTU table the map.txt shows you how each read is mapped to what OTU ./usearch11.0.667 -otutab merged.fq -otus otus.fasta -otutabout otutab.txt -mapout map.txt ### determine Taxonomy for OTUs ### downloaad rdp database wget https://drive5.com/sintax/rdp_16s_v16.fa.gz gunzip rdp_16s_v16.fa.gz Creates taxonomy file with OTU and the taxonomy in the reads.sintax file ./usearch11.0.667 -sintax otus.fasta -db rdp_16s_v16.fa -tabbedout reads.sintax -strand both -sintax_cutoff 0.8 ```","title":"Metabarcoding with QIIME2, Mothur, and USEARCH"},{"location":"Metabarcoding/#metabarcoding-tutorial","text":"Metabarcoding is the process of using next-generation sequencing platforms (Illumina, PacBio, or Oxford Nanopore) to sequence amplicons and determine the ecological community that is present. The most common applications are microbiome analyses to study the community of bacteria or fungi. Given that metabarcoding relies upon PCR, biases do occur (e.g. primer biases, variation in loci copy number, incomplete lineage sorting, etc.). However, other techniques such as metagenomic sequencing cannot fully assemble larger genomes such as fungal genomes and fail to capture all of the species present in high diversity samples. Thus, metabarcoding remains the best option to characterize microbial communities. This tutorial is designed to give you an example of how to take the sequences you get from the sequencing facility and generate: a fasta file of all of the unique sequences known as the representative sequence file (abbreviated to rep-seq) a table with the abundance of each of the representative sequences for every sample. Often referred to as an OTU/ASV table. One point of contention for metabarcoding projects is how to define which sequences are unique. The traditional view was to cluster any sequences that diverged by less than 3% of sequence similarity into a species known as an OTU (operational taxon unit). However, other people reject the clustering step as being arbitrary and define any divergence as noteworthy. This approach is known as ASVs, (amplicon sequence variants). While the debate between clustering to OTUs or using ASVs remains contentious, for most community analyses, each will give you the same biological answer. The right choice will depend on your question and your system. For example, fungal metabarcoding studies use a highly variable region called the internal transcribed spacer (ITS) which is known to diverge by 3% within an individual (they have many copies of the ITS region) and among members of the same species. Thus, clustering to OTUs is more logical for fungal taxa to avoid oversplitting species. However, if you were using a conserved gene such as the 18S, ASV's might give you a better approximation of species. Final note: make sure you are runing the commands in a pbs script or on an interactive node!","title":"Metabarcoding Tutorial"},{"location":"Metabarcoding/#different-pipelines","text":"There are many different pipelines to process metabarcoding samples. For this tutorial we will focus on the three main ones: 1. QIIME2 (using DADA2) 2. Mothur 3. USEARCH","title":"Different pipelines"},{"location":"Metabarcoding/#steps-for-each-pipeline","text":"For eachof them, they will follow these key steps: 1. install * how to set up the environments. 3. join forward and reverse reads * Illumina sequencing produces forward and reverse reads for every sequence. 5. filter reads (remove chimeras) * The merged reads will need to be trimmed of extraneous sequence, poor quality sequences will need be removed, and chimera, the generation of DNA sequences from disparate organisms due to errors in the PCR process, will also need to be removed. 7. create OTUs/ASVs * The unique sequences will be determined based on the chosen algorithm. 9. Creation of OTU/ASV table * The abundances of the OTUs or ASVs will be tabulated per sample to create the table. 11. determine Taxonomy for OTUs/ASVs * The taxonomy of each of the OTUs/ASVs will be inferred by comparing the sequences against commonly used databases.","title":"Steps for each pipeline"},{"location":"Metabarcoding/#data-for-tutorial","text":"For each of the pipelines we will use 16S bacterial of the V4 region provided by the Mothur pipeline.","title":"Data for tutorial"},{"location":"Metabarcoding/#create-folder-structure","text":"mkdir metabarcoding cd metabarcoding src=~/metabarcoding mkdir data cd data # dataset is called miseqsopdata wget https://mothur.s3.us-east-2.amazonaws.com/wiki/miseqsopdata.zip unzip miseqsopdata.zip cd miseqsopdata # gzip all fastq files to save space. gzip *fa # move back to main folder cd $src","title":"create folder structure"},{"location":"Metabarcoding/#qiime2-using-dada2-pipeline","text":"For QIIME2, every file created is either uses a .qsv or .qsa extension. the .qsv is a zip file that contains data and the metadata. The .qsv are visualizations that can be viewed by uploading the file to https://view.qiime2.org/. QIIME2 prefers to create ASVs using the DADA2 method, so this tutorial will do that.","title":"QIIME2 (using DADA2) pipeline"},{"location":"Metabarcoding/#install","text":"We will create a conda environment called qiime2-2021.4. # To install QIIME2 on the Wheeler, Xena, or Hopper clusters, use the following command: module load miniconda3 # To do a similar installation on the Taos cluster, use the following command instead: module load miniconda3-4.10.3-gcc-10.2.0-gu6ytpa #download the yml wget https://data.qiime2.org/distro/core/qiime2-2021.4-py38-linux-conda.yml # create conda environment called qiime2-2020.8 conda env create -n qiime2-2021.4 --file qiime2-2021.4-py38-linux-conda.yml # delete yml rm qiime2-2020.8-py36-linux-conda.yml","title":"install"},{"location":"Metabarcoding/#join-forward-and-reverse-reads","text":"conda activate qiime2-2021.4 cd $scr mkdir qiime2_tutorial cd qiime2_tutorial # merge reads qiime tools import \\ --type 'SampleData[PairedEndSequencesWithQuality]' \\ --input-path $src/data/MiSeq_SOP/fastqs/ \\ --input-format CasavaOneEightSingleLanePerSampleDirFmt \\ --output-path $src/qiime2_tutorial/demux-paired-end.qza # summary figures online # open visual at this link: https://view.qiime2.org/ qiime demux summarize \\ --i-data $src/qiime2_tutorial/demux-paired-end.qza \\ --o-visualization $src/qiime2_tutorial/demux.qzv","title":"join forward and reverse reads"},{"location":"Metabarcoding/#filter-reads-remove-chimeras-and-create-asvs","text":"This step does all of the filtering and creation of ASVs at once. # following what we see in the visualization we will trim the reads and denoise the reads. # this will also create the rep seq qiime file wiht the ASVs. # This method denoises paired-end sequences, dereplicates them, and filters chimeras. qiime dada2 denoise-paired \\ --i-demultiplexed-seqs $src/qiime2_tutorial/demux-paired-end.qza \\ --p-trunc-len-f 240 \\ --p-trunc-len-r 160 \\ --o-representative-sequences $src/qiime2_tutorial/rep-seqs-dada2.qza \\ --o-table $src/qiime2_tutorial/table-dada2.qza \\ --o-denoising-stats $src/qiime2_tutorial/stats-dada2.qza \\ --p-n-threads 0 # use all available cores # create rep-seq file. qiime feature-table tabulate-seqs \\ --i-data $src/qiime2_tutorial/rep-seqs-dada2.qza \\ --o-visualization $src/qiime2_tutorial/rep-seqs.qzv # create summary table qiime feature-table summarize \\ --i-table $src/qiime2_tutorial/table-dada2.qza \\ --o-visualization $src/qiime2_tutorial/table.qzv","title":"filter reads (remove chimeras) and create ASVs"},{"location":"Metabarcoding/#creation-of-asv-table","text":"# export rep seq sequences. # it is exported in the rep-seqs wiht qiime tools export \\ --input-path $src/qiime2_tutorial/rep-seqs-dada2.qza \\ --output-path $src/qiime2_tutorial/rep-seqs # creates OTU table qiime tools export \\ --input-path $src/qiime2_tutorial/table-dada2.qza \\ --output-path $src/qiime2_tutorial/exported-feature-table","title":"Creation of ASV table"},{"location":"Metabarcoding/#mothur-pipeline","text":"Mothur uses a unique syntax in which each command begins is structured liek this: mothur \"#command here(parameters_here=X)\". Mothur can do ASVs but the preference of the its creator is to use OTUS so we will do that here.","title":"Mothur pipeline"},{"location":"Metabarcoding/#install_1","text":"# load miniconda module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik conda env create -n mothur conda install -n mothur -c bioconda mothur","title":"install"},{"location":"Metabarcoding/#join-forward-and-reverse-reads_1","text":"conda activate mothur cd $src mkdir mothur_tutorial cd mothur_tutorial # copy files into mothur folder scp $src/data/MiSeq_SOP/fastqs/*gz . # create a list of the files # output # 1. stability.files mothur \"#make.file(inputdir=., type=gz, prefix=stability)\" # join forward and reverse reads. Gives you a count of reads of how many reads are assembled for each sample # output # 1. stability.trim.contigs.fasta # 2. stability.scrap.contigs.fasta # 3. stability.contigs.report # 4. stability.contigs.groups mothur \"#make.contigs(file=stability.files, processors=8)\" # summary stats on the merged reads mothur \"#summary.seqs(fasta=stability.trim.contigs.fasta)\"","title":"join forward and reverse reads"},{"location":"Metabarcoding/#filter-reads-remove-primer-region-and-remove-chimeras","text":"# removes reads that are longer than 275 bases. Likely to be errors # output files # 1. stability.contigs.pick.groups # 2. stability.trim.contigs.good.fasta # 3. stability.trim.contigs.bad.accnos # 4. stability.contigs.good.groups mothur \"#screen.seqs(fasta=stability.trim.contigs.fasta, group=stability.contigs.groups, maxambig=0, maxlength=275)\" # find unique sequences in dataset. This is to reduce the size of the dataset and reduce redundancies. # output files # 1.stability.trim.contigs.good.names 2. stability.trim.contigs.good.unique.fasta mothur \"#unique.seqs(fasta=stability.trim.contigs.good.fasta)\" # count up how many reads are match the good names. # output file # 1. stability.trim.contigs.good.count_table mothur \"#count.seqs(name=stability.trim.contigs.good.names, group=stability.contigs.good.groups)\" # now going to trim unique reads to silva dataset. # download reference database to trim reads wget https://mothur.s3.us-east-2.amazonaws.com/wiki/silva.bacteria.zip unzip silva.bacteria.zip # trim to the V4 variable region of the silva bacteria dataset. This is the reigon we sequenced # output file # 1. silva.bacteria/silva.bacteria.pcr.fasta mothur \"#pcr.seqs(fasta=silva.bacteria/silva.bacteria.fasta, start=11894, end=25319, keepdots=F, processors=8)\" # align unique reads to Silva reference # output files. # 1. stability.trim.contigs.good.unique.align # 2. stability.trim.contigs.good.unique.align.report mothur \"#align.seqs(fasta=stability.trim.contigs.good.unique.fasta, reference=silva.bacteria/silva.bacteria.pcr.fasta)\" # summary of where the sequences align # output file # 1. stability.trim.contigs.good.unique.summary mothur \"#summary.seqs(fasta=stability.trim.contigs.good.unique.align, count=stability.trim.contigs.good.count_table)\" # trim sequences that extend beyond the Silva alignment (overhangs) and remove gap only columns. # output file # 1. stability.filter # 2. stability.trim.contigs.good.unique.filter.fasta mothur \"#filter.seqs(fasta=stability.trim.contigs.good.unique.align, vertical=T, trump=.)\" # find all unique sequences again in case there are some sequences that are now identical. # creates: # 1. stability.trim.contigs.good.unique.filter.count_table # 2. stability.trim.contigs.good.unique.filter.unique.fasta mothur \"#unique.seqs(fasta=stability.trim.contigs.good.unique.filter.fasta, count=stability.trim.contigs.good.count_table)\" # pre-clustering to clean up sequencing errors. Differences of two nucleotides will be clustered. # creates an output file for each sample so it can create a lot of files. # output files # 1. stability.trim.contigs.good.unique.filter.unique.precluster.fasta # 2. stability.trim.contigs.good.unique.filter.unique.precluster.count_table # 3. stability.trim.contigs.good.unique.filter.unique.precluster.F3D0.map # 4. stability.trim.contigs.good.unique.filter.unique.precluster.F3D1.map # 5. stability.trim.contigs.good.unique.filter.unique.precluster.F3D141.map # 6. stability.trim.contigs.good.unique.filter.unique.precluster.F3D142.map # 7. stability.trim.contigs.good.unique.filter.unique.precluster.F3D143.map # 8. stability.trim.contigs.good.unique.filter.unique.precluster.F3D144.map # 9. stability.trim.contigs.good.unique.filter.unique.precluster.F3D145.map # 10. stability.trim.contigs.good.unique.filter.unique.precluster.F3D146.map # 11. stability.trim.contigs.good.unique.filter.unique.precluster.F3D147.map # 12. stability.trim.contigs.good.unique.filter.unique.precluster.F3D148.map # 13. stability.trim.contigs.good.unique.filter.unique.precluster.F3D149.map # 14. stability.trim.contigs.good.unique.filter.unique.precluster.F3D150.map # 15. stability.trim.contigs.good.unique.filter.unique.precluster.F3D2.map # 16. stability.trim.contigs.good.unique.filter.unique.precluster.F3D3.map # 17. stability.trim.contigs.good.unique.filter.unique.precluster.F3D5.map # 18. stability.trim.contigs.good.unique.filter.unique.precluster.F3D6.map # 19. stability.trim.contigs.good.unique.filter.unique.precluster.F3D7.map # 20. stability.trim.contigs.good.unique.filter.unique.precluster.F3D8.map # 21. stability.trim.contigs.good.unique.filter.unique.precluster.F3D9.map # 22. stability.trim.contigs.good.unique.filter.unique.precluster.Mock.map mothur \"#pre.cluster(fasta=stability.trim.contigs.good.unique.filter.unique.fasta, count=stability.trim.contigs.good.unique.filter.count_table, diffs=2)\" # remove chimeras Output files: # 1. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table # 2. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.chimeras # 3. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.accnos mothur \"#chimera.vsearch(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.fasta, count=stability.trim.contigs.good.unique.filter.unique.precluster.count_table, dereplicate=t)\" # remove chimeras from fasta # output file # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta mothur \"#remove.seqs(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.fasta, accnos=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.accnos)\" # downloading files to classifiy sequences and remove non-bacterial reads wget https://mothur.s3.us-east-2.amazonaws.com/wiki/trainset18_062020.pds.tgz tar zxvf trainset18_062020.pds.tgz # remove sequences that are not bacteria. # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.tax.summary mothur \"#classify.seqs(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, reference=trainset18_062020.pds/trainset18_062020.pds.fasta, taxonomy=trainset18_062020.pds/trainset18_062020.pds.tax, cutoff=80)\" # removing lineages here # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.pick.taxonomy # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.accnos # 3. stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table # 4. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.fasta mothur \"#remove.lineage(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, taxonomy=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota)\" # get summary of taxonomy by counts mothur \"#summary.tax(taxonomy=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.pick.taxonomy, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table)\"","title":"filter reads, remove primer region, and remove chimeras"},{"location":"Metabarcoding/#create-otus","text":"# create file of distances between potential OTUs # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.dist mothur \"#dist.seqs(fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.fasta, cutoff=0.03)\" # create OTUs at 97% similarity. # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.steps # 3. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.sensspec mothur \"#cluster(column=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.dist, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table)\" # Creates OTU table with only OTUs # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.shared mothur \"#make.shared(list=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table, label=0.03)\" # create taxonomy for each OTU # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.cons.taxonomy # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.cons.tax.summary mothur \"#classify.otu(list=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list, count=stability.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.pick.count_table, taxonomy=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.pick.taxonomy, label=0.03)\" # get representative sequences files # output # 1. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.rep.names # 2. stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.0.03.rep.fasta mothur \"#get.oturep(column=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.dist, list=stability.trim.contigs.good.unique.filter.unique.precluster.pick.pick.opti_mcc.list, name=stability.trim.contigs.good.names, fasta=stability.trim.contigs.good.unique.filter.unique.precluster.pick.fasta)\"","title":"create OTUs"},{"location":"Metabarcoding/#usearch","text":"USEARCH is a proprietary software developed by Robert Edgar. For this tutorial, we will use the free, 32-bit version. However, if you have multiple sequencing runs, you will likely need to upgrade to the paid, 64-bit version. USEARCH can do OTUs or ASVs. For this example I stuck with OTUs. Many of the commands can easily be changed to VSEARCH, an alternative that is open-source.","title":"USEARCH"},{"location":"Metabarcoding/#install_2","text":"cd $src mkdir usearch_tutorial cd usearch_tutorial # download the program wget https://www.drive5.com/downloads/usearch11.0.667_i86linux32.gz gunzip usearch11.0.667_i86linux32 # change permissions so you can run it chmod +x usearch11.0.667_i86linux32 # rename so its shorter mv usearch11.0.667_i86linux32 usearch11.0.667 ``` ### join forward and reverse reads ### ``` # need to unzip all the files first cd $src/data/MiSeq_SOP/fastqs gunzip *gz cd $src/usearch_tutorial # merges forward and reverse reads. ./usearch11.0.667 -fastq_mergepairs $src/data/MiSeq_SOP/fastqs/*R1*.fastq -fastqout merged.fq -relabel @ # we can use this step to guide how we will filter the reads. # this will tell us the size of the reads and the error rate. # given the error rate is well below 0.5 and most reaads are above 250 bp, i will use that and 250 as the shortest size for length. ./usearch11.0.667 -fastx_info merged.fq -secs 5 -output reads_info.txt # filter reads based off the suggestions above. This creates a fasta file which can be used for clustering ./usearch11.0.667 -fastq_filter merged.fq -fastaout reads.fasta -fastq_maxee 0.5 -fastq_minlen 250","title":"install"},{"location":"Metabarcoding/#create-otus_1","text":"# dereplicate sequences so we only have unique reads. # creates file with uniques.fasta which each read relabeled as uniq ./usearch11.0.667 -fastx_uniques reads.fasta -fastaout uniques.fasta -sizeout -relabel Uniq # creates otus at 97% and removes chimeras via UPARSE # warning: this will throw out singletons so change -minsize to 1. # creates uparse.txt - log of how the clustering went for each unique read. # creates otus.fasta - representative OTUs. ./usearch11.0.667 -cluster_otus uniques.fasta -otus otus.fasta -uparseout uparse.txt -relabel Otu -minsize 2","title":"Create OTUs ####"},{"location":"Metabarcoding/#create-otu-table","text":"```","title":"create OTU table"},{"location":"Metabarcoding/#map-back-reads-and-create-otu-table","text":"","title":"map back reads and create OTU table"},{"location":"Metabarcoding/#the-otutabtxt-is-the-otu-table","text":"","title":"the otutab.txt is the OTU table"},{"location":"Metabarcoding/#the-maptxt-shows-you-how-each-read-is-mapped-to-what-otu","text":"./usearch11.0.667 -otutab merged.fq -otus otus.fasta -otutabout otutab.txt -mapout map.txt ### determine Taxonomy for OTUs ###","title":"the map.txt shows you how each read is mapped to what OTU"},{"location":"Metabarcoding/#downloaad-rdp-database","text":"wget https://drive5.com/sintax/rdp_16s_v16.fa.gz gunzip rdp_16s_v16.fa.gz","title":"downloaad rdp database"},{"location":"Metabarcoding/#creates-taxonomy-file-with-otu-and-the-taxonomy-in-the-readssintax-file","text":"./usearch11.0.667 -sintax otus.fasta -db rdp_16s_v16.fa -tabbedout reads.sintax -strand both -sintax_cutoff 0.8 ```","title":"Creates taxonomy file with OTU and the taxonomy in the reads.sintax file"},{"location":"ParallelMatlabServer/","text":"Parallel MATLAB Server MATLAB supports parallelization on desktop computers which can be used to increase the speed of analysis drastically. MATLAB also provides the MATLAB Parallel Server (previously the MATLAB Distributed Computing Server) which allows you to write MATLAB code on your local desktop or laptop computer and perform the computation using the CARC high-performance clusters. This QuickByte leads you through the steps needed to set this up. If you run into problems please send an email to help@carc.unm.edu and we will be happy to help. Please ensure you have the MATLAB Parallel Toolbox installed on your local comnputer. How MATLAB Parallel Works Behind the Scenes MATLAB parallel allows the MATLAB session you interact with on your local computer, also known as the MATLAB client, with the PBS scheduler at CARC to create jobs that run on a core, also known as the MATLAB worker. The PBS (Portable BAtch system) scheduler allocates resources requested to users. One of the advantages of using MATLAB at CARC is the ability to scale up, or use many nodes for data intensive and/or computationally complex computations. To do this, you will request more workers from the PBS scheduler by following the tutorial bellow (Workers window). If you request multiple workers, it is important to keep in mind that one worker will be running the batch script you have sent from your MATLAB client. This worker is sending your scripts to the other workers that will perform your computations. You can think about this lead MATLAB worker as a mirror of your MATLAB client that communicates with the PBS scheduler and your scripts to accomplish your job. For more help on how to alter your scripts to take advantage of the scaleing up abilities at CARC please visit the Mathworks tutorials . MATLAB Parallel Server Client Configuration Once MATLAB is installed on your local machine (the MATLAB version on your local machine must match the version on the CARC cluster) click \"add-ons\" to open Add-on explorer. Search for PBS. Click on the link \"Parallel Computing Toolbox plugin for MATLAB Parallel Server with PBS\" (there is a plugin for slurm as well). Click the \"install\" button and the plugin will install and a wizard is started. Choose UNIX in the cluster type Select no for shared job location. Enter the address of the cluster you would like to use: for example, wheeler.alliance.unm.edu or xena.alliance.unm.edu. Enter for the path to the PBS scripts (remote job storage location) that MATLAB will create on the cluster. Select unique subfolders. Select the number of workers and number of threads per worker. This may depend on the program you are running but in general you should have one worker per core on the cluster. For set up and validation leave the number of workers at 1. Leave the threads per worker at 1 unless your software requires more threads. Specify the path to the matlab installation on the compute nodes: /opt/local/MATLAB/R2019a (or 2020a) for the Xena cluster, and /opt/local/MATLAB/R2019a for the Wheeler cluster. It is important that you are running the same version of MATLAB as you are running on the wheeler cluster. Choose flexnet for the license Name your profile. For example \"R2019a_Wheeler_PBS\" Review your profile settings and create the profile. You can create multiple profiles for different CARC clusters and numbers of workers. parallel.cluster.generic.runProfileWizard() Setting your IP Address In the next steps MATLAB will need to know your local IP address to allow incoming/outgoing connections on your computer. You will likely have to tell the system the IP address or hostname of your local machine. This is so the CARC cluster can communicate with your laptop or desktop. You will set the hostname with pctconfig (Parallel Config Toolbox). You can either type the hostname in directly or attempt to have MATLAB find it for you with the following commands: OS X [~,name]=system('ipconfig getifaddr en0'); pctconfig('hostname',name); Linux %% [~,name]=system('hostname -i'); pctconfig('hostname',name); Windows 10 Look up your computers IP address and enter: %% pctconfig('hostname',\"<your IP address>\"); Validating the Configuration Select \"parallel\" then create/manage clusters. Choose the profile you just created. In this example, the profile name is \"R2019a_Wheeler_PBS\" Select the profile you just created and select the validation tab. Press the validate button. It will ask for your CARC username, and here you can also select your ssh keyfile if you use one (cmd+shift+. to reveal hidden directories so you can see you ~/.ssh folder and select your private key), or just enter your password. MATLAB will now validate your setup. If you run into trouble please contact CARC support at help@carc.unm.edu. This completes the install and configuration. We have found it best to restart MATLAB at this point, otherwise setting the hostname in the next step may not work. Writing Parallel Matlab Code Once you have set the hostname you can run your parallel MATLAB code. Mathworks provides extensive documentation on using parallelism in MATLAB: Mathworks Docs The very simple program that follows demonstrates how to run parallel code using MATLAB. The code uses parfor, which can often be used to replace a for loop in serial MATLAB code, to distibute the work across 10 parallel workers. n_workers = 10; % We will request 10 workers to run in parallel p = parpool('R2019a_Wheeler_PBS', n_workers); % Create the pool of workers using the profile created earlier with 10 workers. parfor i = 1:100 % Define a parallel loop that will be distributed accross the 10 workers. i % Print the value of i for this iteration. end delete(p); % Clean up the worker pool Monitoring MATLAB Jobs To check that your jobs are indeed running at CARC, you can log in (ssh) to the cluster you have submitted your job to and check your job status. The command bellow shows only your jobs. ssh username@wheeler.alliance.unm.edu qstat -u <username> If you are testing small scripts, they may run before you can type qstat. To watch your jobs, you can type watch before the qstat (or anyother) command and it will re-run the command every 2sec. This is a good way to watch progress.","title":"Parallel MATLAB Server"},{"location":"ParallelMatlabServer/#parallel-matlab-server","text":"MATLAB supports parallelization on desktop computers which can be used to increase the speed of analysis drastically. MATLAB also provides the MATLAB Parallel Server (previously the MATLAB Distributed Computing Server) which allows you to write MATLAB code on your local desktop or laptop computer and perform the computation using the CARC high-performance clusters. This QuickByte leads you through the steps needed to set this up. If you run into problems please send an email to help@carc.unm.edu and we will be happy to help. Please ensure you have the MATLAB Parallel Toolbox installed on your local comnputer.","title":"Parallel MATLAB Server"},{"location":"ParallelMatlabServer/#how-matlab-parallel-works-behind-the-scenes","text":"MATLAB parallel allows the MATLAB session you interact with on your local computer, also known as the MATLAB client, with the PBS scheduler at CARC to create jobs that run on a core, also known as the MATLAB worker. The PBS (Portable BAtch system) scheduler allocates resources requested to users. One of the advantages of using MATLAB at CARC is the ability to scale up, or use many nodes for data intensive and/or computationally complex computations. To do this, you will request more workers from the PBS scheduler by following the tutorial bellow (Workers window). If you request multiple workers, it is important to keep in mind that one worker will be running the batch script you have sent from your MATLAB client. This worker is sending your scripts to the other workers that will perform your computations. You can think about this lead MATLAB worker as a mirror of your MATLAB client that communicates with the PBS scheduler and your scripts to accomplish your job. For more help on how to alter your scripts to take advantage of the scaleing up abilities at CARC please visit the Mathworks tutorials .","title":"How MATLAB Parallel Works Behind the Scenes"},{"location":"ParallelMatlabServer/#matlab-parallel-server-client-configuration","text":"Once MATLAB is installed on your local machine (the MATLAB version on your local machine must match the version on the CARC cluster) click \"add-ons\" to open Add-on explorer. Search for PBS. Click on the link \"Parallel Computing Toolbox plugin for MATLAB Parallel Server with PBS\" (there is a plugin for slurm as well). Click the \"install\" button and the plugin will install and a wizard is started. Choose UNIX in the cluster type Select no for shared job location. Enter the address of the cluster you would like to use: for example, wheeler.alliance.unm.edu or xena.alliance.unm.edu. Enter for the path to the PBS scripts (remote job storage location) that MATLAB will create on the cluster. Select unique subfolders. Select the number of workers and number of threads per worker. This may depend on the program you are running but in general you should have one worker per core on the cluster. For set up and validation leave the number of workers at 1. Leave the threads per worker at 1 unless your software requires more threads. Specify the path to the matlab installation on the compute nodes: /opt/local/MATLAB/R2019a (or 2020a) for the Xena cluster, and /opt/local/MATLAB/R2019a for the Wheeler cluster. It is important that you are running the same version of MATLAB as you are running on the wheeler cluster. Choose flexnet for the license Name your profile. For example \"R2019a_Wheeler_PBS\" Review your profile settings and create the profile. You can create multiple profiles for different CARC clusters and numbers of workers. parallel.cluster.generic.runProfileWizard()","title":"MATLAB Parallel Server Client Configuration"},{"location":"ParallelMatlabServer/#setting-your-ip-address","text":"In the next steps MATLAB will need to know your local IP address to allow incoming/outgoing connections on your computer. You will likely have to tell the system the IP address or hostname of your local machine. This is so the CARC cluster can communicate with your laptop or desktop. You will set the hostname with pctconfig (Parallel Config Toolbox). You can either type the hostname in directly or attempt to have MATLAB find it for you with the following commands: OS X [~,name]=system('ipconfig getifaddr en0'); pctconfig('hostname',name); Linux %% [~,name]=system('hostname -i'); pctconfig('hostname',name); Windows 10 Look up your computers IP address and enter: %% pctconfig('hostname',\"<your IP address>\");","title":"Setting your IP Address"},{"location":"ParallelMatlabServer/#validating-the-configuration","text":"Select \"parallel\" then create/manage clusters. Choose the profile you just created. In this example, the profile name is \"R2019a_Wheeler_PBS\" Select the profile you just created and select the validation tab. Press the validate button. It will ask for your CARC username, and here you can also select your ssh keyfile if you use one (cmd+shift+. to reveal hidden directories so you can see you ~/.ssh folder and select your private key), or just enter your password. MATLAB will now validate your setup. If you run into trouble please contact CARC support at help@carc.unm.edu. This completes the install and configuration. We have found it best to restart MATLAB at this point, otherwise setting the hostname in the next step may not work.","title":"Validating the Configuration"},{"location":"ParallelMatlabServer/#writing-parallel-matlab-code","text":"Once you have set the hostname you can run your parallel MATLAB code. Mathworks provides extensive documentation on using parallelism in MATLAB: Mathworks Docs The very simple program that follows demonstrates how to run parallel code using MATLAB. The code uses parfor, which can often be used to replace a for loop in serial MATLAB code, to distibute the work across 10 parallel workers. n_workers = 10; % We will request 10 workers to run in parallel p = parpool('R2019a_Wheeler_PBS', n_workers); % Create the pool of workers using the profile created earlier with 10 workers. parfor i = 1:100 % Define a parallel loop that will be distributed accross the 10 workers. i % Print the value of i for this iteration. end delete(p); % Clean up the worker pool","title":"Writing Parallel Matlab Code"},{"location":"ParallelMatlabServer/#monitoring-matlab-jobs","text":"To check that your jobs are indeed running at CARC, you can log in (ssh) to the cluster you have submitted your job to and check your job status. The command bellow shows only your jobs. ssh username@wheeler.alliance.unm.edu qstat -u <username> If you are testing small scripts, they may run before you can type qstat. To watch your jobs, you can type watch before the qstat (or anyother) command and it will re-run the command every 2sec. This is a good way to watch progress.","title":"Monitoring MATLAB Jobs"},{"location":"Parallel_MATLAB_profile_setup_and_batch_submission/","text":"Parallel MATLAB batch submission Setting up the cluster profile In order to submit a PBS script that takes advantage of MATLAB Parallel Server you first need to set up a new cluster profile specific to Wheeler. Thankfully this has already been done and all you need to do as a user is import the profile and that only needs to be done once. If you would like to do this interactively you can start an interactive session with the following: wheeler:~$ srun --pty bash Once you have a node allocated to you load the MATLAB module and start a MATLAB session: wheeler001:~$ module load matlab wheeler001:~$ matlab To get started, type doc. For product information, visit www.mathworks.com. >> Now simply import the wheeler cluster profile availble in the root matlab folder: >> profile = parallel.importProfile('/opt/local/MATLAB/wheeler-normal.settings') With the settings imported you can now launch parallel pools for computation using the wheeler cluster profile. The code below is an example to test parallel computing across two nodes on Wheeler while timing execution: >> poolobj = parpool(profile, 16) >> tic >> n = 200 >> A = 500 >> a = zeros(1,n) >> parfor i = 1:n >> a(i) = max(abs(eig(rand(A)))) >> end % You may need to hit enter more than once to get the prompt back. >> toc >> delete(poolobj); Even better is to do everything using a batch script and avoid the mistakes associated with interactive computing. Below is an example MATLAB script named parallel_matlab.m that will import our cluster profile and compare the time of computation for a sequential for loop and a parallel for loop with 16 cores ('workers' in MATLAB speak): profile = parallel.importProfile('/opt/local/MATLAB/wheeler-normal.settings') poolobj = parpool(profile, 16); tic n = 200; A = 500; a = zeros(1,n); for i=1:n; a(i) = max(abs(eig(rand(A)))) end toc tic n = 200; A = 500; a = zeros(1,n); parfor i=1:n; a(i) = max(abs(eig(rand(A)))) end toc delete(poolobj); Now the PBS script we will call parallel_matlab.pbs to submit your sample MATLAB program: #!/bin/bash #PBS -N parallel_matlab #PBS -l walltime=01:00:00 #PBS -l nodes=1:ppn=8 #PBS -j oe cd #PBS_O_WORKDIR module load matlab/R2019a matlab -r -nodisplay parallel_matlab > parallel_matlab.out Submit your PBS script with qsub parallel_matlab.pbs and hopefully all goes swimmingly. If you require assistance with MATLAB parallel computing please send an email to help@carc.unm.edu.","title":"Parallel MATLAB batch submission"},{"location":"Parallel_MATLAB_profile_setup_and_batch_submission/#parallel-matlab-batch-submission","text":"","title":"Parallel MATLAB batch submission"},{"location":"Parallel_MATLAB_profile_setup_and_batch_submission/#setting-up-the-cluster-profile","text":"In order to submit a PBS script that takes advantage of MATLAB Parallel Server you first need to set up a new cluster profile specific to Wheeler. Thankfully this has already been done and all you need to do as a user is import the profile and that only needs to be done once. If you would like to do this interactively you can start an interactive session with the following: wheeler:~$ srun --pty bash Once you have a node allocated to you load the MATLAB module and start a MATLAB session: wheeler001:~$ module load matlab wheeler001:~$ matlab To get started, type doc. For product information, visit www.mathworks.com. >> Now simply import the wheeler cluster profile availble in the root matlab folder: >> profile = parallel.importProfile('/opt/local/MATLAB/wheeler-normal.settings') With the settings imported you can now launch parallel pools for computation using the wheeler cluster profile. The code below is an example to test parallel computing across two nodes on Wheeler while timing execution: >> poolobj = parpool(profile, 16) >> tic >> n = 200 >> A = 500 >> a = zeros(1,n) >> parfor i = 1:n >> a(i) = max(abs(eig(rand(A)))) >> end % You may need to hit enter more than once to get the prompt back. >> toc >> delete(poolobj); Even better is to do everything using a batch script and avoid the mistakes associated with interactive computing. Below is an example MATLAB script named parallel_matlab.m that will import our cluster profile and compare the time of computation for a sequential for loop and a parallel for loop with 16 cores ('workers' in MATLAB speak): profile = parallel.importProfile('/opt/local/MATLAB/wheeler-normal.settings') poolobj = parpool(profile, 16); tic n = 200; A = 500; a = zeros(1,n); for i=1:n; a(i) = max(abs(eig(rand(A)))) end toc tic n = 200; A = 500; a = zeros(1,n); parfor i=1:n; a(i) = max(abs(eig(rand(A)))) end toc delete(poolobj); Now the PBS script we will call parallel_matlab.pbs to submit your sample MATLAB program: #!/bin/bash #PBS -N parallel_matlab #PBS -l walltime=01:00:00 #PBS -l nodes=1:ppn=8 #PBS -j oe cd #PBS_O_WORKDIR module load matlab/R2019a matlab -r -nodisplay parallel_matlab > parallel_matlab.out Submit your PBS script with qsub parallel_matlab.pbs and hopefully all goes swimmingly. If you require assistance with MATLAB parallel computing please send an email to help@carc.unm.edu.","title":"Setting up the cluster profile"},{"location":"Paraview_Hopper/","text":"Remote Visualization using ParaView in parallel. ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView's batch processing capabilities. ParaView was developed to analyze extremely large datasets using distributed memory computing resources. It can be run on supercomputers to analyze datasets of petascale size. These steps will help you setup Paraview to work as a client/server mode, being your laptop/desktop computer a client and the cluster a server. Be sure that the ParaView version installed on your local computer matches the same one that is installed on Wheeler and Hopper clusters. To see a current list of paraview versions installed on CARC clusters login to the cluster and run module spider paraview Downloads: https://www.paraview.org/download/ ParaView User's Guide: https://docs.paraview.org/en/latest/UsersGuide/index.html Hopper Cluster Connection The most common approach to use ParaView on Hopper is through the Client-Server mode support by ParaView, which requires an installation of ParaView on your local computer (Client). There are two methods to connect to Paraview Server (PVSERVER): Method 1: Direct Connection (Off-Campus) The process to connecto to ParaView is, in one terminal you will ask Hopper to assign you compute nodes, where you will run the ParaView server. Once the ParaView server is listening for connections, you will open an ssh tunnel in another terminal window (This process is from your local computer to one of the compute nodes you were assigned). Then, you will tell the ParaView client on your computer to connect to the tunnel and so to the compute nodes at CARC, where it will perform the rendering. Terminal 1: Login to Hopper and allocate resources 1. Login to Hopper ssh username@hopper.alliance.unm.edu 2. Allocating 2 nodes : Total 64 cores for 30 minutes in the \"General\" queue/partition. salloc --nodes 2 --exclusive --partition general --time 00:30:00 3. Load Module module load paraview/5.11.0 4. Running pvserver (this command will allow a connection between your local computer and Hopper). mpiexec -np 64 pvserver --mpi --force-offscreen-rendering --server-port=11111 Terminal 2: Hopper SSH Tunneling The hopper### corresponds to the compute node allocated by slurm, and do not forget to change your username. ssh -L 11111:hopper###:11111 username@hopper.alliance.unm.edu ParaView 5.11.0 RC1 Client and Setup Server Configuration File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Hopper Server Type: \"Client / Server\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the \"scancel\" command on the cluster head node. Method 2: Reverse Connection (UNM On-Campus) This process allows you to connect to Hopper service node. This process requires to know your localhost IP address \"local_host_IP\". Check your firewall setting if you are having firewall connectivity issues. Terminal 1: Login to Hopper and allocate resources 1. Login to Hopper ssh username@hopper.alliance.unm.edu 2. Allocating 2 nodes : Total 64 cores for 30 minutes in the \"General\" queue/partition. salloc --nodes 2 --exclusive --partition general --time 00:30:00 3. Load Module module load paraview/5.11.0 4. Running pvserver (this command will allow a connection between your local computer and Hopper). mpiexec -np 64 pvserver --mpi --force-offscreen-rendering --rc --client-host=My_Public_IP Opening ParaView 5.11.0 RC1 Client and Setup Server Configuration Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Hopper RC Server Type: \"Client / Server (Reverse Connection)\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the \"scancel\" command on the cluster head node. ParaView executables ParaView comes with several executables that serve different purposes. These are: paraview, pvpython, pvbatch, pvserver, pvdataserver and pvrenderserver. To learn more about this executables, https://docs.paraview.org/en/latest/UsersGuide/introduction.html#paraview-executables.","title":"Paraview Hopper"},{"location":"Paraview_Hopper/#remote-visualization-using-paraview-in-parallel","text":"ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView's batch processing capabilities. ParaView was developed to analyze extremely large datasets using distributed memory computing resources. It can be run on supercomputers to analyze datasets of petascale size. These steps will help you setup Paraview to work as a client/server mode, being your laptop/desktop computer a client and the cluster a server. Be sure that the ParaView version installed on your local computer matches the same one that is installed on Wheeler and Hopper clusters. To see a current list of paraview versions installed on CARC clusters login to the cluster and run module spider paraview Downloads: https://www.paraview.org/download/ ParaView User's Guide: https://docs.paraview.org/en/latest/UsersGuide/index.html","title":"Remote Visualization using ParaView in parallel."},{"location":"Paraview_Hopper/#hopper-cluster-connection","text":"The most common approach to use ParaView on Hopper is through the Client-Server mode support by ParaView, which requires an installation of ParaView on your local computer (Client). There are two methods to connect to Paraview Server (PVSERVER):","title":"Hopper Cluster Connection"},{"location":"Paraview_Hopper/#method-1-direct-connection-off-campus","text":"The process to connecto to ParaView is, in one terminal you will ask Hopper to assign you compute nodes, where you will run the ParaView server. Once the ParaView server is listening for connections, you will open an ssh tunnel in another terminal window (This process is from your local computer to one of the compute nodes you were assigned). Then, you will tell the ParaView client on your computer to connect to the tunnel and so to the compute nodes at CARC, where it will perform the rendering.","title":"Method 1: Direct Connection (Off-Campus)"},{"location":"Paraview_Hopper/#terminal-1-login-to-hopper-and-allocate-resources","text":"","title":"Terminal 1: Login to Hopper and allocate resources"},{"location":"Paraview_Hopper/#1-login-to-hopper","text":"ssh username@hopper.alliance.unm.edu","title":"1. Login to Hopper"},{"location":"Paraview_Hopper/#2-allocating-2-nodes-total-64-cores-for-30-minutes-in-the-general-queuepartition","text":"salloc --nodes 2 --exclusive --partition general --time 00:30:00","title":"2. Allocating 2 nodes : Total 64 cores for 30 minutes in the \"General\" queue/partition."},{"location":"Paraview_Hopper/#3-load-module","text":"module load paraview/5.11.0","title":"3. Load Module"},{"location":"Paraview_Hopper/#4-running-pvserver-this-command-will-allow-a-connection-between-your-local-computer-and-hopper","text":"mpiexec -np 64 pvserver --mpi --force-offscreen-rendering --server-port=11111","title":"4. Running pvserver (this command will allow a connection between your local computer and Hopper)."},{"location":"Paraview_Hopper/#terminal-2-hopper-ssh-tunneling","text":"The hopper### corresponds to the compute node allocated by slurm, and do not forget to change your username. ssh -L 11111:hopper###:11111 username@hopper.alliance.unm.edu","title":"Terminal 2: Hopper SSH Tunneling"},{"location":"Paraview_Hopper/#paraview-5110-rc1-client-and-setup-server-configuration","text":"File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Hopper Server Type: \"Client / Server\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the \"scancel\" command on the cluster head node.","title":"ParaView 5.11.0 RC1 Client and Setup Server Configuration"},{"location":"Paraview_Hopper/#method-2-reverse-connection-unm-on-campus","text":"This process allows you to connect to Hopper service node. This process requires to know your localhost IP address \"local_host_IP\". Check your firewall setting if you are having firewall connectivity issues.","title":"Method 2: Reverse Connection (UNM On-Campus)"},{"location":"Paraview_Hopper/#terminal-1-login-to-hopper-and-allocate-resources_1","text":"","title":"Terminal 1: Login to Hopper and allocate resources"},{"location":"Paraview_Hopper/#1-login-to-hopper_1","text":"ssh username@hopper.alliance.unm.edu","title":"1. Login to Hopper"},{"location":"Paraview_Hopper/#2-allocating-2-nodes-total-64-cores-for-30-minutes-in-the-general-queuepartition_1","text":"salloc --nodes 2 --exclusive --partition general --time 00:30:00","title":"2. Allocating 2 nodes : Total 64 cores for 30 minutes in the \"General\" queue/partition."},{"location":"Paraview_Hopper/#3-load-module_1","text":"module load paraview/5.11.0","title":"3. Load Module"},{"location":"Paraview_Hopper/#4-running-pvserver-this-command-will-allow-a-connection-between-your-local-computer-and-hopper_1","text":"mpiexec -np 64 pvserver --mpi --force-offscreen-rendering --rc --client-host=My_Public_IP","title":"4. Running pvserver (this command will allow a connection between your local computer and Hopper)."},{"location":"Paraview_Hopper/#opening-paraview-5110-rc1-client-and-setup-server-configuration","text":"Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Hopper RC Server Type: \"Client / Server (Reverse Connection)\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the \"scancel\" command on the cluster head node.","title":"Opening ParaView 5.11.0 RC1 Client and Setup Server Configuration"},{"location":"Paraview_Hopper/#paraview-executables","text":"ParaView comes with several executables that serve different purposes. These are: paraview, pvpython, pvbatch, pvserver, pvdataserver and pvrenderserver. To learn more about this executables, https://docs.paraview.org/en/latest/UsersGuide/introduction.html#paraview-executables.","title":"ParaView executables"},{"location":"PyTorch_1.9_Xena/","text":"Setting Up PyTorch using Xena SSH in to Xena To connect to the Xena machine, you will need to use the secure shell command below with your username in place of $USERNAME. This will prompt you for your password. When typing your password, you will not get any visual feedback. If you have issues with connecting to the machine, please reach out to the CARC helpdesk. ssh $USERNAME@xena.alliance.unm.edu Create the Condarc File To create the Condarc file that you will need, use your favorite text editor to create an empty file '.condarc'. For this tutorial I will use nano, which is an easy beginning text editor. nano .condarc The above code creates a blank file using the nano text editor. In the condarc file you just created, paste the following then change the $USERNAME prompt to your username. auto_activate_base: false channels: - conda-forge - defaults envs_dirs: - /users/$USERNAME/.conda/envs pkgs_dirs: - /users/$USERNAME/.conda/pkgs If you are using nano, notice that at the bottom of the screen below, it lists options for how users can interact with the open file. To save the lines of code you just pasted into the file, hold down the control key and press 'O'. This \"writes out\" or saves the text. You can then exit the text editor by holding down the control key and pressing 'X'. To confirm that the file has been edited and contains the correct code, run the command. cat .condarc The following text should appear under your command line. auto_activate_base: false channels: - conda-forge - defaults envs_dirs: - /users/{YOUR USERNAME HERE}/.conda/envs pkgs_dirs: - /users/{YOUR USERNAME HERE}/.conda/pkgs If you do not see this text appear, walk through the creation of the condarc file again. Navigate to the PyTorch1.9-K40-Compatible directory Run the following code to change directories into the PyTorch1.9-K40-Compatible directory. cd /projects/shared/pytorch/PyTorch1.9-K40-Compatible Assuming you have a default command line prompt, this will change your command prompt to reflect the updated directory. If you are not in the PyTorch1.9-K40-Compatible directory, you will run into issues in later steps. To confirm that you are in the correct directory, print your working directory with the following command. pwd Create the Conda Environment Now that you are in the correct directory, you'll create your python environment. To create a python environment, you first need to load the miniconda3 module with the following command: (No output should print from this command) module load miniconda3 You can then create the python environment by running the following command: conda env create -f torch-1.9.0+cu11.1-K40.yml When asked to proceed, enter \"y\" Activate the Conda Environment Once the environment is created, you will need to activate it before you start the pytorch installation. Activate the environment by running the following: conda activate pytorch-1.9-cuda-11-K40 This command will add a prefix to your command prompt that indicates which environment you are in. Your command line prompt should take the form of: (pytorch-1.9-cuda-11-K40)[(username)@(hostname) PyTorch1.9-K40-Compatible]$ Pip installation To install pytorch, run the following command: pip3 install --user torch-1.9.0+cu11.1-cp39-cp39-linux_x86_64.whl After the installation is complete, we can confirm that the torch install was sucessful in JupyterHub. Follow the link below to the CARC website. http://carc.unm.edu Navigate to Systems > JupyterHub Cluster Links > Xena Next, log in to JupyterHub with your CARC username and password. If you are logged in without being prompted to select a server, click on the control panel button in the upper right corner. Then select \"Stop My Server\", then select \"Start Server\" You will be prompted to choose a server. For this tutorial, I will choose a Xena server with 2 GPU's. Example: Xena 1 hour, 2 GPUs, 16 cores, 60 GB RAM Create a new notebook by selecting new > Python [conda env:.conda-pytorch-1.9-cuda-11-K40] To test that the installation was successful, run the following code. import torch torch.cuda.device_count() With 2 GPUs, you should get an output of 2.","title":"Installing PyTorch on Xena"},{"location":"PyTorch_1.9_Xena/#setting-up-pytorch-using-xena","text":"","title":"Setting Up PyTorch using Xena"},{"location":"PyTorch_1.9_Xena/#ssh-in-to-xena","text":"To connect to the Xena machine, you will need to use the secure shell command below with your username in place of $USERNAME. This will prompt you for your password. When typing your password, you will not get any visual feedback. If you have issues with connecting to the machine, please reach out to the CARC helpdesk. ssh $USERNAME@xena.alliance.unm.edu","title":"SSH in to Xena"},{"location":"PyTorch_1.9_Xena/#create-the-condarc-file","text":"To create the Condarc file that you will need, use your favorite text editor to create an empty file '.condarc'. For this tutorial I will use nano, which is an easy beginning text editor. nano .condarc The above code creates a blank file using the nano text editor. In the condarc file you just created, paste the following then change the $USERNAME prompt to your username. auto_activate_base: false channels: - conda-forge - defaults envs_dirs: - /users/$USERNAME/.conda/envs pkgs_dirs: - /users/$USERNAME/.conda/pkgs If you are using nano, notice that at the bottom of the screen below, it lists options for how users can interact with the open file. To save the lines of code you just pasted into the file, hold down the control key and press 'O'. This \"writes out\" or saves the text. You can then exit the text editor by holding down the control key and pressing 'X'. To confirm that the file has been edited and contains the correct code, run the command. cat .condarc The following text should appear under your command line. auto_activate_base: false channels: - conda-forge - defaults envs_dirs: - /users/{YOUR USERNAME HERE}/.conda/envs pkgs_dirs: - /users/{YOUR USERNAME HERE}/.conda/pkgs If you do not see this text appear, walk through the creation of the condarc file again.","title":"Create the Condarc File"},{"location":"PyTorch_1.9_Xena/#navigate-to-the-pytorch19-k40-compatible-directory","text":"Run the following code to change directories into the PyTorch1.9-K40-Compatible directory. cd /projects/shared/pytorch/PyTorch1.9-K40-Compatible Assuming you have a default command line prompt, this will change your command prompt to reflect the updated directory. If you are not in the PyTorch1.9-K40-Compatible directory, you will run into issues in later steps. To confirm that you are in the correct directory, print your working directory with the following command. pwd","title":"Navigate to the PyTorch1.9-K40-Compatible directory"},{"location":"PyTorch_1.9_Xena/#create-the-conda-environment","text":"Now that you are in the correct directory, you'll create your python environment. To create a python environment, you first need to load the miniconda3 module with the following command: (No output should print from this command) module load miniconda3 You can then create the python environment by running the following command: conda env create -f torch-1.9.0+cu11.1-K40.yml When asked to proceed, enter \"y\"","title":"Create the Conda Environment"},{"location":"PyTorch_1.9_Xena/#activate-the-conda-environment","text":"Once the environment is created, you will need to activate it before you start the pytorch installation. Activate the environment by running the following: conda activate pytorch-1.9-cuda-11-K40 This command will add a prefix to your command prompt that indicates which environment you are in. Your command line prompt should take the form of: (pytorch-1.9-cuda-11-K40)[(username)@(hostname) PyTorch1.9-K40-Compatible]$","title":"Activate the Conda Environment"},{"location":"PyTorch_1.9_Xena/#pip-installation","text":"To install pytorch, run the following command: pip3 install --user torch-1.9.0+cu11.1-cp39-cp39-linux_x86_64.whl After the installation is complete, we can confirm that the torch install was sucessful in JupyterHub. Follow the link below to the CARC website. http://carc.unm.edu Navigate to Systems > JupyterHub Cluster Links > Xena Next, log in to JupyterHub with your CARC username and password. If you are logged in without being prompted to select a server, click on the control panel button in the upper right corner. Then select \"Stop My Server\", then select \"Start Server\" You will be prompted to choose a server. For this tutorial, I will choose a Xena server with 2 GPU's. Example: Xena 1 hour, 2 GPUs, 16 cores, 60 GB RAM Create a new notebook by selecting new > Python [conda env:.conda-pytorch-1.9-cuda-11-K40] To test that the installation was successful, run the following code. import torch torch.cuda.device_count() With 2 GPUs, you should get an output of 2.","title":"Pip installation"},{"location":"R_usage/","text":"R Programming in HPC What is R? R is a programming language and a software environment for statistical computing and graphics techniques. This is widely used now a days by statisticians and data miners for developing software tools required for data analysis. The R language is primarily derived from the S language developed at Bell Laboratories in 1975. R provides various tools and techniques for linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering etc. More history and documentation of R are available at this link How to run R? First login to one of the CARC machines via SSH. ssh -X user@machine_name.alliance.unm.edu Once logged into the machine, you have to load the module which has R program files. module load r-3.5.0-gcc-4.8.5-python2-khqxja7 After loading the R module, begin R programming by typing R This will shows R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. This means the R module is loaded and you are ready to use R for your research Running a sample script example.R Let us look a sample script, example.R which will print \"Hello World\" The script looks like # Program to print Hello World print('Hello World') In order to execute this script, type Rscript example.R This will give an output of [1] \"Hello World\" If you want to generate samples from a normal distribution, you can use the rnorm() function. Let's make some change to the example.R script. # Program to print Hello World and to generate normal random numbers print('Hello World') #Calling the rnorm() function to generate three random numbers from a normal distribution with mean 5 and a standard deviation of 5 rnorm(3,mean=5,sd=5) Execute the script again and the look at the new output. [1] \"Hello World\" [1] 3.744463 7.954163 3.363275","title":"R Programming in HPC"},{"location":"R_usage/#r-programming-in-hpc","text":"","title":"R Programming in HPC"},{"location":"R_usage/#what-is-r","text":"R is a programming language and a software environment for statistical computing and graphics techniques. This is widely used now a days by statisticians and data miners for developing software tools required for data analysis. The R language is primarily derived from the S language developed at Bell Laboratories in 1975. R provides various tools and techniques for linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering etc. More history and documentation of R are available at this link","title":"What is R?"},{"location":"R_usage/#how-to-run-r","text":"First login to one of the CARC machines via SSH. ssh -X user@machine_name.alliance.unm.edu Once logged into the machine, you have to load the module which has R program files. module load r-3.5.0-gcc-4.8.5-python2-khqxja7 After loading the R module, begin R programming by typing R This will shows R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. This means the R module is loaded and you are ready to use R for your research","title":"How to run R?"},{"location":"R_usage/#running-a-sample-script-exampler","text":"Let us look a sample script, example.R which will print \"Hello World\" The script looks like # Program to print Hello World print('Hello World') In order to execute this script, type Rscript example.R This will give an output of [1] \"Hello World\" If you want to generate samples from a normal distribution, you can use the rnorm() function. Let's make some change to the example.R script. # Program to print Hello World and to generate normal random numbers print('Hello World') #Calling the rnorm() function to generate three random numbers from a normal distribution with mean 5 and a standard deviation of 5 rnorm(3,mean=5,sd=5) Execute the script again and the look at the new output. [1] \"Hello World\" [1] 3.744463 7.954163 3.363275","title":"Running a sample script example.R"},{"location":"Resource_usage/","text":"Storage Policy Home directories are limited to 200 GB of storage. Project space is limited to 250 GB of storage. Scratch storage is limited to 1 TB (2 TB on Xena). Center-wide project scratch space is limited to 3 TB. To purchase additional storage please see our pricing spreadsheet . Resource Usage Policy Soft limits and hard limits are used to provide fair scheduling without wasting resources. The job scheduler will schedule all jobs meeting the soft limit requirements. If there are additional resources after all jobs meeting the soft limits are scheduled, then jobs meeting the hard limits are scheduled. The hard limits prevent users from monopolising the cluster for long periods by starting large jobs during temporary lulls in utilisation. Xena Configuration Parameters Soft Limit Hardlimit Number of Processors 64 128 Number of Nodes 4 (xena) 2(BigMem) 8 (xena) 4 (Bigmem) Processors per Node 16(xena) 32(BigMem) 16 (xena) 32 (BigMem) Node Access Policy Single Job/Node ie (Multiple users can\u2019t share resources across same node) Wheeler Configuration Queue: Default Default Debug Debug parameter soft limit hard limit soft limit hard limit Number of Processors 160 400 32 32 Number of Nodes 20 50 4 4 Processors per Node 8 8 8 8 Walltime 48:00:00 48:00:00 01:00:00 01:00:00 Node Access Policy Multi Job/Node ie (Multiple users may share resources across same node) Gibbs Configuration Parameters Soft Limit Hardlimit Number of Processors 96 96 Number of Nodes 6 6 Processors per Node 16 16 Walltime 72:00:00 72:00:00 Node Access Policy Single Job/Node ie (Multiple users can\u2019t share resources across same node)","title":"Storage Policy"},{"location":"Resource_usage/#storage-policy","text":"Home directories are limited to 200 GB of storage. Project space is limited to 250 GB of storage. Scratch storage is limited to 1 TB (2 TB on Xena). Center-wide project scratch space is limited to 3 TB. To purchase additional storage please see our pricing spreadsheet .","title":"Storage Policy"},{"location":"Resource_usage/#resource-usage-policy","text":"Soft limits and hard limits are used to provide fair scheduling without wasting resources. The job scheduler will schedule all jobs meeting the soft limit requirements. If there are additional resources after all jobs meeting the soft limits are scheduled, then jobs meeting the hard limits are scheduled. The hard limits prevent users from monopolising the cluster for long periods by starting large jobs during temporary lulls in utilisation.","title":"Resource Usage Policy"},{"location":"Resource_usage/#xena-configuration","text":"Parameters Soft Limit Hardlimit Number of Processors 64 128 Number of Nodes 4 (xena) 2(BigMem) 8 (xena) 4 (Bigmem) Processors per Node 16(xena) 32(BigMem) 16 (xena) 32 (BigMem) Node Access Policy Single Job/Node ie (Multiple users can\u2019t share resources across same node)","title":"Xena Configuration"},{"location":"Resource_usage/#wheeler-configuration","text":"Queue: Default Default Debug Debug parameter soft limit hard limit soft limit hard limit Number of Processors 160 400 32 32 Number of Nodes 20 50 4 4 Processors per Node 8 8 8 8 Walltime 48:00:00 48:00:00 01:00:00 01:00:00 Node Access Policy Multi Job/Node ie (Multiple users may share resources across same node)","title":"Wheeler Configuration"},{"location":"Resource_usage/#gibbs-configuration","text":"Parameters Soft Limit Hardlimit Number of Processors 96 96 Number of Nodes 6 6 Processors per Node 16 16 Walltime 72:00:00 72:00:00 Node Access Policy Single Job/Node ie (Multiple users can\u2019t share resources across same node)","title":"Gibbs Configuration"},{"location":"SimCov/","text":"This tutorial contains instructions for compiling and running the SimCov immunology model on the Wheeler cluster. Download the SimCov Source Code from GitHub Change directory to your home: cd ~ Clone the simcov Github repository into your home directory: git clone --recurse-submodules https://github.com/AdaptiveComputationLab/simcov.git Build SimCov from Source Load Wheeler modules and set UPCXX variables (NOTE: modules subject to change use 'module spider' to find availability): export UPCXX_THREADMODE=seq export UPCXX_CODEMODE=opt module load gcc/11.2.0-otgt module load cmake/3.22.2-c2dw module load upcxx/2021.9.0-r4of Run the build script: cd simcov ./build.sh Release Configure SimCov The config files are in ~/simcov and end with \".config\". You can edit them with a text editor. Submit a SimCov Job A wheeler PBS script is provided for you. We have submitted the script below to the simcov developers - so hopefully by the time you pull simcov the code below will already be in the wheeler_simcov_run.pbs. If not update the script to contain the following: This PBS submission script will run simcov on a compute node using covid_default.config: #!/bin/bash #PBS -q normal #PBS -l nodes=2:ppn=8 #PBS -l walltime=01:00:00 #PBS -N simcov_test #PBS -j oe module load gcc/11.2.0-otgt module load upcxx/2021.9.0-r4of module load cmake/3.22.2-c2dw cd $PBS_O_WORKDIR upcxx-run -n $PBS_NP -N $PBS_NUM_NODES -- install/bin/simcov --config=covid_default.config --output=results To run simcov on a compute node enter qsub wheeler_simcov_run.pbs Outputs will be in a results folder by default","title":"SimCov on Wheeler"},{"location":"SimCov/#download-the-simcov-source-code-from-github","text":"Change directory to your home: cd ~ Clone the simcov Github repository into your home directory: git clone --recurse-submodules https://github.com/AdaptiveComputationLab/simcov.git","title":"Download the SimCov Source Code from GitHub"},{"location":"SimCov/#build-simcov-from-source","text":"Load Wheeler modules and set UPCXX variables (NOTE: modules subject to change use 'module spider' to find availability): export UPCXX_THREADMODE=seq export UPCXX_CODEMODE=opt module load gcc/11.2.0-otgt module load cmake/3.22.2-c2dw module load upcxx/2021.9.0-r4of Run the build script: cd simcov ./build.sh Release","title":"Build SimCov from Source"},{"location":"SimCov/#configure-simcov","text":"The config files are in ~/simcov and end with \".config\". You can edit them with a text editor.","title":"Configure SimCov"},{"location":"SimCov/#submit-a-simcov-job","text":"A wheeler PBS script is provided for you. We have submitted the script below to the simcov developers - so hopefully by the time you pull simcov the code below will already be in the wheeler_simcov_run.pbs. If not update the script to contain the following: This PBS submission script will run simcov on a compute node using covid_default.config: #!/bin/bash #PBS -q normal #PBS -l nodes=2:ppn=8 #PBS -l walltime=01:00:00 #PBS -N simcov_test #PBS -j oe module load gcc/11.2.0-otgt module load upcxx/2021.9.0-r4of module load cmake/3.22.2-c2dw cd $PBS_O_WORKDIR upcxx-run -n $PBS_NP -N $PBS_NUM_NODES -- install/bin/simcov --config=covid_default.config --output=results To run simcov on a compute node enter qsub wheeler_simcov_run.pbs Outputs will be in a results folder by default","title":"Submit a SimCov Job"},{"location":"Stacks_quickbyte/","text":"Running Stacks on CARC Stacks is a common and well documented pipeline for processing RADseq data. RADseq data is a method of reduced representation genomic sequencing, where genomic DNA is cut up with restriction enzymes, which are then targeted by sequencing adapters. This allows a researcher to get thousands of loci randomly scattered across the genome, which can be sequenced at moderate depths for low prices. This is sufficient for many population genomic analyses, such as tests of population structure, phylogenetics, gene flow, and even coarse attempts to locate regions of the genome that are under selection. Stacks can be run with or without a reference genome, but using a reference genome is reccomended for improved accuracy. Stacks can easily be run on Wheeler with installed modules, and here we outline how with some simple \"quality of life\" adjustments and tips. We'll be focused on the reference based method, as the non-reference-based is sufficiently run through a driver script provided by the developers of Stacks (denovo_map.pl). We'll quickly mention it at the end. This can often be run on a single node on Wheeler, as the only intense step tends to be alignment, which is quick due to the small size of RADseq data. For example, a dataset of ~90 bird individuals with an average of 1 million reads/sample took four hours on one node. Organisms with larger genomes will take more time and memory. Preliminaries Sample list and Population maps Before you get started, you'll need a list of sample names to run bwa conveniently. This is a file we'll call \"sample_list\", and an example is below. For this and similar files, the first line describes what goes in each column and where tabs go, and the following lines give an example of what that looks like. For sample list, note that an empty newline should be at the end of the list.: <SAMPLE NAME> M_americana_Florida_MSBBIRD49539 M_americana_NM_MSBBIRD39487 ...... Then, key to many pieces of population genetics software, Stacks needs a population map (popmap) for calculating metrics like F st and genetic dviersity. It will also allow the creation of certain imput files. It is simply a tab delimited file with one line per individual, with the first column representing the sample name and the second its population. Note that reduced versions of the popmaps can be made for running gstacks and populations for only a subset of your dataset. <SAMPLE NAME>\\t<SAMPLE POPULATION> M_americana_Florida_MSBBIRD49539 EastBlackScoter M_americana_NewMexico_MSBBIRD39487 WestBlackScoter .................................. ..... Demultiplexing with process_radtags Demultiplexing with Stacks is comparatively easy. You just need a file of barcode information (we'll call it BARCODES.file) and your raw, multiplexed reads. The barcode information file can be paired or unapired, and should look like: <BARCODE1>\\t<BARCODE2>\\t<SAMPLE NAME> ATGCAT GTACGT M_americana_Florida_MSBBIRD49539 ACGTAT CTAGAT M_americana_NewMexico_MSBBIRD39487 ...... ...... ...... Here is how to run it, assuming you are dealing with paired end reads, gzipped fastq files, and used a single enzyme (ndeI) for your restriction digest. Note the the \"raw_reads\" directory is the one that you'll use for the aligning step at the start of the reference-based assembly: process_radtags -p /path/to/MULTIPLEXED_READS/ -b /path/to/BARCODES.file -o /path/to/raw_reads/ \\ -i gzfastq -e ndeI -c -q -r -E phred33 The command is different for single end reads. You must specify each fastq input indiviually with the -f flag, as shown below. The rest is the same: process_radtags -f /path/to/MULTIPLEXED_READS/RAW_READS_01.fastq.gz -b /path/to/BARCODES.file -o /path/to/raw_reads/ \\ -i gzfastq -e ndeI -c -q -r -E phred33 process_radtags -f /path/to/MULTIPLEXED_READS/RAW_READS_02.fastq.gz -b /path/to/BARCODES.file -o /path/to/raw_reads/ \\ -i gzfastq -e ndeI -c -q -r -E phred33 ....... You can find full details on process_radtags here . Reference Based Assembly We'll assume you demultiplex your reads before running the pipeline described below. The contents of this and the following section should all be put in a PBS/Slurm script, just make sure your directory names are correct. This method has several intermediate files, so we'll make directories to keep them separate (do this outside of script): mkdir raw_reads mkdir sam_files mkdir bam_files mkdir stacks_out mkdir populations_out The modules you need are stacks, bwa, and samtools. All are availible on Conda, but you will almost certainly be running this on Wheeler (low resource use), which has recent versions of all three installed: module load stacks-2.41-gcc-7.4.0-7r6auk7 module load bwa-0.7.17-intel-18.0.2-7jvpfu2 module load samtools-1.10-gcc-9.3.0-python3-ikifznw We'll also set some variables for refering to paths to stuff. We assume that the reference names (ReferenceBaseName): src=$PBS_O_WORKDIR bwa_ref=$src/ReferenceBaseName threads=[number of threads] Next, we need to index our reference: bwa index -p $bwa_ref $bwa_ref.fa This is the big step, which uses the Burroughs-Wheeler Aligner to align our reads to our reference. Note that this should be able to be done using pipes, but I've had issues with that, so we just remove the files at the end of each loop. while read indiv do # echo is only to help you keep track of where the pipeline is echo ${indiv} # align reads to reference genome bwa mem -t $threads $bwa_ref $src/raw_reads/${indiv}.fq.gz > $src/sam_files/${indiv}.sam # covert sam file to bam file (makes it much smaller and easier to work with) samtools view -bS $src/sam_files/${indiv}.sam > $src/bam_files/${indiv}_unsort.bam # sorts bam file, which is needed for next analyses samtools sort $src/bam_files/${indiv}_unsort.bam -o $src/bam_files/${indiv}.bam rm $src/sam_files/${indiv}.sam rm $src/bam_files/${indiv}_unsort.bam done < sample_list The next step is to run gstacks, which runs these \"traditional\" genomics files into something Stacks can work with to make output files: gstacks -I $src/bam_files/ -M $src/popmap -O $src/stacks_out/ -t $threads Finally, we run populations! This specific command will give us a 75% complete matrix of SNPs, one random SNP per locus, F st values, PLINK .bed and .map files, and use kernel smoothing for specific statistics. We also output a Variant Call Format (VCF) file, which can be used to generate inputs for most programs: populations -P $src/stacks_out/ -M $src/popmap -O $src/populations_out/ \\ --vcf -R .75 --write-random-snp --fstats --plink --smooth -t $threads A quick note, if you want input for RAxML or similar phylogenetic programs, you can get a interleaved phylip file by making a popmap file with each individual having its own population and specifying you want a phylip output. Please note that this is strict phylip format, meaning you want a maximum of 9 letters in your \"population\" column of the popmap (10 works, but will cause errors when input to certain programs). Also, this assumes you have a directory \"populations_individual\": populations -P $src/stacks_out/ -M $src/popmap_individual -O $src/popualtions_individual/ \\ -R .75 --phylip-var-all -t $threads Learn more about the outputs and options for populations on the Stacks website . Also, as mentioned above, you can easily subset your data by changing the popmap used in gstacks and populations, as each sample has alignments performed separately. DeNovo Assembly This is a lot simpler, but is generally considered less robust than a reference-based approach. It is described in full here . There is a lot to think about for parameters when building loci, and we use default ones here, read up on the Stacks website about them . First, you only need the Stacks module and one new directory (assumes reads are in \"raw_reads\"). mkdir stacks_denovo module load stacks-2.41-gcc-7.4.0-7r6auk7 src=$PBS_O_WORKDIR threads=[number of threads] Then you just run a single line! denovo_map.pl -T $threads -o $src/stacks_denovo/ --popmap $src/popmap --samples $src/raw_reads/ \\ -X \"<Extra parameters for populations here>\" As I mentioned above, I mostly included this to demystify DeNovo Stacks, please read more on it before running anything! There are many parameters to optimize.","title":"Stacks for RAD-Seq Data"},{"location":"Stacks_quickbyte/#running-stacks-on-carc","text":"Stacks is a common and well documented pipeline for processing RADseq data. RADseq data is a method of reduced representation genomic sequencing, where genomic DNA is cut up with restriction enzymes, which are then targeted by sequencing adapters. This allows a researcher to get thousands of loci randomly scattered across the genome, which can be sequenced at moderate depths for low prices. This is sufficient for many population genomic analyses, such as tests of population structure, phylogenetics, gene flow, and even coarse attempts to locate regions of the genome that are under selection. Stacks can be run with or without a reference genome, but using a reference genome is reccomended for improved accuracy. Stacks can easily be run on Wheeler with installed modules, and here we outline how with some simple \"quality of life\" adjustments and tips. We'll be focused on the reference based method, as the non-reference-based is sufficiently run through a driver script provided by the developers of Stacks (denovo_map.pl). We'll quickly mention it at the end. This can often be run on a single node on Wheeler, as the only intense step tends to be alignment, which is quick due to the small size of RADseq data. For example, a dataset of ~90 bird individuals with an average of 1 million reads/sample took four hours on one node. Organisms with larger genomes will take more time and memory.","title":"Running Stacks on CARC"},{"location":"Stacks_quickbyte/#preliminaries","text":"","title":"Preliminaries"},{"location":"Stacks_quickbyte/#sample-list-and-population-maps","text":"Before you get started, you'll need a list of sample names to run bwa conveniently. This is a file we'll call \"sample_list\", and an example is below. For this and similar files, the first line describes what goes in each column and where tabs go, and the following lines give an example of what that looks like. For sample list, note that an empty newline should be at the end of the list.: <SAMPLE NAME> M_americana_Florida_MSBBIRD49539 M_americana_NM_MSBBIRD39487 ...... Then, key to many pieces of population genetics software, Stacks needs a population map (popmap) for calculating metrics like F st and genetic dviersity. It will also allow the creation of certain imput files. It is simply a tab delimited file with one line per individual, with the first column representing the sample name and the second its population. Note that reduced versions of the popmaps can be made for running gstacks and populations for only a subset of your dataset. <SAMPLE NAME>\\t<SAMPLE POPULATION> M_americana_Florida_MSBBIRD49539 EastBlackScoter M_americana_NewMexico_MSBBIRD39487 WestBlackScoter .................................. .....","title":"Sample list and Population maps"},{"location":"Stacks_quickbyte/#demultiplexing-with-process_radtags","text":"Demultiplexing with Stacks is comparatively easy. You just need a file of barcode information (we'll call it BARCODES.file) and your raw, multiplexed reads. The barcode information file can be paired or unapired, and should look like: <BARCODE1>\\t<BARCODE2>\\t<SAMPLE NAME> ATGCAT GTACGT M_americana_Florida_MSBBIRD49539 ACGTAT CTAGAT M_americana_NewMexico_MSBBIRD39487 ...... ...... ...... Here is how to run it, assuming you are dealing with paired end reads, gzipped fastq files, and used a single enzyme (ndeI) for your restriction digest. Note the the \"raw_reads\" directory is the one that you'll use for the aligning step at the start of the reference-based assembly: process_radtags -p /path/to/MULTIPLEXED_READS/ -b /path/to/BARCODES.file -o /path/to/raw_reads/ \\ -i gzfastq -e ndeI -c -q -r -E phred33 The command is different for single end reads. You must specify each fastq input indiviually with the -f flag, as shown below. The rest is the same: process_radtags -f /path/to/MULTIPLEXED_READS/RAW_READS_01.fastq.gz -b /path/to/BARCODES.file -o /path/to/raw_reads/ \\ -i gzfastq -e ndeI -c -q -r -E phred33 process_radtags -f /path/to/MULTIPLEXED_READS/RAW_READS_02.fastq.gz -b /path/to/BARCODES.file -o /path/to/raw_reads/ \\ -i gzfastq -e ndeI -c -q -r -E phred33 ....... You can find full details on process_radtags here .","title":"Demultiplexing with process_radtags"},{"location":"Stacks_quickbyte/#reference-based-assembly","text":"We'll assume you demultiplex your reads before running the pipeline described below. The contents of this and the following section should all be put in a PBS/Slurm script, just make sure your directory names are correct. This method has several intermediate files, so we'll make directories to keep them separate (do this outside of script): mkdir raw_reads mkdir sam_files mkdir bam_files mkdir stacks_out mkdir populations_out The modules you need are stacks, bwa, and samtools. All are availible on Conda, but you will almost certainly be running this on Wheeler (low resource use), which has recent versions of all three installed: module load stacks-2.41-gcc-7.4.0-7r6auk7 module load bwa-0.7.17-intel-18.0.2-7jvpfu2 module load samtools-1.10-gcc-9.3.0-python3-ikifznw We'll also set some variables for refering to paths to stuff. We assume that the reference names (ReferenceBaseName): src=$PBS_O_WORKDIR bwa_ref=$src/ReferenceBaseName threads=[number of threads] Next, we need to index our reference: bwa index -p $bwa_ref $bwa_ref.fa This is the big step, which uses the Burroughs-Wheeler Aligner to align our reads to our reference. Note that this should be able to be done using pipes, but I've had issues with that, so we just remove the files at the end of each loop. while read indiv do # echo is only to help you keep track of where the pipeline is echo ${indiv} # align reads to reference genome bwa mem -t $threads $bwa_ref $src/raw_reads/${indiv}.fq.gz > $src/sam_files/${indiv}.sam # covert sam file to bam file (makes it much smaller and easier to work with) samtools view -bS $src/sam_files/${indiv}.sam > $src/bam_files/${indiv}_unsort.bam # sorts bam file, which is needed for next analyses samtools sort $src/bam_files/${indiv}_unsort.bam -o $src/bam_files/${indiv}.bam rm $src/sam_files/${indiv}.sam rm $src/bam_files/${indiv}_unsort.bam done < sample_list The next step is to run gstacks, which runs these \"traditional\" genomics files into something Stacks can work with to make output files: gstacks -I $src/bam_files/ -M $src/popmap -O $src/stacks_out/ -t $threads Finally, we run populations! This specific command will give us a 75% complete matrix of SNPs, one random SNP per locus, F st values, PLINK .bed and .map files, and use kernel smoothing for specific statistics. We also output a Variant Call Format (VCF) file, which can be used to generate inputs for most programs: populations -P $src/stacks_out/ -M $src/popmap -O $src/populations_out/ \\ --vcf -R .75 --write-random-snp --fstats --plink --smooth -t $threads A quick note, if you want input for RAxML or similar phylogenetic programs, you can get a interleaved phylip file by making a popmap file with each individual having its own population and specifying you want a phylip output. Please note that this is strict phylip format, meaning you want a maximum of 9 letters in your \"population\" column of the popmap (10 works, but will cause errors when input to certain programs). Also, this assumes you have a directory \"populations_individual\": populations -P $src/stacks_out/ -M $src/popmap_individual -O $src/popualtions_individual/ \\ -R .75 --phylip-var-all -t $threads Learn more about the outputs and options for populations on the Stacks website . Also, as mentioned above, you can easily subset your data by changing the popmap used in gstacks and populations, as each sample has alignments performed separately.","title":"Reference Based Assembly"},{"location":"Stacks_quickbyte/#denovo-assembly","text":"This is a lot simpler, but is generally considered less robust than a reference-based approach. It is described in full here . There is a lot to think about for parameters when building loci, and we use default ones here, read up on the Stacks website about them . First, you only need the Stacks module and one new directory (assumes reads are in \"raw_reads\"). mkdir stacks_denovo module load stacks-2.41-gcc-7.4.0-7r6auk7 src=$PBS_O_WORKDIR threads=[number of threads] Then you just run a single line! denovo_map.pl -T $threads -o $src/stacks_denovo/ --popmap $src/popmap --samples $src/raw_reads/ \\ -X \"<Extra parameters for populations here>\" As I mentioned above, I mostly included this to demystify DeNovo Stacks, please read more on it before running anything! There are many parameters to optimize.","title":"DeNovo Assembly"},{"location":"Tensorflow_documentation/","text":"Introduction to Tensorflow The relatively recent mainstream availability of complex algorithms and computationally efficient hardware is creating a platform for new innovations never before available to the scientific computing world. Since the development of computer systems, computing times have been drastically reduced, making more complex computations feasible. The continuous cycle of improvements in computation speeds and hardware leading to ever more complex computations can be seen in the scaling of hardware to meet these more complex computation goals. A resolution to this cycle can be found in the utilization of GPU's in high-performance computing for Machine learning and deep learning algorithms. Most of the complex computing strategies can be simplified into basic linear algebra operations such as addition, multiplication, subtraction, inversion and such. Out of the listed operations, matrix multiplication and inversion are the most computationally expensive operations. Most matrix operations are performed sequentially on the CPU resulting in computation time that scales with the size of the matrix as a factor \u03b8(n 3 ). Hence, computation cycles and duration of time to be allocated towards computation is proportional to the size of matrices under the constrained hardware with limited cache memory and RAM. The same problem still exists with multicore systems or distributed systems due to the threshold on the resources mentioned. On the other hand, a GPU is composed of several thousand cores, combining to provide the user with several GBs of computational memory compared to the MBs provided by CPU cache memory. The distributed computing this configuration provides enables parallelism across GPU cores and allows a super fast flow of data resulting from incredibly high bandwidth. This distribution across multiple cores amounts to massively reduced computation time as the device is able to scale its performance with data size. The enormous gains in computation time should give researchers a valid reason to switch from CPUs to GPUs for computationally heavy operations where the CPU-based operations do not scale with the data at a constant rate. Utilization of this methodology will provide enormous benefits for the computationally heavy domains such as machine learning, deep learning, linear algebra, optimization, data structures, etc. To illustrate this statement, we've included some benchmarks run on the Xena system at CARC using intensive linear algebra operations, i.e., matrix multiplication and matrix-inversion, on a CPU only compared to CPU utilizing the GPU as well. The CPU version was deployed on a multicore processor with 16 cores and 64GB of RAM with using numpy arrays in python. The GPU Version was deployed on NVIDIA Tesla K40 with 11GB of GPU memory using Tensorflow. Here the CPU implementations were carried with two different types of numpy compilations. mkl_mul stands for multiplication operation carried with numpy compiled with math kernel library. Nomkl_mul stands for the numpy without math kernel library. mkl based numpy was installed in an anaconda enviroment using conda to install numpy whereas the numpy installed with pip doesn't integrate math kernel library. Xena has nodes with single GPU and dual GPU. Dual GPU node offers users 2x11GB of computational GPU memory which allows the use of a larger batch size and double the number of cores for faster implementation of highly complex models with a large number of parameters. The GPU_mul corresponds to multiplication operations utilizing single GPU node and dualgpu_mul corresonds to the one utilizing a dual GPU node. Another interesting benchmark was performed for inversion operations similar to those done for multiplication Fig 1. Time for Matrix Inversion vs size of Matrix N Fig 2. Time for Matrix Multiplication vs size of Matrix N The implementations can be found here. Tensorflow is an open source deep learning library provided by Google. It provides primitives for functions definitions on tensor and a mechanism to compute their derivatives automatically. It uses a tensor to represent any multidimensional array of numbers. Comparision between Numpy and Tensorflow TensorFlow's computational housing is a tensor, similar to Numpy's housing of data in Ndarray's making both of them N-d array libraries. However, Numpy does not offer a method to create tensor functions and automatically compute derivatives, nor does it support GPU implementation. Thus, for processing data of higher dimensions,Tensorflow outperforms Numpy arrays due largely to its GPU implementations. Numpy vs Tensorflow Implementations Numpy Implementation of Matrix Addition import numpy as np a=np.zeros((2,2)) b=np.zeros((2,2)) np.sum(b,axis=0) a.shape np.reshape(b,(1,3)) Tensorflow Implementation of Matrix Addition import tensorflow as tf tf.InteractiveSession() a=tf.zeros((2,2)) b=tf.ones((2,2)) tf.reduce_sum(b,reduction_indices=1).eval() a.get_shape() tf.reshape(b,(1,3)).eval() It is important to note that tensorflow requires explicit evaluation, i.e, tensorflow computation defines a computational graph which only gets initialized with values after a session has been evaluated. Numpy for example a=np.zeros((2,2)) ; print(a) will immediately give the value of \"a\". However, for tensorflow: a=tf.zeros((2,2)) print(a) will not return the value of \"a\" until it is evaluated with print(ta.eval()) So It is important to understand how tensorflow works and initializes the environment. Tensorflow uses a \"session object\" which encapsulates the environment in which the tensors are evaluated. A Tensorflow (tf) session for performing multiplication is demonstrated below: a= tf.constant(9999999) b=tf.constant(111111111) c=a*b with tf.Session() as sess: print(sess.run(c)) print(c.eval()) Tensorflow firstly structures the program, creates a graph integrating the variables, and uses session to exectute the process. *** Tensorflow Variables *** Similar to other programming language variables, tensorflow uses a variable object to store and update the parameters. They are stored in memory buffers that contain tensors. TensorFlow variables must be initialized before they have values! This is in contrast with constant tensors: W=tf.Variable(tf.zeros((2,2)), name=\"weights\") R=tf.Variable(tf.random_normal((2,2)), name=\"Random_weights\") with tf.Session() as sess: sess.run(tf.initialize_all_variables()) print(sess.run(W)) print(sess.run(R)) Converting numpy data to tensor: a=np.zeros((3,3)) t_a=tf.convert_to_tensor(a) with tf.Session() as sess: print(sess.run(t_a)) For scalable variables for performing operations we can use tf.placeholder which defines a placeholder and provides entry points for the data to be viewed in a computational graph. feed_dict is used in the below example to map from tf.placeholder variables to data (np arrays, list, etc). input1= tf.placeholder(tf.float32) input2 = tf.placeholder(tf.float32) output = tf.multiply(input1, input2) with tf.Session() as sess: print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))","title":"Tensorflow"},{"location":"Tensorflow_documentation/#introduction-to-tensorflow","text":"The relatively recent mainstream availability of complex algorithms and computationally efficient hardware is creating a platform for new innovations never before available to the scientific computing world. Since the development of computer systems, computing times have been drastically reduced, making more complex computations feasible. The continuous cycle of improvements in computation speeds and hardware leading to ever more complex computations can be seen in the scaling of hardware to meet these more complex computation goals. A resolution to this cycle can be found in the utilization of GPU's in high-performance computing for Machine learning and deep learning algorithms. Most of the complex computing strategies can be simplified into basic linear algebra operations such as addition, multiplication, subtraction, inversion and such. Out of the listed operations, matrix multiplication and inversion are the most computationally expensive operations. Most matrix operations are performed sequentially on the CPU resulting in computation time that scales with the size of the matrix as a factor \u03b8(n 3 ). Hence, computation cycles and duration of time to be allocated towards computation is proportional to the size of matrices under the constrained hardware with limited cache memory and RAM. The same problem still exists with multicore systems or distributed systems due to the threshold on the resources mentioned. On the other hand, a GPU is composed of several thousand cores, combining to provide the user with several GBs of computational memory compared to the MBs provided by CPU cache memory. The distributed computing this configuration provides enables parallelism across GPU cores and allows a super fast flow of data resulting from incredibly high bandwidth. This distribution across multiple cores amounts to massively reduced computation time as the device is able to scale its performance with data size. The enormous gains in computation time should give researchers a valid reason to switch from CPUs to GPUs for computationally heavy operations where the CPU-based operations do not scale with the data at a constant rate. Utilization of this methodology will provide enormous benefits for the computationally heavy domains such as machine learning, deep learning, linear algebra, optimization, data structures, etc. To illustrate this statement, we've included some benchmarks run on the Xena system at CARC using intensive linear algebra operations, i.e., matrix multiplication and matrix-inversion, on a CPU only compared to CPU utilizing the GPU as well. The CPU version was deployed on a multicore processor with 16 cores and 64GB of RAM with using numpy arrays in python. The GPU Version was deployed on NVIDIA Tesla K40 with 11GB of GPU memory using Tensorflow. Here the CPU implementations were carried with two different types of numpy compilations. mkl_mul stands for multiplication operation carried with numpy compiled with math kernel library. Nomkl_mul stands for the numpy without math kernel library. mkl based numpy was installed in an anaconda enviroment using conda to install numpy whereas the numpy installed with pip doesn't integrate math kernel library. Xena has nodes with single GPU and dual GPU. Dual GPU node offers users 2x11GB of computational GPU memory which allows the use of a larger batch size and double the number of cores for faster implementation of highly complex models with a large number of parameters. The GPU_mul corresponds to multiplication operations utilizing single GPU node and dualgpu_mul corresonds to the one utilizing a dual GPU node. Another interesting benchmark was performed for inversion operations similar to those done for multiplication Fig 1. Time for Matrix Inversion vs size of Matrix N Fig 2. Time for Matrix Multiplication vs size of Matrix N The implementations can be found here. Tensorflow is an open source deep learning library provided by Google. It provides primitives for functions definitions on tensor and a mechanism to compute their derivatives automatically. It uses a tensor to represent any multidimensional array of numbers. Comparision between Numpy and Tensorflow TensorFlow's computational housing is a tensor, similar to Numpy's housing of data in Ndarray's making both of them N-d array libraries. However, Numpy does not offer a method to create tensor functions and automatically compute derivatives, nor does it support GPU implementation. Thus, for processing data of higher dimensions,Tensorflow outperforms Numpy arrays due largely to its GPU implementations. Numpy vs Tensorflow Implementations Numpy Implementation of Matrix Addition import numpy as np a=np.zeros((2,2)) b=np.zeros((2,2)) np.sum(b,axis=0) a.shape np.reshape(b,(1,3)) Tensorflow Implementation of Matrix Addition import tensorflow as tf tf.InteractiveSession() a=tf.zeros((2,2)) b=tf.ones((2,2)) tf.reduce_sum(b,reduction_indices=1).eval() a.get_shape() tf.reshape(b,(1,3)).eval() It is important to note that tensorflow requires explicit evaluation, i.e, tensorflow computation defines a computational graph which only gets initialized with values after a session has been evaluated. Numpy for example a=np.zeros((2,2)) ; print(a) will immediately give the value of \"a\". However, for tensorflow: a=tf.zeros((2,2)) print(a) will not return the value of \"a\" until it is evaluated with print(ta.eval()) So It is important to understand how tensorflow works and initializes the environment. Tensorflow uses a \"session object\" which encapsulates the environment in which the tensors are evaluated. A Tensorflow (tf) session for performing multiplication is demonstrated below: a= tf.constant(9999999) b=tf.constant(111111111) c=a*b with tf.Session() as sess: print(sess.run(c)) print(c.eval()) Tensorflow firstly structures the program, creates a graph integrating the variables, and uses session to exectute the process. *** Tensorflow Variables *** Similar to other programming language variables, tensorflow uses a variable object to store and update the parameters. They are stored in memory buffers that contain tensors. TensorFlow variables must be initialized before they have values! This is in contrast with constant tensors: W=tf.Variable(tf.zeros((2,2)), name=\"weights\") R=tf.Variable(tf.random_normal((2,2)), name=\"Random_weights\") with tf.Session() as sess: sess.run(tf.initialize_all_variables()) print(sess.run(W)) print(sess.run(R)) Converting numpy data to tensor: a=np.zeros((3,3)) t_a=tf.convert_to_tensor(a) with tf.Session() as sess: print(sess.run(t_a)) For scalable variables for performing operations we can use tf.placeholder which defines a placeholder and provides entry points for the data to be viewed in a computational graph. feed_dict is used in the below example to map from tf.placeholder variables to data (np arrays, list, etc). input1= tf.placeholder(tf.float32) input2 = tf.placeholder(tf.float32) output = tf.multiply(input1, input2) with tf.Session() as sess: print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))","title":"Introduction to Tensorflow"},{"location":"Using_GPUs_on_Xena_with_MATLAB/","text":"Using GPUs with MATLAB Using a single GPU on Xena Use GPU in Interactive Session Identify and Select GPU Using Arrays on GPU Initialize Array Test if array is on GPU Retrieve Array from GPU Use functions on GPU Arrays Schedule a job MATLAB Script PBS Script Slurm Script Submit Job to Queue Using Multiple GPUs on a single Xena node MATLAB Script Slurm Script Submit Job to Queue Using Multiple Nodes with their own GPUs Using a single GPU on Xena MATLAB allows the utilization of a single GPU that is part of a machine. The following sections show how to access and utilize a GPU on xena. Use GPU in Interactive Session First, we will open MATLAB in an interactive session on a xena compute node. Identify and Select GPU Start by requesting an interactive session: xena:~$ srun -G 1 --pty bash Once you have a node allocated to you, load the MATLAB module and start a MATLAB session: xena01:~$ module load matlab xena01:~$ matlab To get started, type doc. For product information, visit www.mathworks.com. >> Now you can check to see the number of GPUs available: >> gpuDeviceCount(\"available\") You should see the following: ans = 1 This means that you have access to a single GPU. To get information about the available gpus, use this function: >> gpuDeviceTable That will print something that looks like this on Xena: ans = 1x5 table Index Name ComputeCapability DeviceAvailable DeviceSelected _____ ____________ _________________ _______________ ______________ 1 \"Tesla K40m\" \"3.5\" true false Next, you can tell MATLAB which GPU to use (pass in the desired index from the above table). If you do not do this, MATLAB will automatically grab the lowest index GPU when you try to use one. >> gpuDevice(1) Running the gpuDeviceTable command again shows this change: >> gpuDeviceTable ans = 1x5 table Index Name ComputeCapability DeviceAvailable DeviceSelected _____ ____________ _________________ _______________ ______________ 1 \"Tesla K40m\" \"3.5\" true true Using Arrays on GPU In order to utilize the GPU, data must be loaded into a gpuArray object. For a full description of the gpuArray object, please visit the official MathWorks Documentation at https://www.mathworks.com/help/parallel-computing/gpuarray.html Initialize Array First, create a normal array using any method you like. In this example we will use the magic(8) function to create a magic square matrix that is 8x8. >> A = magic(8) Next, pass that into a gpuArray object. This will copy the contents of a normal array into an array on the GPU. >> B = gpuArray(A) Test if array is on GPU The isgpuarray function tests if an array is on a GPU: >> isgpuarray(A) ans = logical 0 >> isgpuarray(B) ans = logical 1 This confirms that array A is not on the GPU, but array B is. Retrieve Array from GPU In order to retrieve an array from the GPU and put it back in the MATLAB workspace, use the gather function. It will copy the contents of an array on the GPU into a normal array. This is neccesary if you want to to perform non-GPU actions on your data after using the GPU. >> C = gather(B) Now, we can test to see if C is stored on the gpu: >> isgpuarray(C) ans = logical 0 Use functions on GPU Arrays To perform functions on gpuArray objects, use the arrayfun function. In this example, we will apply the MATLAB sqrt function to the array (B) that we created in the previous step: result = arrayfun(@sqrt,B) This will apply the sqrt function to every element in the GPU array. result is also a GPU array: >> isgpuarray(result) ans = logical 1 To see a list of MATLAB functions that are supported using gpus, visit https://www.mathworks.com/help/parallel-computing/gpuarray.html You can also create your own functions to pass into arrayfun . Schedule a job It is good idea to do everything using a batch script and avoid the mistakes associated with interactive computing. To get an idea of why performing functions on gpuArray objects is a good idea, let's create a simple MATLAB script that displays the amount of time it takes to perform the same computation on a cpu and on a gpu. We will then create a PBS script that schedules a job with a GPU to run the MATLAB script for us. MATLAB Script We will perform the sqrt function on a 5000x5000 array. The use of tic and toc allow us to time the seperate applications of sqrt . Create the following script with the name gpu_matlab.m : gpuDevice(1); A = magic(5000); disp(\"sqrt of 5000x5000 matrix on cpu:\") tic B = arrayfun(@sqrt, A); toc disp(\"sqrt of 5000x5000 matrix on gpu:\") C = gpuArray(magic(5000)); tic D = arrayfun(@sqrt,C); toc PBS Script Now, let's create a PBS script called gpu_matlab.pbs . Replace the <DIR> with the path to the directory containing the MATLAB script created above. This script will request the desired resrouces, load the MATLAB module, then run the script. The output of the script will be sent to the file: gpu_matlab.out #!/bin/bash #PBS -N gpu_test #PBS -l walltime=00:05:00 #PBS -l nodes=1:ppn=1:gpus=1 #PBS -j oe cd <DIR> module load matlab matlab -nodisplay -r gpu_matlab > gpu_matlab.out Slurm Script Now, let's create a Slurm script called gpu_matlab.sh . Replace the <DIR> with the path to the directory containing the MATLAB script created above. This script will request the desired resrouces, load the MATLAB module, then run the script. The output of the script will be sent to the file: gpu_matlab.out #!/bin/bash #SBATCH --job-name gpu_matlab_job #SBATCH --output gpu_matlab_job.out #SBATCH --error gpu_matlab_job.err #SBATCH --time 00:05:00 #SBATCH --ntasks 1 #SBATCH -G 1 cd <DIR> module load matlab matlab -nodisplay -r gpu_matlab > gpu_matlab.out Submit Job to Queue Now we can submit the job to the scheduler from the xena head node: PBS script version: xena:~$ qsub gpu_matlab.pbs Slurm script version: xena:~$ sbatch gpu_matlab.sh View the results: xena:~$ cat gpu_matlab.out Using Multiple GPUs on a single Xena node Xena contains some nodes with two GPUs. MATLAB allows for the utilization of multiple GPUs on a single node in the same way you use multiple CPUs. To show how this works, below is an example MATLAB script that will create a logistic map using all available GPU's on the assigned node. The parpool object is used to create workers to parallelize the execution. Each worker will grab it's own GPU when performing actions with gpuArray objects. For this to work properly, ensure that you have been allocated an equal number of CPUs as GPUs on the machine. An example slurm script is included below to give an idea of how to ask for the proper resources to be allocated. MATLAB Script Create the following MATLAB script called gpu_logistic_map.m This simple MATLAB script creates a Logistic Map by iterating the logistic equation on a set of random populations. A worker is created for each available GPU. They will then split up the work performed in the parfor loop. The result is a logistic map figure saved as 'logistic_map.jpg' It also contains calls to time the execution of the parfor loop. N = 1000; r = gpuArray.linspace(0,4,N); numIterations = 1000; numGPUs = gpuDeviceCount(\"available\"); parpool(numGPUs); numSimulations = 100; X = zeros(numSimulations,N,'gpuArray'); disp(\"Timing execution of parfor loop:\") tic parfor i=1:numSimulations X(i,:) = rand(1,N,'gpuArray') for n=1:numIterations X(i,:) = r.*X(i,:).*(1-X(i,:)); end end toc f = figure('visible','off'); plot(r,X,'.'); saveas(f,'logistic_map','jpg') return Slurm Script When using the --partition dualGPU flag on xena, you must also set --cpus-per-task 2 and -G 2 for MATLAB to correctly find and utilize the available GPUs. These numbers should match, as MATLAB will use a CPU to access each GPU. For this partition, we ask for two CPUs and two GPUs. Create the following slurm scrpt called gpu_logistic_map.sh . Replace the <DIR> with the path to the directory containing the MATLAB script created above. This script will ask the scheduler for the proper resources. Once the resrouces are allocated, the script will run the MATLAB script from the above step. The MATLAB script will create a .jpg image once it has finished. Any output of the MATLAB script is redirected to gpu_logistic_map.out . #!/bin/bash #SBATCH --job-name gpu_logistic_map_job #SBATCH --output gpu_logistic_map_job.out #SBATCH --error gpu_logistic_map_job.err #SBATCH --time 00:10:00 #SBATCH --partition dualGPU #SBATCH --ntasks 1 #SBATCH --cpus-per-task 2 #SBATCH -G 2 cd <DIR> module load matlab matlab -nodisplay -r gpu_logistic_map > gpu_logistic_map.out Submit Job to Queue Now we can submit the job to the scheduler from the xena head node: xena:~$ sbatch gpu_logistic_map.sh View the results: xena:~$ cat gpu_logistic_map.out You can also view the logistic_map.jpg image using your preferred method. Using Multiple Nodes with their own GPUs Coming Soon! (Maybe)","title":"Using GPUs with MATLAB"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#using-gpus-with-matlab","text":"Using a single GPU on Xena Use GPU in Interactive Session Identify and Select GPU Using Arrays on GPU Initialize Array Test if array is on GPU Retrieve Array from GPU Use functions on GPU Arrays Schedule a job MATLAB Script PBS Script Slurm Script Submit Job to Queue Using Multiple GPUs on a single Xena node MATLAB Script Slurm Script Submit Job to Queue Using Multiple Nodes with their own GPUs","title":"Using GPUs with MATLAB"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#using-a-single-gpu-on-xena","text":"MATLAB allows the utilization of a single GPU that is part of a machine. The following sections show how to access and utilize a GPU on xena.","title":"Using a single GPU on Xena"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#use-gpu-in-interactive-session","text":"First, we will open MATLAB in an interactive session on a xena compute node.","title":"Use GPU in Interactive Session"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#identify-and-select-gpu","text":"Start by requesting an interactive session: xena:~$ srun -G 1 --pty bash Once you have a node allocated to you, load the MATLAB module and start a MATLAB session: xena01:~$ module load matlab xena01:~$ matlab To get started, type doc. For product information, visit www.mathworks.com. >> Now you can check to see the number of GPUs available: >> gpuDeviceCount(\"available\") You should see the following: ans = 1 This means that you have access to a single GPU. To get information about the available gpus, use this function: >> gpuDeviceTable That will print something that looks like this on Xena: ans = 1x5 table Index Name ComputeCapability DeviceAvailable DeviceSelected _____ ____________ _________________ _______________ ______________ 1 \"Tesla K40m\" \"3.5\" true false Next, you can tell MATLAB which GPU to use (pass in the desired index from the above table). If you do not do this, MATLAB will automatically grab the lowest index GPU when you try to use one. >> gpuDevice(1) Running the gpuDeviceTable command again shows this change: >> gpuDeviceTable ans = 1x5 table Index Name ComputeCapability DeviceAvailable DeviceSelected _____ ____________ _________________ _______________ ______________ 1 \"Tesla K40m\" \"3.5\" true true","title":"Identify and Select GPU"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#using-arrays-on-gpu","text":"In order to utilize the GPU, data must be loaded into a gpuArray object. For a full description of the gpuArray object, please visit the official MathWorks Documentation at https://www.mathworks.com/help/parallel-computing/gpuarray.html","title":"Using Arrays on GPU"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#initialize-array","text":"First, create a normal array using any method you like. In this example we will use the magic(8) function to create a magic square matrix that is 8x8. >> A = magic(8) Next, pass that into a gpuArray object. This will copy the contents of a normal array into an array on the GPU. >> B = gpuArray(A)","title":"Initialize Array"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#test-if-array-is-on-gpu","text":"The isgpuarray function tests if an array is on a GPU: >> isgpuarray(A) ans = logical 0 >> isgpuarray(B) ans = logical 1 This confirms that array A is not on the GPU, but array B is.","title":"Test if array is on GPU"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#retrieve-array-from-gpu","text":"In order to retrieve an array from the GPU and put it back in the MATLAB workspace, use the gather function. It will copy the contents of an array on the GPU into a normal array. This is neccesary if you want to to perform non-GPU actions on your data after using the GPU. >> C = gather(B) Now, we can test to see if C is stored on the gpu: >> isgpuarray(C) ans = logical 0","title":"Retrieve Array from GPU"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#use-functions-on-gpu-arrays","text":"To perform functions on gpuArray objects, use the arrayfun function. In this example, we will apply the MATLAB sqrt function to the array (B) that we created in the previous step: result = arrayfun(@sqrt,B) This will apply the sqrt function to every element in the GPU array. result is also a GPU array: >> isgpuarray(result) ans = logical 1 To see a list of MATLAB functions that are supported using gpus, visit https://www.mathworks.com/help/parallel-computing/gpuarray.html You can also create your own functions to pass into arrayfun .","title":"Use functions on GPU Arrays"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#schedule-a-job","text":"It is good idea to do everything using a batch script and avoid the mistakes associated with interactive computing. To get an idea of why performing functions on gpuArray objects is a good idea, let's create a simple MATLAB script that displays the amount of time it takes to perform the same computation on a cpu and on a gpu. We will then create a PBS script that schedules a job with a GPU to run the MATLAB script for us.","title":"Schedule a job"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#matlab-script","text":"We will perform the sqrt function on a 5000x5000 array. The use of tic and toc allow us to time the seperate applications of sqrt . Create the following script with the name gpu_matlab.m : gpuDevice(1); A = magic(5000); disp(\"sqrt of 5000x5000 matrix on cpu:\") tic B = arrayfun(@sqrt, A); toc disp(\"sqrt of 5000x5000 matrix on gpu:\") C = gpuArray(magic(5000)); tic D = arrayfun(@sqrt,C); toc","title":"MATLAB Script"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#pbs-script","text":"Now, let's create a PBS script called gpu_matlab.pbs . Replace the <DIR> with the path to the directory containing the MATLAB script created above. This script will request the desired resrouces, load the MATLAB module, then run the script. The output of the script will be sent to the file: gpu_matlab.out #!/bin/bash #PBS -N gpu_test #PBS -l walltime=00:05:00 #PBS -l nodes=1:ppn=1:gpus=1 #PBS -j oe cd <DIR> module load matlab matlab -nodisplay -r gpu_matlab > gpu_matlab.out","title":"PBS Script"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#slurm-script","text":"Now, let's create a Slurm script called gpu_matlab.sh . Replace the <DIR> with the path to the directory containing the MATLAB script created above. This script will request the desired resrouces, load the MATLAB module, then run the script. The output of the script will be sent to the file: gpu_matlab.out #!/bin/bash #SBATCH --job-name gpu_matlab_job #SBATCH --output gpu_matlab_job.out #SBATCH --error gpu_matlab_job.err #SBATCH --time 00:05:00 #SBATCH --ntasks 1 #SBATCH -G 1 cd <DIR> module load matlab matlab -nodisplay -r gpu_matlab > gpu_matlab.out","title":"Slurm Script"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#submit-job-to-queue","text":"Now we can submit the job to the scheduler from the xena head node: PBS script version: xena:~$ qsub gpu_matlab.pbs Slurm script version: xena:~$ sbatch gpu_matlab.sh View the results: xena:~$ cat gpu_matlab.out","title":"Submit Job to Queue"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#using-multiple-gpus-on-a-single-xena-node","text":"Xena contains some nodes with two GPUs. MATLAB allows for the utilization of multiple GPUs on a single node in the same way you use multiple CPUs. To show how this works, below is an example MATLAB script that will create a logistic map using all available GPU's on the assigned node. The parpool object is used to create workers to parallelize the execution. Each worker will grab it's own GPU when performing actions with gpuArray objects. For this to work properly, ensure that you have been allocated an equal number of CPUs as GPUs on the machine. An example slurm script is included below to give an idea of how to ask for the proper resources to be allocated.","title":"Using Multiple GPUs on a single Xena node"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#matlab-script_1","text":"Create the following MATLAB script called gpu_logistic_map.m This simple MATLAB script creates a Logistic Map by iterating the logistic equation on a set of random populations. A worker is created for each available GPU. They will then split up the work performed in the parfor loop. The result is a logistic map figure saved as 'logistic_map.jpg' It also contains calls to time the execution of the parfor loop. N = 1000; r = gpuArray.linspace(0,4,N); numIterations = 1000; numGPUs = gpuDeviceCount(\"available\"); parpool(numGPUs); numSimulations = 100; X = zeros(numSimulations,N,'gpuArray'); disp(\"Timing execution of parfor loop:\") tic parfor i=1:numSimulations X(i,:) = rand(1,N,'gpuArray') for n=1:numIterations X(i,:) = r.*X(i,:).*(1-X(i,:)); end end toc f = figure('visible','off'); plot(r,X,'.'); saveas(f,'logistic_map','jpg') return","title":"MATLAB Script"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#slurm-script_1","text":"When using the --partition dualGPU flag on xena, you must also set --cpus-per-task 2 and -G 2 for MATLAB to correctly find and utilize the available GPUs. These numbers should match, as MATLAB will use a CPU to access each GPU. For this partition, we ask for two CPUs and two GPUs. Create the following slurm scrpt called gpu_logistic_map.sh . Replace the <DIR> with the path to the directory containing the MATLAB script created above. This script will ask the scheduler for the proper resources. Once the resrouces are allocated, the script will run the MATLAB script from the above step. The MATLAB script will create a .jpg image once it has finished. Any output of the MATLAB script is redirected to gpu_logistic_map.out . #!/bin/bash #SBATCH --job-name gpu_logistic_map_job #SBATCH --output gpu_logistic_map_job.out #SBATCH --error gpu_logistic_map_job.err #SBATCH --time 00:10:00 #SBATCH --partition dualGPU #SBATCH --ntasks 1 #SBATCH --cpus-per-task 2 #SBATCH -G 2 cd <DIR> module load matlab matlab -nodisplay -r gpu_logistic_map > gpu_logistic_map.out","title":"Slurm Script"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#submit-job-to-queue_1","text":"Now we can submit the job to the scheduler from the xena head node: xena:~$ sbatch gpu_logistic_map.sh View the results: xena:~$ cat gpu_logistic_map.out You can also view the logistic_map.jpg image using your preferred method.","title":"Submit Job to Queue"},{"location":"Using_GPUs_on_Xena_with_MATLAB/#using-multiple-nodes-with-their-own-gpus","text":"Coming Soon! (Maybe)","title":"Using Multiple Nodes with their own GPUs"},{"location":"X11_forwarding/","text":"X11 Forwarding X11 Forwarding allows any graphical user interface to open on your client machine while the software itself is being run on a CARC cluster compute node. This QuickByte will show you how to do X11 forwarding yourself. On a Mac Before getting started, make sure you have XQuartz downloaded and installed. This allows the X11 forwarding. On a PC Before getting started, download MobaXterm and install. This has a built in xserver, that will connect your computer to CARC. Step by Step Example of X11 forwarding with MATLAB In terminal sign into the CARC system using secure system connection. However, use a -X flag to enable X11 forwarding. $ ssh -X username@wheeler.alliance.unm.edu Start an interactive session using qsub, but again include the -X flag. $ qsub -IX -l nodes=1:ppn=1 Note that the equivalent command using slurm would be as follows: srun --x11 --nodes=1 --ntasks=1 xterm Once you have been assigned a node, load the MATLAB module. Then start MATLAB. You should automatically get a MATLAB GUI poping us on your computer. $ module load matlab $ matlab Use the MATLAB GUI to load your add-on as you usually would. When you are finished loading your add-on, click the x to close the GUI. Lastly, exit the interactive session to release the node for other users. One can find a CARC quickbyte video on x forwarding by accessing the link below: https://www.youtube.com/watch?v=-5ic9JWHuqI&list=PLvr5gRBLi7VAzEB_t5aXOLHLfdIu2s1hZ&index=12 If you have any trouble at any point please reach out to us at help@carc.unm.edu","title":"X11 Forwarding"},{"location":"X11_forwarding/#x11-forwarding","text":"X11 Forwarding allows any graphical user interface to open on your client machine while the software itself is being run on a CARC cluster compute node. This QuickByte will show you how to do X11 forwarding yourself.","title":"X11 Forwarding"},{"location":"X11_forwarding/#on-a-mac","text":"Before getting started, make sure you have XQuartz downloaded and installed. This allows the X11 forwarding.","title":"On a Mac"},{"location":"X11_forwarding/#on-a-pc","text":"Before getting started, download MobaXterm and install. This has a built in xserver, that will connect your computer to CARC.","title":"On a PC"},{"location":"X11_forwarding/#step-by-step-example-of-x11-forwarding-with-matlab","text":"In terminal sign into the CARC system using secure system connection. However, use a -X flag to enable X11 forwarding. $ ssh -X username@wheeler.alliance.unm.edu Start an interactive session using qsub, but again include the -X flag. $ qsub -IX -l nodes=1:ppn=1 Note that the equivalent command using slurm would be as follows: srun --x11 --nodes=1 --ntasks=1 xterm Once you have been assigned a node, load the MATLAB module. Then start MATLAB. You should automatically get a MATLAB GUI poping us on your computer. $ module load matlab $ matlab Use the MATLAB GUI to load your add-on as you usually would. When you are finished loading your add-on, click the x to close the GUI. Lastly, exit the interactive session to release the node for other users. One can find a CARC quickbyte video on x forwarding by accessing the link below: https://www.youtube.com/watch?v=-5ic9JWHuqI&list=PLvr5gRBLi7VAzEB_t5aXOLHLfdIu2s1hZ&index=12 If you have any trouble at any point please reach out to us at help@carc.unm.edu","title":"Step by Step Example of X11 forwarding with MATLAB"},{"location":"alphafold/","text":"Alphafold # Alphafold predicts the 3D structure of proteins from their amino acid sequence. A deep learning system that uses a combination of sequence alignment, evolutionary information, and physical principles to generate its predictions. Primarily written in python, the first version of alphafold was released in 2016, and has been updated as recently as 2022. There are two ways to run alphafold here at CARC. Option 1 is to use localcolabfold. You may find that Localcolabfold is easier to get running, however it will come with tradeoffs in certain areas, For Example, localcolabfold uses the pdb70 database, and there is not a great way to choose a different database to use. If you are unsure which version is best for you, we recommend you review the readme & issues for localcolabfold to determine if there are any features you may need. Alphafold w/ LocalColabFold: Localcolabfold is set up as part of our module system at CARC. As with anything as part of the module system, you can use module spider localcolabfold to get more information about the module. This will also list other dependencies, if any, such as a different compiler. localColabFold runs with the command colabfold_batch . The only other thing you will need to provide is the input fasta file. Below is an example of a slurm script to run using localcolabfold on Hopper. #!/bin/bash #SBATCH --job-name alphafold #SBATCH --mail-user=<your_email> #SBATCH --partition general #SBATCH --Nodes 1 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 64 #SBATCH --time 1:00:00 module load localcolabfold INPUT_FILE_PATH=/path/to/input.fasta now=$(date +\"%m_%d_%H_%M_%S\") OUTPUTDIR=$SLURM_SUBMIT_DIR/$SLURM_JOB_NAME-$now mkdir -p $OUTPUTDIR cd $SLURM_SUBMIT_DIR srun colabfold_batch $INPUT_FILE_PATH $OUTPUTDIR Reference the localcolabfold documentation for flags you may find useful. Alphafold w/ Singularity Image: Choose your alphafold version ## There are multiple versions of alphafold installed using singularity images. You can view each of the versions installed with the command: ls /projects/shared/singularity/alphafold* and hit tab. You'll see below we currently have version 2.0 and 2.3.1 installed. For this tutorial we will be using version 2.0. For this tutorial we will be using version 2.0. Now we can create a new directory with mkdir alphafold and move into that directory with cd alphafold Running Alphafold Inside the alphafold directory, you will be able to run the program using the slurm script, this script will differ based on the machine you are using. Xena is the machine at CARC that has GPU resources, so you will need to use xena if you hope to run using the gpus. Choose one of the scripts below, in this case we will be using Hopper. Create a new file using your favorite editor. For example, vim alphafold.sh then hit i to go into insert mode, and past the contents from the below script into this file. You can then add your email to get alerts about the run. When you are finished editing this file, type ESC to exit insert mode, followed by :wq to write & quite the file, this will save your changes. Xena Script Here, we are passing two additional flags when running the script, the first is --partition=singleGPU which will make sure we are assigned a node that only has a single gpu. The second is -G 1 which is what tells the program to use the gpu. While optimizing, you might find that switching to one of the nodes with multiple gpus will increase your speed. You can achieve this by instead adding the --partition=dualGPU as well as -G 2 . #SBATCH --job-name alphafold #SBATCH --time=08:00:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=20G #SBATCH --partition=singleGPU #SBATCH --output alphafold.out #SBATCH --error alphafold.err #SBATCH -G 1 #SBATCH --mail-user < your email > #SBATCH --mail-type all module load singularity # Specify input/output paths SINGULARITY_IMAGE_PATH=/projects/shared/singularity/ ALPHAFOLD_DATA_PATH=/carc/scratch/shared/alphafold/data/70 ALPHAFOLD_MODELS=$ALPHAFOLD_DATA_PATH/params ALPHAFOLD_INPUT_FASTA=$SLURM_SUBMIT_DIR/input_test.fasta NOW=$(date +\"%Y_%m_%d_%H_%M_%S\") ALPHAFOLD_OUTPUT_DIR=$SLURM_SUBMIT_DIR/alphafold_output-$NOW mkdir -p $ALPHAFOLD_OUTPUT_DIR #Run the command singularity run --nv \\ --bind $ALPHAFOLD_DATA_PATH:/data \\ --bind $ALPHAFOLD_MODELS \\ --bind $ALPHAFOLD_OUTPUT_DIR:/alphafold_output \\ --bind $ALPHAFOLD_INPUT_FASTA:/input.fasta \\ --bind .:/etc \\ --pwd /app/alphafold $SINGULARITY_IMAGE_PATH/alphafold-2.0.sif \\ --fasta_paths=/input.fasta \\ --uniref90_database_path=/data/uniref90/uniref90.fasta \\ --data_dir=/data \\ --mgnify_database_path=/data/mgnify/mgy_clusters.fa \\ --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --uniclust30_database_path=/data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\ --pdb70_database_path=/data/pdb70/pdb70 \\ --template_mmcif_dir=/data/pdb_mmcif/mmcif_files \\ --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\ --max_template_date=2020-05-14 \\ --output_dir=/alphafold_output \\ --model_names='model_1' \\ --preset=casp14 Hopper Script #!/bin/bash #SBATCH --job-name alphafold #SBATCH --time=08:00:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=32 #SBATCH --mem=0G #SBATCH --partition=general #SBATCH --output alphafold.out #SBATCH --error alphafold.err #SBATCH --mail-user < your email > #SBATCH --mail-type all module load singularity # Specify input/output paths SINGULARITY_IMAGE_PATH=/projects/shared/singularity/ ALPHAFOLD_DATA_PATH=/carc/scratch/shared/alphafold/data/70 ALPHAFOLD_MODELS=$ALPHAFOLD_DATA_PATH/params ALPHAFOLD_INPUT_FASTA=$SLURM_SUBMIT_DIR/input_test.fasta NOW=$(date +\"%Y_%m_%d_%H_%M_%S\") ALPHAFOLD_OUTPUT_DIR=$SLURM_SUBMIT_DIR/alphafold_output-$NOW mkdir -p $ALPHAFOLD_OUTPUT_DIR #Run the command singularity run --nv \\ --bind $ALPHAFOLD_DATA_PATH:/data \\ --bind $ALPHAFOLD_MODELS \\ --bind $ALPHAFOLD_OUTPUT_DIR:/alphafold_output \\ --bind $ALPHAFOLD_INPUT_FASTA:/input.fasta \\ --bind .:/etc \\ --pwd /app/alphafold $SINGULARITY_IMAGE_PATH/alphafold-2.0.sif \\ --fasta_paths=/input.fasta \\ --uniref90_database_path=/data/uniref90/uniref90.fasta \\ --data_dir=/data \\ --mgnify_database_path=/data/mgnify/mgy_clusters.fa \\ --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --uniclust30_database_path=/data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\ --pdb70_database_path=/data/pdb70/pdb70 \\ --template_mmcif_dir=/data/pdb_mmcif/mmcif_files \\ --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\ --max_template_date=2020-05-14 \\ --output_dir=/alphafold_output \\ --model_names='model_1' \\ --preset=casp14 Input File These scripts expect you to have a file named input_test.fasta where you will give your input sequence. This should be in the format: (alphafold/input_test.fasta) > 350 residue example sequence MTANHLESPNCDWKNNRMAIVHMVNVTPLRMMEEPRAAVEAAFEGIMEPAVVGDMVEYWNKMISTCCNYYQMGSSRSHLEEKAQMVDRFWFCPCIYYASGKWRNMFLNILHVWGHHHYPRNDLKPCSYLSCKLPDLRIFFNHMQTCCHFVTLLFLTEWPTYMIYNSVDLCPMTIPRRNTCRTMTEVSSWCEPAIPEWWQATVKGGWMSTHTKFCWYPVLDPHHEYAESKMDTYGQCKKGGMVRCYKHKQQVWGNNHNESKAPCDDQPTYLCPPGEVYKGDHISKREAENMTNAWLGEDTHNFMEIMHCTAKMASTHFGSTTIYWAWGGHVRPAATWRVYPMIQEGSHCQC localtime Your job will fail within the first few moments if the input file is not formatted properly. It will also require you to have a localtime file in the directory in which you are running, which you can create with touch localtime This file does not directly impact the simulation in any way, but is used to track the time to make your tests reproducible. If this file is empty, it will default to using the current time in UTC, but you could also place the correct time in: (alphafold/localtime) 2022-10-28T16:15:29 Run You can now run your alphafold sequence with the command sbatch alphafold.sh This will hand your script you made above to the slurm scheduler. If you added your email to the script, you will receive an email that it has been added to the queue, once it starts, and once it ends. It will also email if it fails before a successful completion. If your run fails, more information can be found in the alphafold.out & alphafold.err files which will be generated as each run begins. You can check if your run is still running with squeue --me This will list all jobs you currently have both queued & running. On the general partitions your time will be limited to between 4 and 48 hours of runtime. Output After a successful job, you will notice multiple output files. They will be placed in the ./alphafold_output-<Timestamp>/input/* Where you will have the resulting .pdb, .pkl, and .json files. Now that it is on your local computer, you can now view this file on your computer if you have the proper software to view a pdb file.","title":"Alphafold"},{"location":"alphafold/#alphafold","text":"Alphafold predicts the 3D structure of proteins from their amino acid sequence. A deep learning system that uses a combination of sequence alignment, evolutionary information, and physical principles to generate its predictions. Primarily written in python, the first version of alphafold was released in 2016, and has been updated as recently as 2022. There are two ways to run alphafold here at CARC. Option 1 is to use localcolabfold. You may find that Localcolabfold is easier to get running, however it will come with tradeoffs in certain areas, For Example, localcolabfold uses the pdb70 database, and there is not a great way to choose a different database to use. If you are unsure which version is best for you, we recommend you review the readme & issues for localcolabfold to determine if there are any features you may need.","title":"Alphafold #"},{"location":"alphafold/#alphafold-w-localcolabfold","text":"Localcolabfold is set up as part of our module system at CARC. As with anything as part of the module system, you can use module spider localcolabfold to get more information about the module. This will also list other dependencies, if any, such as a different compiler. localColabFold runs with the command colabfold_batch . The only other thing you will need to provide is the input fasta file. Below is an example of a slurm script to run using localcolabfold on Hopper. #!/bin/bash #SBATCH --job-name alphafold #SBATCH --mail-user=<your_email> #SBATCH --partition general #SBATCH --Nodes 1 #SBATCH --ntasks 1 #SBATCH --cpus-per-task 64 #SBATCH --time 1:00:00 module load localcolabfold INPUT_FILE_PATH=/path/to/input.fasta now=$(date +\"%m_%d_%H_%M_%S\") OUTPUTDIR=$SLURM_SUBMIT_DIR/$SLURM_JOB_NAME-$now mkdir -p $OUTPUTDIR cd $SLURM_SUBMIT_DIR srun colabfold_batch $INPUT_FILE_PATH $OUTPUTDIR Reference the localcolabfold documentation for flags you may find useful.","title":"Alphafold w/ LocalColabFold:"},{"location":"alphafold/#alphafold-w-singularity-image","text":"","title":"Alphafold w/ Singularity Image:"},{"location":"alphafold/#choose-your-alphafold-version","text":"There are multiple versions of alphafold installed using singularity images. You can view each of the versions installed with the command: ls /projects/shared/singularity/alphafold* and hit tab. You'll see below we currently have version 2.0 and 2.3.1 installed. For this tutorial we will be using version 2.0. For this tutorial we will be using version 2.0. Now we can create a new directory with mkdir alphafold and move into that directory with cd alphafold","title":"Choose your alphafold version ##"},{"location":"alphafold/#running-alphafold","text":"Inside the alphafold directory, you will be able to run the program using the slurm script, this script will differ based on the machine you are using. Xena is the machine at CARC that has GPU resources, so you will need to use xena if you hope to run using the gpus. Choose one of the scripts below, in this case we will be using Hopper. Create a new file using your favorite editor. For example, vim alphafold.sh then hit i to go into insert mode, and past the contents from the below script into this file. You can then add your email to get alerts about the run. When you are finished editing this file, type ESC to exit insert mode, followed by :wq to write & quite the file, this will save your changes.","title":"Running Alphafold"},{"location":"alphafold/#xena-script","text":"Here, we are passing two additional flags when running the script, the first is --partition=singleGPU which will make sure we are assigned a node that only has a single gpu. The second is -G 1 which is what tells the program to use the gpu. While optimizing, you might find that switching to one of the nodes with multiple gpus will increase your speed. You can achieve this by instead adding the --partition=dualGPU as well as -G 2 . #SBATCH --job-name alphafold #SBATCH --time=08:00:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=20G #SBATCH --partition=singleGPU #SBATCH --output alphafold.out #SBATCH --error alphafold.err #SBATCH -G 1 #SBATCH --mail-user < your email > #SBATCH --mail-type all module load singularity # Specify input/output paths SINGULARITY_IMAGE_PATH=/projects/shared/singularity/ ALPHAFOLD_DATA_PATH=/carc/scratch/shared/alphafold/data/70 ALPHAFOLD_MODELS=$ALPHAFOLD_DATA_PATH/params ALPHAFOLD_INPUT_FASTA=$SLURM_SUBMIT_DIR/input_test.fasta NOW=$(date +\"%Y_%m_%d_%H_%M_%S\") ALPHAFOLD_OUTPUT_DIR=$SLURM_SUBMIT_DIR/alphafold_output-$NOW mkdir -p $ALPHAFOLD_OUTPUT_DIR #Run the command singularity run --nv \\ --bind $ALPHAFOLD_DATA_PATH:/data \\ --bind $ALPHAFOLD_MODELS \\ --bind $ALPHAFOLD_OUTPUT_DIR:/alphafold_output \\ --bind $ALPHAFOLD_INPUT_FASTA:/input.fasta \\ --bind .:/etc \\ --pwd /app/alphafold $SINGULARITY_IMAGE_PATH/alphafold-2.0.sif \\ --fasta_paths=/input.fasta \\ --uniref90_database_path=/data/uniref90/uniref90.fasta \\ --data_dir=/data \\ --mgnify_database_path=/data/mgnify/mgy_clusters.fa \\ --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --uniclust30_database_path=/data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\ --pdb70_database_path=/data/pdb70/pdb70 \\ --template_mmcif_dir=/data/pdb_mmcif/mmcif_files \\ --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\ --max_template_date=2020-05-14 \\ --output_dir=/alphafold_output \\ --model_names='model_1' \\ --preset=casp14","title":"Xena Script"},{"location":"alphafold/#hopper-script","text":"#!/bin/bash #SBATCH --job-name alphafold #SBATCH --time=08:00:00 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=32 #SBATCH --mem=0G #SBATCH --partition=general #SBATCH --output alphafold.out #SBATCH --error alphafold.err #SBATCH --mail-user < your email > #SBATCH --mail-type all module load singularity # Specify input/output paths SINGULARITY_IMAGE_PATH=/projects/shared/singularity/ ALPHAFOLD_DATA_PATH=/carc/scratch/shared/alphafold/data/70 ALPHAFOLD_MODELS=$ALPHAFOLD_DATA_PATH/params ALPHAFOLD_INPUT_FASTA=$SLURM_SUBMIT_DIR/input_test.fasta NOW=$(date +\"%Y_%m_%d_%H_%M_%S\") ALPHAFOLD_OUTPUT_DIR=$SLURM_SUBMIT_DIR/alphafold_output-$NOW mkdir -p $ALPHAFOLD_OUTPUT_DIR #Run the command singularity run --nv \\ --bind $ALPHAFOLD_DATA_PATH:/data \\ --bind $ALPHAFOLD_MODELS \\ --bind $ALPHAFOLD_OUTPUT_DIR:/alphafold_output \\ --bind $ALPHAFOLD_INPUT_FASTA:/input.fasta \\ --bind .:/etc \\ --pwd /app/alphafold $SINGULARITY_IMAGE_PATH/alphafold-2.0.sif \\ --fasta_paths=/input.fasta \\ --uniref90_database_path=/data/uniref90/uniref90.fasta \\ --data_dir=/data \\ --mgnify_database_path=/data/mgnify/mgy_clusters.fa \\ --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --uniclust30_database_path=/data/uniclust30/uniclust30_2018_08/uniclust30_2018_08 \\ --pdb70_database_path=/data/pdb70/pdb70 \\ --template_mmcif_dir=/data/pdb_mmcif/mmcif_files \\ --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\ --max_template_date=2020-05-14 \\ --output_dir=/alphafold_output \\ --model_names='model_1' \\ --preset=casp14","title":"Hopper Script"},{"location":"alphafold/#input-file","text":"These scripts expect you to have a file named input_test.fasta where you will give your input sequence. This should be in the format: (alphafold/input_test.fasta) > 350 residue example sequence MTANHLESPNCDWKNNRMAIVHMVNVTPLRMMEEPRAAVEAAFEGIMEPAVVGDMVEYWNKMISTCCNYYQMGSSRSHLEEKAQMVDRFWFCPCIYYASGKWRNMFLNILHVWGHHHYPRNDLKPCSYLSCKLPDLRIFFNHMQTCCHFVTLLFLTEWPTYMIYNSVDLCPMTIPRRNTCRTMTEVSSWCEPAIPEWWQATVKGGWMSTHTKFCWYPVLDPHHEYAESKMDTYGQCKKGGMVRCYKHKQQVWGNNHNESKAPCDDQPTYLCPPGEVYKGDHISKREAENMTNAWLGEDTHNFMEIMHCTAKMASTHFGSTTIYWAWGGHVRPAATWRVYPMIQEGSHCQC","title":"Input File"},{"location":"alphafold/#localtime","text":"Your job will fail within the first few moments if the input file is not formatted properly. It will also require you to have a localtime file in the directory in which you are running, which you can create with touch localtime This file does not directly impact the simulation in any way, but is used to track the time to make your tests reproducible. If this file is empty, it will default to using the current time in UTC, but you could also place the correct time in: (alphafold/localtime) 2022-10-28T16:15:29","title":"localtime"},{"location":"alphafold/#run","text":"You can now run your alphafold sequence with the command sbatch alphafold.sh This will hand your script you made above to the slurm scheduler. If you added your email to the script, you will receive an email that it has been added to the queue, once it starts, and once it ends. It will also email if it fails before a successful completion. If your run fails, more information can be found in the alphafold.out & alphafold.err files which will be generated as each run begins. You can check if your run is still running with squeue --me This will list all jobs you currently have both queued & running. On the general partitions your time will be limited to between 4 and 48 hours of runtime.","title":"Run"},{"location":"alphafold/#output","text":"After a successful job, you will notice multiple output files. They will be placed in the ./alphafold_output-<Timestamp>/input/* Where you will have the resulting .pdb, .pkl, and .json files. Now that it is on your local computer, you can now view this file on your computer if you have the proper software to view a pdb file.","title":"Output"},{"location":"anaconda_general_intro/","text":"Anaconda What is Anaconda? At a basic level Anaconda is a distribution of Python and R, although there is an emphasis on working with python, that provides access collections of associated packages optimized specifically for data science maintained in repositories. The installation and management of these packages is handled with the Anaconda package manager Conda. While initially focused mainly on python packages the repositories hosted by Anaconda and others now house a large collection of non-python packages. Conda is more than just a package manager however, it also creates and manages the environments that packages are installed in to. The use of environments to isolate software means you can have multiple versions of the same software installed in different environments and avoid conflicts or incompatibilities between software or dependencies. This is accomplished by installing packages into a separate directory which is then appended to your PATH when that environment is activated. The next couple of pages will provide a brief introduction on how to use Conda to create and maintain locally administered environments on the CARC machines. For more information on the usage and various features of Conda, please visit their website at this link .","title":"General intro to Conda"},{"location":"anaconda_general_intro/#anaconda","text":"","title":"Anaconda"},{"location":"anaconda_general_intro/#what-is-anaconda","text":"At a basic level Anaconda is a distribution of Python and R, although there is an emphasis on working with python, that provides access collections of associated packages optimized specifically for data science maintained in repositories. The installation and management of these packages is handled with the Anaconda package manager Conda. While initially focused mainly on python packages the repositories hosted by Anaconda and others now house a large collection of non-python packages. Conda is more than just a package manager however, it also creates and manages the environments that packages are installed in to. The use of environments to isolate software means you can have multiple versions of the same software installed in different environments and avoid conflicts or incompatibilities between software or dependencies. This is accomplished by installing packages into a separate directory which is then appended to your PATH when that environment is activated. The next couple of pages will provide a brief introduction on how to use Conda to create and maintain locally administered environments on the CARC machines. For more information on the usage and various features of Conda, please visit their website at this link .","title":"What is Anaconda?"},{"location":"anaconda_intro/","text":"Anaconda What is Anaconda? Fundamentally, Anaconda is a distribution of Python and R with a collection of associated packages optimized for data science. The installation and management of these packages is handled with the Anaconda package manager Conda. Conda is more than just a package manager however, it also creates and manages the environments that packages are installed in to. The usage of environments means you can have multiple versions of certain software installed in different environments and avoid conflicts or incompatibilities between software or dependencies. This is accomplished by installing packages into a separate directory which is then appended to your PATH when that environment is activated. Creating a new conda environment Let's create an environment on Wheeler to run a python machine learning script that uses the TensorFlow library, python version 3.5, and the pandas library. Once you log in to Wheeler using ssh load the anaconda software module with the command: module load anaconda3 We use conda to create new environments and install/upgrade packages within environments. To create our machine learning environment we type: conda create --name TensorFlow python=3.5 pandas tensorflow The command you are calling here is conda and you are telling it you want to create a new environment named TensorFlow with the packages python version 3.5 specifically, pandas, and tensorflow. When you enter this command conda prints out the plan for this environment to stdout : Solving environment: done ## Package Plan ## environment location: /users/yourusername/.conda/envs/TensorFlow added / updated specs: - pandas - python=3.5 - tensorflow The following packages will be downloaded: package | build ---------------------------|----------------- certifi-2018.8.24 | py35_1 139 KB termcolor-1.1.0 | py35_1 7 KB pip-10.0.1 | py35_0 1.8 MB pytz-2018.5 | py35_0 231 KB protobuf-3.6.0 | py35hf484d3e_0 615 KB werkzeug-0.14.1 | py35_0 426 KB astor-0.7.1 | py35_0 43 KB libprotobuf-3.6.0 | hdbcaa40_0 4.1 MB markdown-2.6.11 | py35_0 104 KB mkl_fft-1.0.4 | py35h4414c95_1 148 KB mkl_random-1.0.1 | py35h629b387_0 364 KB tensorboard-1.10.0 | py35hf484d3e_0 3.3 MB tensorflow-base-1.10.0 |mkl_py35h3c3e929_0 82.1 MB python-dateutil-2.7.3 | py35_0 261 KB numpy-base-1.15.1 | py35h81de0dd_0 4.2 MB wheel-0.31.1 | py35_0 63 KB _tflow_1100_select-0.0.3 | mkl 2 KB python-3.5.5 | hc3d631a_4 28.3 MB setuptools-40.2.0 | py35_0 571 KB grpcio-1.12.1 | py35hdbcaa40_0 1.7 MB gast-0.2.0 | py35_0 15 KB absl-py-0.4.0 | py35h28b3542_0 144 KB six-1.11.0 | py35h423b573_1 21 KB tensorflow-1.10.0 |mkl_py35heddcb22_0 4 KB pandas-0.23.4 | py35h04863e7_0 10.0 MB numpy-1.15.1 | py35h3b04361_0 37 KB ------------------------------------------------------------ Total: 138.6 MB The following NEW packages will be INSTALLED: _tflow_1100_select: 0.0.3-mkl absl-py: 0.4.0-py35h28b3542_0 astor: 0.7.1-py35_0 blas: 1.0-mkl ca-certificates: 2018.03.07-0 certifi: 2018.8.24-py35_1 gast: 0.2.0-py35_0 grpcio: 1.12.1-py35hdbcaa40_0 intel-openmp: 2018.0.3-0 libedit: 3.1.20170329-h6b74fdf_2 libffi: 3.2.1-hd88cf55_4 libgcc-ng: 8.2.0-hdf63c60_1 libgfortran-ng: 7.3.0-hdf63c60_0 libprotobuf: 3.6.0-hdbcaa40_0 libstdcxx-ng: 8.2.0-hdf63c60_1 markdown: 2.6.11-py35_0 mkl: 2018.0.3-1 mkl_fft: 1.0.4-py35h4414c95_1 mkl_random: 1.0.1-py35h629b387_0 ncurses: 6.1-hf484d3e_0 numpy: 1.15.1-py35h3b04361_0 numpy-base: 1.15.1-py35h81de0dd_0 openssl: 1.0.2p-h14c3975_0 pandas: 0.23.4-py35h04863e7_0 pip: 10.0.1-py35_0 protobuf: 3.6.0-py35hf484d3e_0 python: 3.5.5-hc3d631a_4 python-dateutil: 2.7.3-py35_0 pytz: 2018.5-py35_0 readline: 7.0-ha6073c6_4 setuptools: 40.2.0-py35_0 six: 1.11.0-py35h423b573_1 sqlite: 3.24.0-h84994c4_0 tensorboard: 1.10.0-py35hf484d3e_0 tensorflow: 1.10.0-mkl_py35heddcb22_0 tensorflow-base: 1.10.0-mkl_py35h3c3e929_0 termcolor: 1.1.0-py35_1 tk: 8.6.7-hc745277_3 werkzeug: 0.14.1-py35_0 wheel: 0.31.1-py35_0 xz: 5.2.4-h14c3975_4 zlib: 1.2.11-ha838bed_2 Proceed ([y]/n)? This gives you the list of all packages you requested to be installed and their dependencies, as well as the package version and build. Of note is the environment location pathway at the top of the package plan, you will notice that conda by default installs into your local directory and does not need administrative access to install packages. This means that you can administer your own Anaconda environments at CARC. When you verify the package plan conda will proceed with downloading package binaries and installing them into the environment directory. You will see the progress of installation and a message with how to activate your environment once complete: Downloading and Extracting Packages certifi-2018.8.24 | 139 KB | ####################################### | 100% python-3.6.6 | 15.4 MB | ####################################### | 100% tensorflow-base-1.10 | 55.3 MB | ####################################### | 100% setuptools-40.2.0 | 554 KB | ####################################### | 100% libprotobuf-3.6.0 | 3.8 MB | ####################################### | 100% sqlite-3.24.0 | 2.2 MB | ####################################### | 100% mkl-2018.0.3 | 149.2 MB| ###################################### | 100% mkl_random-1.0.1 | 349 KB | ####################################### | 100% mkl_fft-1.0.4 | 137 KB | ####################################### | 100% openssl-1.0.2p | 3.4 MB | ####################################### | 100% six-1.11.0 | 21 KB | ####################################### | 100% tensorflow-1.10.0 | 4 KB | ####################################### | 100% numpy-base-1.15.1 | 4.0 MB | ####################################### | 100% protobuf-3.6.0 | 604 KB | ####################################### | 100% numpy-1.15.1 | 37 KB | ####################################### | 100% intel-openmp-2018.0. | 1004 KB | ####################################### | 100% absl-py-0.4.0 | 143 KB | ####################################### | 100% tensorboard-1.10.0 | 3.3 MB | ####################################### | 100% _tflow_1100_select-0 | 3 KB | ####################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use: # > source activate TensorFlow # # To deactivate an active environment, use: # > source deactivate # Now we have our machine learning environment created to run our machine learning python script. To activate the environment we just created you use the command source activate my_environment_name , which is source activate TensorFlow for this example. Remember to include the lines below in your PBS script when working with Anaconda environments: # load anaconda software module module load anaconda3 # activate your desired anaconda environment source activate environment_name For more information on managing environments visit the Conda documentation site at this link , or by adding the flag --help to any conda command, for example, conda create --help will print a help page for creating environments.","title":"Intro to Conda with example"},{"location":"anaconda_intro/#anaconda","text":"","title":"Anaconda"},{"location":"anaconda_intro/#what-is-anaconda","text":"Fundamentally, Anaconda is a distribution of Python and R with a collection of associated packages optimized for data science. The installation and management of these packages is handled with the Anaconda package manager Conda. Conda is more than just a package manager however, it also creates and manages the environments that packages are installed in to. The usage of environments means you can have multiple versions of certain software installed in different environments and avoid conflicts or incompatibilities between software or dependencies. This is accomplished by installing packages into a separate directory which is then appended to your PATH when that environment is activated.","title":"What is Anaconda?"},{"location":"anaconda_intro/#creating-a-new-conda-environment","text":"Let's create an environment on Wheeler to run a python machine learning script that uses the TensorFlow library, python version 3.5, and the pandas library. Once you log in to Wheeler using ssh load the anaconda software module with the command: module load anaconda3 We use conda to create new environments and install/upgrade packages within environments. To create our machine learning environment we type: conda create --name TensorFlow python=3.5 pandas tensorflow The command you are calling here is conda and you are telling it you want to create a new environment named TensorFlow with the packages python version 3.5 specifically, pandas, and tensorflow. When you enter this command conda prints out the plan for this environment to stdout : Solving environment: done ## Package Plan ## environment location: /users/yourusername/.conda/envs/TensorFlow added / updated specs: - pandas - python=3.5 - tensorflow The following packages will be downloaded: package | build ---------------------------|----------------- certifi-2018.8.24 | py35_1 139 KB termcolor-1.1.0 | py35_1 7 KB pip-10.0.1 | py35_0 1.8 MB pytz-2018.5 | py35_0 231 KB protobuf-3.6.0 | py35hf484d3e_0 615 KB werkzeug-0.14.1 | py35_0 426 KB astor-0.7.1 | py35_0 43 KB libprotobuf-3.6.0 | hdbcaa40_0 4.1 MB markdown-2.6.11 | py35_0 104 KB mkl_fft-1.0.4 | py35h4414c95_1 148 KB mkl_random-1.0.1 | py35h629b387_0 364 KB tensorboard-1.10.0 | py35hf484d3e_0 3.3 MB tensorflow-base-1.10.0 |mkl_py35h3c3e929_0 82.1 MB python-dateutil-2.7.3 | py35_0 261 KB numpy-base-1.15.1 | py35h81de0dd_0 4.2 MB wheel-0.31.1 | py35_0 63 KB _tflow_1100_select-0.0.3 | mkl 2 KB python-3.5.5 | hc3d631a_4 28.3 MB setuptools-40.2.0 | py35_0 571 KB grpcio-1.12.1 | py35hdbcaa40_0 1.7 MB gast-0.2.0 | py35_0 15 KB absl-py-0.4.0 | py35h28b3542_0 144 KB six-1.11.0 | py35h423b573_1 21 KB tensorflow-1.10.0 |mkl_py35heddcb22_0 4 KB pandas-0.23.4 | py35h04863e7_0 10.0 MB numpy-1.15.1 | py35h3b04361_0 37 KB ------------------------------------------------------------ Total: 138.6 MB The following NEW packages will be INSTALLED: _tflow_1100_select: 0.0.3-mkl absl-py: 0.4.0-py35h28b3542_0 astor: 0.7.1-py35_0 blas: 1.0-mkl ca-certificates: 2018.03.07-0 certifi: 2018.8.24-py35_1 gast: 0.2.0-py35_0 grpcio: 1.12.1-py35hdbcaa40_0 intel-openmp: 2018.0.3-0 libedit: 3.1.20170329-h6b74fdf_2 libffi: 3.2.1-hd88cf55_4 libgcc-ng: 8.2.0-hdf63c60_1 libgfortran-ng: 7.3.0-hdf63c60_0 libprotobuf: 3.6.0-hdbcaa40_0 libstdcxx-ng: 8.2.0-hdf63c60_1 markdown: 2.6.11-py35_0 mkl: 2018.0.3-1 mkl_fft: 1.0.4-py35h4414c95_1 mkl_random: 1.0.1-py35h629b387_0 ncurses: 6.1-hf484d3e_0 numpy: 1.15.1-py35h3b04361_0 numpy-base: 1.15.1-py35h81de0dd_0 openssl: 1.0.2p-h14c3975_0 pandas: 0.23.4-py35h04863e7_0 pip: 10.0.1-py35_0 protobuf: 3.6.0-py35hf484d3e_0 python: 3.5.5-hc3d631a_4 python-dateutil: 2.7.3-py35_0 pytz: 2018.5-py35_0 readline: 7.0-ha6073c6_4 setuptools: 40.2.0-py35_0 six: 1.11.0-py35h423b573_1 sqlite: 3.24.0-h84994c4_0 tensorboard: 1.10.0-py35hf484d3e_0 tensorflow: 1.10.0-mkl_py35heddcb22_0 tensorflow-base: 1.10.0-mkl_py35h3c3e929_0 termcolor: 1.1.0-py35_1 tk: 8.6.7-hc745277_3 werkzeug: 0.14.1-py35_0 wheel: 0.31.1-py35_0 xz: 5.2.4-h14c3975_4 zlib: 1.2.11-ha838bed_2 Proceed ([y]/n)? This gives you the list of all packages you requested to be installed and their dependencies, as well as the package version and build. Of note is the environment location pathway at the top of the package plan, you will notice that conda by default installs into your local directory and does not need administrative access to install packages. This means that you can administer your own Anaconda environments at CARC. When you verify the package plan conda will proceed with downloading package binaries and installing them into the environment directory. You will see the progress of installation and a message with how to activate your environment once complete: Downloading and Extracting Packages certifi-2018.8.24 | 139 KB | ####################################### | 100% python-3.6.6 | 15.4 MB | ####################################### | 100% tensorflow-base-1.10 | 55.3 MB | ####################################### | 100% setuptools-40.2.0 | 554 KB | ####################################### | 100% libprotobuf-3.6.0 | 3.8 MB | ####################################### | 100% sqlite-3.24.0 | 2.2 MB | ####################################### | 100% mkl-2018.0.3 | 149.2 MB| ###################################### | 100% mkl_random-1.0.1 | 349 KB | ####################################### | 100% mkl_fft-1.0.4 | 137 KB | ####################################### | 100% openssl-1.0.2p | 3.4 MB | ####################################### | 100% six-1.11.0 | 21 KB | ####################################### | 100% tensorflow-1.10.0 | 4 KB | ####################################### | 100% numpy-base-1.15.1 | 4.0 MB | ####################################### | 100% protobuf-3.6.0 | 604 KB | ####################################### | 100% numpy-1.15.1 | 37 KB | ####################################### | 100% intel-openmp-2018.0. | 1004 KB | ####################################### | 100% absl-py-0.4.0 | 143 KB | ####################################### | 100% tensorboard-1.10.0 | 3.3 MB | ####################################### | 100% _tflow_1100_select-0 | 3 KB | ####################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use: # > source activate TensorFlow # # To deactivate an active environment, use: # > source deactivate # Now we have our machine learning environment created to run our machine learning python script. To activate the environment we just created you use the command source activate my_environment_name , which is source activate TensorFlow for this example. Remember to include the lines below in your PBS script when working with Anaconda environments: # load anaconda software module module load anaconda3 # activate your desired anaconda environment source activate environment_name For more information on managing environments visit the Conda documentation site at this link , or by adding the flag --help to any conda command, for example, conda create --help will print a help page for creating environments.","title":"Creating a new conda environment"},{"location":"anaconda_pip_channels/","text":"Installing with pip and adding channels Installing packages with pip Not all versions of all software have Conda packages available however, especially for some python libraries. Pip, the python package manager, is automatically installed by default in all environments created by Conda, and can install packages alongside those installed by Conda without conflict. For example, say you need the library psutil, but you specifically need version 5.3.0. When you search for psutil using conda you get the following: $ conda search psutil=5.3 Loading channels: done # Name Version Build Channel psutil 5.3.1 py27_0 conda-forge psutil 5.3.1 py27h4c169b4_0 pkgs/main psutil 5.3.1 py35_0 conda-forge psutil 5.3.1 py35h6e9e629_0 pkgs/main psutil 5.3.1 py36_0 conda-forge psutil 5.3.1 py36h0e357b8_0 pkgs/main Unfortunately there are no packages built for psutil version 5.3.0. We can use pip to install the version we want however. $ source activate py-2.7 (py-2.7)$ pip install psutil==5.3.0 Collecting psutil==5.3.0 Downloading https://files.pythonhosted.org/packages/1c/da/555e3ad3cad30f30bcf0d539cdeae5c8e7ef9e2a6078af645c70aa81e418/psutil-5.3.0.tar.gz (397kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 399kB 1.3MB/s Building wheels for collected packages: psutil Running setup.py bdist_wheel for psutil ... done Stored in directory: /users/yourusername/.cache/pip/wheels/ff/c5/4f/1ee2208203f1cfeda16e91fccd8bfce5f4840b683671729d57 Successfully built psutil (py-2.7)$ conda list # packages in environment at /users/yourusername/.Conda/envs/py-2.7: # # Name Version Build ca-certificates 2018.03.07 0 certifi 2018.8.24 py27_1 libedit 3.1.20170329 h6b74fdf_2 libffi 3.2.1 hd88cf55_4 libgcc-ng 8.2.0 hdf63c60_1 libstdcxx-ng 8.2.0 hdf63c60_1 ncurses 6.1 hf484d3e_0 openssl 1.0.2p h14c3975_0 pip 10.0.1 py27_0 **psutil 5.3.0 <pip> python 2.7.15 h1571d57_0 readline 7.0 h7b6447c_5 setuptools 40.2.0 py27_0 sqlite 3.24.0 h84994c4_0 tk 8.6.8 hbc83047_0 wheel 0.31.1 py27_0 zlib 1.2.11 ha838bed_2 When installing packages using pip it is important to first activate the Conda environment that you want to install the package in since pip is strictly a package manager and cannot modify Conda environments from outside that environment. You can see that our psutil package, marked with a double asterisk, is version 5.3.0, just like we wanted. Under the 'build' column however you will see that conda is not sure which build it is since it was installed with pip , as indicated by the <pip> designator. Performance with Conda versus Pip One thing to note when installing packages is that it is always preferable to first install necessary packages with conda first, only then use pip to install only those packages that were not available through Anaconda repositories. It is usually best practice to install needed packages and dependencies with conda and use pip to install any remaining packages that were not available instead of vice versa. Adding package repositories (channels) Sometimes the default repositories, or channels for Conda, do not have the package you are looking for, but that does not mean that it is necessarily unavailable entirely. Say you are working with some Illumina sequence data and need the Burrows-Wheeler Aligner (bwa) in your pipeline, so you activate your bioinformatics environment and type conda install bwa which prints the following: Solving environment: failed PackagesNotFoundError: The following packages are not available from current channels: - bwa Current channels: - https://repo.anaconda.com/pkgs/main/linux-64 - https://repo.anaconda.com/pkgs/main/noarch - https://repo.anaconda.com/pkgs/free/linux-64 - https://repo.anaconda.com/pkgs/free/noarch - https://repo.anaconda.com/pkgs/r/linux-64 - https://repo.anaconda.com/pkgs/r/noarch - https://repo.anaconda.com/pkgs/pro/linux-64 - https://repo.anaconda.com/pkgs/pro/noarch To search for alternate channels that may provide the conda package you're looking for, navigate to https://anaconda.org and use the search bar at the top of the page. We can search other channels that may have the package we are interested in with the -c flag. For example, BioConda is a large repository that hosts several thousand bioinformatics packages. We can search for our bwa package by specifying that channel. $ conda search -c bioConda bwa Which yields better results: Loading channels: done # Name Version Build Channel bwa 0.5.9 0 bioConda bwa 0.5.9 1 bioConda bwa 0.6.2 0 bioConda bwa 0.6.2 1 bioConda bwa 0.7.3a 0 bioConda bwa 0.7.3a 1 bioConda bwa 0.7.3a ha92aebf_2 bioConda bwa 0.7.4 ha92aebf_0 bioConda bwa 0.7.8 0 bioConda bwa 0.7.8 1 bioConda bwa 0.7.8 ha92aebf_2 bioConda bwa 0.7.12 0 bioConda bwa 0.7.12 1 bioConda bwa 0.7.13 0 bioConda bwa 0.7.13 1 bioConda bwa 0.7.15 0 bioConda bwa 0.7.15 1 bioConda bwa 0.7.16 pl5.22.0_0 bioConda bwa 0.7.17 ha92aebf_3 bioConda bwa 0.7.17 pl5.22.0_0 bioConda bwa 0.7.17 pl5.22.0_1 bioConda bwa 0.7.17 pl5.22.0_2 bioConda We can then install our bwa package using conda install -c bioConda bwa and continue with our analyses. You can permanently add channels by appending your .condarc file either directly in a text editor, or with conda by using the config command: $ conda config --append channels bioconda This will permanently add the BioConda channel to your configuration file meaning Conda will automatically search BioConda as well as the default channels when looking for packages. For more information on managing channels and installing with pip please refer to the Conda support documentation at this link .","title":"Anaconda pip channels"},{"location":"anaconda_pip_channels/#installing-with-pip-and-adding-channels","text":"","title":"Installing with pip and adding channels"},{"location":"anaconda_pip_channels/#installing-packages-with-pip","text":"Not all versions of all software have Conda packages available however, especially for some python libraries. Pip, the python package manager, is automatically installed by default in all environments created by Conda, and can install packages alongside those installed by Conda without conflict. For example, say you need the library psutil, but you specifically need version 5.3.0. When you search for psutil using conda you get the following: $ conda search psutil=5.3 Loading channels: done # Name Version Build Channel psutil 5.3.1 py27_0 conda-forge psutil 5.3.1 py27h4c169b4_0 pkgs/main psutil 5.3.1 py35_0 conda-forge psutil 5.3.1 py35h6e9e629_0 pkgs/main psutil 5.3.1 py36_0 conda-forge psutil 5.3.1 py36h0e357b8_0 pkgs/main Unfortunately there are no packages built for psutil version 5.3.0. We can use pip to install the version we want however. $ source activate py-2.7 (py-2.7)$ pip install psutil==5.3.0 Collecting psutil==5.3.0 Downloading https://files.pythonhosted.org/packages/1c/da/555e3ad3cad30f30bcf0d539cdeae5c8e7ef9e2a6078af645c70aa81e418/psutil-5.3.0.tar.gz (397kB) 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 399kB 1.3MB/s Building wheels for collected packages: psutil Running setup.py bdist_wheel for psutil ... done Stored in directory: /users/yourusername/.cache/pip/wheels/ff/c5/4f/1ee2208203f1cfeda16e91fccd8bfce5f4840b683671729d57 Successfully built psutil (py-2.7)$ conda list # packages in environment at /users/yourusername/.Conda/envs/py-2.7: # # Name Version Build ca-certificates 2018.03.07 0 certifi 2018.8.24 py27_1 libedit 3.1.20170329 h6b74fdf_2 libffi 3.2.1 hd88cf55_4 libgcc-ng 8.2.0 hdf63c60_1 libstdcxx-ng 8.2.0 hdf63c60_1 ncurses 6.1 hf484d3e_0 openssl 1.0.2p h14c3975_0 pip 10.0.1 py27_0 **psutil 5.3.0 <pip> python 2.7.15 h1571d57_0 readline 7.0 h7b6447c_5 setuptools 40.2.0 py27_0 sqlite 3.24.0 h84994c4_0 tk 8.6.8 hbc83047_0 wheel 0.31.1 py27_0 zlib 1.2.11 ha838bed_2 When installing packages using pip it is important to first activate the Conda environment that you want to install the package in since pip is strictly a package manager and cannot modify Conda environments from outside that environment. You can see that our psutil package, marked with a double asterisk, is version 5.3.0, just like we wanted. Under the 'build' column however you will see that conda is not sure which build it is since it was installed with pip , as indicated by the <pip> designator.","title":"Installing packages with pip"},{"location":"anaconda_pip_channels/#performance-with-conda-versus-pip","text":"One thing to note when installing packages is that it is always preferable to first install necessary packages with conda first, only then use pip to install only those packages that were not available through Anaconda repositories. It is usually best practice to install needed packages and dependencies with conda and use pip to install any remaining packages that were not available instead of vice versa.","title":"Performance with Conda versus Pip"},{"location":"anaconda_pip_channels/#adding-package-repositories-channels","text":"Sometimes the default repositories, or channels for Conda, do not have the package you are looking for, but that does not mean that it is necessarily unavailable entirely. Say you are working with some Illumina sequence data and need the Burrows-Wheeler Aligner (bwa) in your pipeline, so you activate your bioinformatics environment and type conda install bwa which prints the following: Solving environment: failed PackagesNotFoundError: The following packages are not available from current channels: - bwa Current channels: - https://repo.anaconda.com/pkgs/main/linux-64 - https://repo.anaconda.com/pkgs/main/noarch - https://repo.anaconda.com/pkgs/free/linux-64 - https://repo.anaconda.com/pkgs/free/noarch - https://repo.anaconda.com/pkgs/r/linux-64 - https://repo.anaconda.com/pkgs/r/noarch - https://repo.anaconda.com/pkgs/pro/linux-64 - https://repo.anaconda.com/pkgs/pro/noarch To search for alternate channels that may provide the conda package you're looking for, navigate to https://anaconda.org and use the search bar at the top of the page. We can search other channels that may have the package we are interested in with the -c flag. For example, BioConda is a large repository that hosts several thousand bioinformatics packages. We can search for our bwa package by specifying that channel. $ conda search -c bioConda bwa Which yields better results: Loading channels: done # Name Version Build Channel bwa 0.5.9 0 bioConda bwa 0.5.9 1 bioConda bwa 0.6.2 0 bioConda bwa 0.6.2 1 bioConda bwa 0.7.3a 0 bioConda bwa 0.7.3a 1 bioConda bwa 0.7.3a ha92aebf_2 bioConda bwa 0.7.4 ha92aebf_0 bioConda bwa 0.7.8 0 bioConda bwa 0.7.8 1 bioConda bwa 0.7.8 ha92aebf_2 bioConda bwa 0.7.12 0 bioConda bwa 0.7.12 1 bioConda bwa 0.7.13 0 bioConda bwa 0.7.13 1 bioConda bwa 0.7.15 0 bioConda bwa 0.7.15 1 bioConda bwa 0.7.16 pl5.22.0_0 bioConda bwa 0.7.17 ha92aebf_3 bioConda bwa 0.7.17 pl5.22.0_0 bioConda bwa 0.7.17 pl5.22.0_1 bioConda bwa 0.7.17 pl5.22.0_2 bioConda We can then install our bwa package using conda install -c bioConda bwa and continue with our analyses. You can permanently add channels by appending your .condarc file either directly in a text editor, or with conda by using the config command: $ conda config --append channels bioconda This will permanently add the BioConda channel to your configuration file meaning Conda will automatically search BioConda as well as the default channels when looking for packages. For more information on managing channels and installing with pip please refer to the Conda support documentation at this link .","title":"Adding package repositories (channels)"},{"location":"check_jobs/","text":"Checking on running jobs Checking on the status of your Job: If you would like to check the status of your job, you can use the qstat command to do so. Typing qstat without any options will output all currently running or queued jobs to your terminal window, but there are many options to help display relevant information. To find more of these options type man qstat when logged in to a CARC machine. To see which jobs are running and queued in the standard output type the following in a terminal window: qstat Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 288:43:2 R default 127508.wheeler-sn.alliance.un pebble30_90 user 279:41:4 R default 127509.wheeler-sn.alliance.un pebble30_70 user 323:06:0 R default 128012.wheeler-sn.alliance.un canu_wheeler.sh user 0 Q default The output of qstat give you the Job ID, the name of the Job, which user owns that Job, CPU time, the status of the Job, either queued (Q), running (R), and sometimes on hold (H), and lastly, which queue the Job is in. To look at a specific job without seeing everything running you can use the Job ID by typing qstat Job ID , or by using the -u flag followed by the username, qstat -u user . For example: qstat 127506 Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 289:04:1 R default A useful option is the -a flag which shows more information about jobs than qstat alone. As well as the information above, the -a option also outputs requested nodes, processors, memory, wall time, and actual runtime instead of CPU time. Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 127506.wheeler-sn.alli user default pebble30_80 8739 1 8 -- 240:00:00 R 229:13:18 127508.wheeler-sn.alli user default pebble30_90 25507 1 8 -- 240:00:00 R 229:09:10 127509.wheeler-sn.alli user default pebble30_70 20372 1 8 -- 240:00:00 R 229:08:46 128012.wheeler-sn.alli user default canu_wheeler.sh -- 1 8 64gb 24:00:00 Q Determining which nodes your Job is using: If you would like to check which nodes your job is using, you can pass the -n option to qstat. When your job is finished, your processes on each node will be killed by the system, and the node will be released back into the available resource pool. qstat -an wheeler-sn.alliance.unm.edu: Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------------------- ------------ -------- ---------- ------- --- --- ------ -------- -- -------- 55811.wheeler-sn.alliance.u user default B19F_re5e4 0 4 32 - - 48:00:00 R 47:30:42 wheeler296/0-7+wheeler295/0-7+wheeler282/0-7+wheeler280/0-7 Here, the nodes that this job is running on are wheeler296, wheeler295, wheeler282, and wheeler280, with 8 processors per node. Viewing Output and Error Files: Once your job has completed, you should see two files, one output file and one error file, in the directory from which you submitted the Job: Jobname.oJobID and Jobname.eJobID (where Jobname refers to the name of the Job returned by qstat , and JobID refers to the numerical portion of the job identifier returned by qstat ). For the example job above, these two files would be named B19F_re5E4.o55811 and B19F_re5E4.e55811 respectively. Any output from the job sent to \u201cstandard output\u201d will be written to the output file, and any output sent to \u201cstandard error\u201d will be written to the error file. The amount of information in the output and error files varies depending on the program being run and how the PBS batch script was set up.","title":"Checking on running jobs"},{"location":"check_jobs/#checking-on-running-jobs","text":"","title":"Checking on running jobs"},{"location":"check_jobs/#checking-on-the-status-of-your-job","text":"If you would like to check the status of your job, you can use the qstat command to do so. Typing qstat without any options will output all currently running or queued jobs to your terminal window, but there are many options to help display relevant information. To find more of these options type man qstat when logged in to a CARC machine. To see which jobs are running and queued in the standard output type the following in a terminal window: qstat Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 288:43:2 R default 127508.wheeler-sn.alliance.un pebble30_90 user 279:41:4 R default 127509.wheeler-sn.alliance.un pebble30_70 user 323:06:0 R default 128012.wheeler-sn.alliance.un canu_wheeler.sh user 0 Q default The output of qstat give you the Job ID, the name of the Job, which user owns that Job, CPU time, the status of the Job, either queued (Q), running (R), and sometimes on hold (H), and lastly, which queue the Job is in. To look at a specific job without seeing everything running you can use the Job ID by typing qstat Job ID , or by using the -u flag followed by the username, qstat -u user . For example: qstat 127506 Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 289:04:1 R default A useful option is the -a flag which shows more information about jobs than qstat alone. As well as the information above, the -a option also outputs requested nodes, processors, memory, wall time, and actual runtime instead of CPU time. Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 127506.wheeler-sn.alli user default pebble30_80 8739 1 8 -- 240:00:00 R 229:13:18 127508.wheeler-sn.alli user default pebble30_90 25507 1 8 -- 240:00:00 R 229:09:10 127509.wheeler-sn.alli user default pebble30_70 20372 1 8 -- 240:00:00 R 229:08:46 128012.wheeler-sn.alli user default canu_wheeler.sh -- 1 8 64gb 24:00:00 Q","title":"Checking on the status of your Job:"},{"location":"check_jobs/#determining-which-nodes-your-job-is-using","text":"If you would like to check which nodes your job is using, you can pass the -n option to qstat. When your job is finished, your processes on each node will be killed by the system, and the node will be released back into the available resource pool. qstat -an wheeler-sn.alliance.unm.edu: Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------------------- ------------ -------- ---------- ------- --- --- ------ -------- -- -------- 55811.wheeler-sn.alliance.u user default B19F_re5e4 0 4 32 - - 48:00:00 R 47:30:42 wheeler296/0-7+wheeler295/0-7+wheeler282/0-7+wheeler280/0-7 Here, the nodes that this job is running on are wheeler296, wheeler295, wheeler282, and wheeler280, with 8 processors per node.","title":"Determining which nodes your Job is using:"},{"location":"check_jobs/#viewing-output-and-error-files","text":"Once your job has completed, you should see two files, one output file and one error file, in the directory from which you submitted the Job: Jobname.oJobID and Jobname.eJobID (where Jobname refers to the name of the Job returned by qstat , and JobID refers to the numerical portion of the job identifier returned by qstat ). For the example job above, these two files would be named B19F_re5E4.o55811 and B19F_re5E4.e55811 respectively. Any output from the job sent to \u201cstandard output\u201d will be written to the output file, and any output sent to \u201cstandard error\u201d will be written to the error file. The amount of information in the output and error files varies depending on the program being run and how the PBS batch script was set up.","title":"Viewing Output and Error Files:"},{"location":"checking_on_running_jobs/","text":"Checking on running jobs Checking on the status of your Job: If you would like to check the status of your job, you can use the qstat command to do so. Typing qstat without any options will output all currently running or queued jobs to your terminal window, but there are many options to help display relevant information. To find more of these options type man qstat when logged in to a CARC machine. To see which jobs are running and queued in the standard output type the following in a terminal window: qstat Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 288:43:2 R default 127508.wheeler-sn.alliance.un pebble30_90 user 279:41:4 R default 127509.wheeler-sn.alliance.un pebble30_70 user 323:06:0 R default 128012.wheeler-sn.alliance.un canu_wheeler.sh user 0 Q default The output of qstat give you the Job ID, the name of the Job, which user owns that Job, CPU time, the status of the Job, either queued (Q), running (R), and sometimes on hold (H), and lastly, which queue the Job is in. To look at a specific job without seeing everything running you can use the Job ID by typing qstat Job ID , or by using the -u flag followed by the username, qstat -u user . For example: qstat 127506 Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 289:04:1 R default A useful option is the -a flag which shows more information about jobs than qstat alone. As well as the information above, the -a option also outputs requested nodes, processors, memory, wall time, and actual runtime instead of CPU time. Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 127506.wheeler-sn.alli user default pebble30_80 8739 1 8 -- 240:00:00 R 229:13:18 127508.wheeler-sn.alli user default pebble30_90 25507 1 8 -- 240:00:00 R 229:09:10 127509.wheeler-sn.alli user default pebble30_70 20372 1 8 -- 240:00:00 R 229:08:46 128012.wheeler-sn.alli user default canu_wheeler.sh -- 1 8 64gb 24:00:00 Q qstat -f Specifies a \"full\" format display of information. It displays the informations regarding job name,owner,cpu_time, memory usage, walltime, job staus, error and output file path, executing host, nodes and core allocation and others. qstat -f <jobid> displays the information corresponding to that jobid. Example (user) xena:~$ qstat qstat -f 67048 Job Id: 67048.xena.xena.alliance.unm.edu Job_Name = BipolarCox_138 Job_Owner = user@xena.xena.alliance.unm.edu resources_used.cput = 00:35:53 resources_used.energy_used = 0 resources_used.mem = 31427708kb resources_used.vmem = 31792364kb resources_used.walltime = 00:35:58 job_state = R queue = singleGPU server = xena.xena.alliance.unm.edu Checkpoint = u ctime = Mon Feb 18 16:19:19 2019 Error_Path = xena.xena.alliance.unm.edu:/users/user/experiments/newsui cidality-injury/BipolarCox_138.e67048 exec_host = xena21/0-1 Hold_Types = n Join_Path = n Keep_Files = n Mail_Points = a mtime = Tue Feb 19 12:47:56 2019 Output_Path = xena.xena.alliance.unm.edu:/users/user/experiments/newsu icidality-injury/BipolarCox_138.o67048 Priority = 0 qtime = Mon Feb 18 16:19:19 2019 Rerunable = True Resource_List.nodect = 1 Resource_List.nodes = 1:ppn=2 Resource_List.walltime = 03:00:00 session_id = 74594 Shell_Path_List = /bin/bash euser = dccannon egroup = users queue_type = E etime = Mon Feb 18 16:19:19 2019 submit_args = -N BipolarCox_138 -v run_id=138 runRScript.sh start_time = Tue Feb 19 12:47:56 2019 Walltime.Remaining = 8598 start_count = 1 fault_tolerant = False job_radix = 0 submit_host = xena.xena.alliance.unm.edu request_version = 1 watch qstat -u <username> allows an interactive statistics of jobs for that user which updates for every 2sec. Example (user) xena:~$watch qstat -u ceodspsp Every 2.0s: qstat -u ceodspsp Tue Feb 19 13:45:50 2019 xena.xena.alliance.unm.edu: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 66908.xena.xena.allian ceodspsp dualGPU smoke_1_5 103419 2 32 -- 48:00:00 R 21:50:33 67438.xena.xena.allian ceodspsp dualGPU smoke_5_10 66632 2 32 -- 48:00:00 R 09:39:00 Determining which nodes your Job is using: If you would like to check which nodes your job is using, you can pass the -n option to qstat. When your job is finished, your processes on each node will be killed by the system, and the node will be released back into the available resource pool. qstat -an wheeler-sn.alliance.unm.edu: Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------------------- ------------ -------- ---------- ------- --- --- ------ -------- -- -------- 55811.wheeler-sn.alliance.u user default B19F_re5e4 0 4 32 - - 48:00:00 R 47:30:42 wheeler296/0-7+wheeler295/0-7+wheeler282/0-7+wheeler280/0-7 Here, the nodes that this job is running on are wheeler296, wheeler295, wheeler282, and wheeler280, with 8 processors per node. Viewing Output and Error Files: Once your job has completed, you should see two files, one output file and one error file, in the directory from which you submitted the Job: Jobname.oJobID and Jobname.eJobID (where Jobname refers to the name of the Job returned by qstat , and JobID refers to the numerical portion of the job identifier returned by qstat ). For the example job above, these two files would be named B19F_re5E4.o55811 and B19F_re5E4.e55811 respectively. Any output from the job sent to \u201cstandard output\u201d will be written to the output file, and any output sent to \u201cstandard error\u201d will be written to the error file. The amount of information in the output and error files varies depending on the program being run and how the PBS batch script was set up.","title":"Check running jobs"},{"location":"checking_on_running_jobs/#checking-on-running-jobs","text":"","title":"Checking on running jobs"},{"location":"checking_on_running_jobs/#checking-on-the-status-of-your-job","text":"If you would like to check the status of your job, you can use the qstat command to do so. Typing qstat without any options will output all currently running or queued jobs to your terminal window, but there are many options to help display relevant information. To find more of these options type man qstat when logged in to a CARC machine. To see which jobs are running and queued in the standard output type the following in a terminal window: qstat Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 288:43:2 R default 127508.wheeler-sn.alliance.un pebble30_90 user 279:41:4 R default 127509.wheeler-sn.alliance.un pebble30_70 user 323:06:0 R default 128012.wheeler-sn.alliance.un canu_wheeler.sh user 0 Q default The output of qstat give you the Job ID, the name of the Job, which user owns that Job, CPU time, the status of the Job, either queued (Q), running (R), and sometimes on hold (H), and lastly, which queue the Job is in. To look at a specific job without seeing everything running you can use the Job ID by typing qstat Job ID , or by using the -u flag followed by the username, qstat -u user . For example: qstat 127506 Job ID Name User Time Use S Queue ------------------------- ---------------- --------------- -------- - ----- 127506.wheeler-sn.alliance.un pebble30_80 user 289:04:1 R default A useful option is the -a flag which shows more information about jobs than qstat alone. As well as the information above, the -a option also outputs requested nodes, processors, memory, wall time, and actual runtime instead of CPU time. Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 127506.wheeler-sn.alli user default pebble30_80 8739 1 8 -- 240:00:00 R 229:13:18 127508.wheeler-sn.alli user default pebble30_90 25507 1 8 -- 240:00:00 R 229:09:10 127509.wheeler-sn.alli user default pebble30_70 20372 1 8 -- 240:00:00 R 229:08:46 128012.wheeler-sn.alli user default canu_wheeler.sh -- 1 8 64gb 24:00:00 Q qstat -f Specifies a \"full\" format display of information. It displays the informations regarding job name,owner,cpu_time, memory usage, walltime, job staus, error and output file path, executing host, nodes and core allocation and others. qstat -f <jobid> displays the information corresponding to that jobid. Example (user) xena:~$ qstat qstat -f 67048 Job Id: 67048.xena.xena.alliance.unm.edu Job_Name = BipolarCox_138 Job_Owner = user@xena.xena.alliance.unm.edu resources_used.cput = 00:35:53 resources_used.energy_used = 0 resources_used.mem = 31427708kb resources_used.vmem = 31792364kb resources_used.walltime = 00:35:58 job_state = R queue = singleGPU server = xena.xena.alliance.unm.edu Checkpoint = u ctime = Mon Feb 18 16:19:19 2019 Error_Path = xena.xena.alliance.unm.edu:/users/user/experiments/newsui cidality-injury/BipolarCox_138.e67048 exec_host = xena21/0-1 Hold_Types = n Join_Path = n Keep_Files = n Mail_Points = a mtime = Tue Feb 19 12:47:56 2019 Output_Path = xena.xena.alliance.unm.edu:/users/user/experiments/newsu icidality-injury/BipolarCox_138.o67048 Priority = 0 qtime = Mon Feb 18 16:19:19 2019 Rerunable = True Resource_List.nodect = 1 Resource_List.nodes = 1:ppn=2 Resource_List.walltime = 03:00:00 session_id = 74594 Shell_Path_List = /bin/bash euser = dccannon egroup = users queue_type = E etime = Mon Feb 18 16:19:19 2019 submit_args = -N BipolarCox_138 -v run_id=138 runRScript.sh start_time = Tue Feb 19 12:47:56 2019 Walltime.Remaining = 8598 start_count = 1 fault_tolerant = False job_radix = 0 submit_host = xena.xena.alliance.unm.edu request_version = 1 watch qstat -u <username> allows an interactive statistics of jobs for that user which updates for every 2sec. Example (user) xena:~$watch qstat -u ceodspsp Every 2.0s: qstat -u ceodspsp Tue Feb 19 13:45:50 2019 xena.xena.alliance.unm.edu: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 66908.xena.xena.allian ceodspsp dualGPU smoke_1_5 103419 2 32 -- 48:00:00 R 21:50:33 67438.xena.xena.allian ceodspsp dualGPU smoke_5_10 66632 2 32 -- 48:00:00 R 09:39:00","title":"Checking on the status of your Job:"},{"location":"checking_on_running_jobs/#determining-which-nodes-your-job-is-using","text":"If you would like to check which nodes your job is using, you can pass the -n option to qstat. When your job is finished, your processes on each node will be killed by the system, and the node will be released back into the available resource pool. qstat -an wheeler-sn.alliance.unm.edu: Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------------------- ------------ -------- ---------- ------- --- --- ------ -------- -- -------- 55811.wheeler-sn.alliance.u user default B19F_re5e4 0 4 32 - - 48:00:00 R 47:30:42 wheeler296/0-7+wheeler295/0-7+wheeler282/0-7+wheeler280/0-7 Here, the nodes that this job is running on are wheeler296, wheeler295, wheeler282, and wheeler280, with 8 processors per node.","title":"Determining which nodes your Job is using:"},{"location":"checking_on_running_jobs/#viewing-output-and-error-files","text":"Once your job has completed, you should see two files, one output file and one error file, in the directory from which you submitted the Job: Jobname.oJobID and Jobname.eJobID (where Jobname refers to the name of the Job returned by qstat , and JobID refers to the numerical portion of the job identifier returned by qstat ). For the example job above, these two files would be named B19F_re5E4.o55811 and B19F_re5E4.e55811 respectively. Any output from the job sent to \u201cstandard output\u201d will be written to the output file, and any output sent to \u201cstandard error\u201d will be written to the error file. The amount of information in the output and error files varies depending on the program being run and how the PBS batch script was set up.","title":"Viewing Output and Error Files:"},{"location":"coldfront/","text":"CARC Cluster Allocation and Accounting Services Mokey is web application that provides self-service user account management for CARC systems. ColdFront WebUI Projects: Projects are acontainer that includes users, allocations, publications, grants, and other research information. Allocation: Allocations provides access the users to a resource. Resources: The resources list is everything that CARC maintains and provides services/access for to carc community. These can be shared resources such as clusters, storage or individual servers that single user or small group has access to. ColdFront: CARC Allocation Management System coldfront.alliance.unm.edu CARC's central portal for administration, reporting, and measuring scientific impact of cyberinfrastructure resources. * PI's Submitting a project Adding users to a project * CARC User's Resources Allocation for Users Mokey: Account Services mokey.alliance.unm.edu Mokey has implemented the following features, these are: Account signup, password reset/change, the use of SSH public keys and a Timed-based one-time password as dual-factor authentication. Account Signup Forgot/Change Password Two-Factor Authentication Add/Remove SSH Public Keys Add/Remove TOTP Tokens","title":"CARC Cluster Allocation and Accounting Services"},{"location":"coldfront/#carc-cluster-allocation-and-accounting-services","text":"Mokey is web application that provides self-service user account management for CARC systems.","title":"CARC Cluster Allocation and Accounting Services"},{"location":"coldfront/#coldfront-webui","text":"","title":"ColdFront WebUI"},{"location":"coldfront/#projects","text":"Projects are acontainer that includes users, allocations, publications, grants, and other research information.","title":"Projects:"},{"location":"coldfront/#allocation","text":"Allocations provides access the users to a resource.","title":"Allocation:"},{"location":"coldfront/#resources","text":"The resources list is everything that CARC maintains and provides services/access for to carc community. These can be shared resources such as clusters, storage or individual servers that single user or small group has access to.","title":"Resources:"},{"location":"coldfront/#coldfront-carc-allocation-management-system","text":"coldfront.alliance.unm.edu CARC's central portal for administration, reporting, and measuring scientific impact of cyberinfrastructure resources.","title":"ColdFront: CARC Allocation Management System"},{"location":"coldfront/#pis","text":"Submitting a project Adding users to a project","title":"* PI's"},{"location":"coldfront/#carc-users","text":"Resources Allocation for Users","title":"* CARC User's"},{"location":"coldfront/#mokey-account-services","text":"mokey.alliance.unm.edu Mokey has implemented the following features, these are: Account signup, password reset/change, the use of SSH public keys and a Timed-based one-time password as dual-factor authentication. Account Signup Forgot/Change Password Two-Factor Authentication Add/Remove SSH Public Keys Add/Remove TOTP Tokens","title":"Mokey: Account Services"},{"location":"export_control/","text":"Export control 1. Overview of Export Restrictions The U.S. Government controls exports of sensitive equipment, software and technology as a means to promote U.S. national security interests and foreign policy objectives. 2. Application to Software Software is regulated by the U.S. Department of Commerce, Bureau of Industry and Security, under the Export Administration Regulations (\"EAR\"). A software export under the EAR includes \"any release of technology or software subject to the EAR in a foreign country,\" or any release of \"source code subject to the EAR to a foreign national.\" The term \"release\" is defined very broadly to include inspection by foreign nationals, exchanges of information, or the application of personal knowledge or technical experience acquired in the United States. 3. Exceptions EAR excludes publicly-available software, except for some encryption technology. Other exceptions may be available for non-publicly licensed software. To determine if exceptions are available, faculty principal investigators should contact the UNM Main Campus Office of Export Control for guidance prior to requesting EAR-controlled software installations on CARC systems. A written exception from UNM Export Control will be required. 4. Installation of Export Control Software All requests for installation of export-controlled software on CARC systems will require prior written approval or a written exception from UNM Export Control. 5. Contact Information Main Campus email: export@unm.edu phone: 505-277-2968","title":"Export Control"},{"location":"export_control/#export-control","text":"","title":"Export control"},{"location":"export_control/#1-overview-of-export-restrictions","text":"The U.S. Government controls exports of sensitive equipment, software and technology as a means to promote U.S. national security interests and foreign policy objectives.","title":"1. Overview of Export Restrictions"},{"location":"export_control/#2-application-to-software","text":"Software is regulated by the U.S. Department of Commerce, Bureau of Industry and Security, under the Export Administration Regulations (\"EAR\"). A software export under the EAR includes \"any release of technology or software subject to the EAR in a foreign country,\" or any release of \"source code subject to the EAR to a foreign national.\" The term \"release\" is defined very broadly to include inspection by foreign nationals, exchanges of information, or the application of personal knowledge or technical experience acquired in the United States.","title":"2. Application to Software"},{"location":"export_control/#3-exceptions","text":"EAR excludes publicly-available software, except for some encryption technology. Other exceptions may be available for non-publicly licensed software. To determine if exceptions are available, faculty principal investigators should contact the UNM Main Campus Office of Export Control for guidance prior to requesting EAR-controlled software installations on CARC systems. A written exception from UNM Export Control will be required.","title":"3. Exceptions"},{"location":"export_control/#4-installation-of-export-control-software","text":"All requests for installation of export-controlled software on CARC systems will require prior written approval or a written exception from UNM Export Control.","title":"4. Installation of Export Control Software"},{"location":"export_control/#5-contact-information","text":"Main Campus email: export@unm.edu phone: 505-277-2968","title":"5. Contact Information"},{"location":"genome_evaluation/","text":"Reference genome evaluation with QUAST and BUSCO Whether you're using an already assembled reference genome from a database like NCBI or evaluating one you've assembled for completeness, it's important to check the contiguity and completeness of your reference before using it. Here we'll discuss tools for assessing these metrics: QUAST (Quality Assessment Tool for Genome Assemblies) for contiguity and BUSCO (Benchmarking Universal Single-Copy Orthologs) for completeness. Here we'll go over how to run these two programs on CARC and optionally install them with conda. Assessing contiguity with QUAST Although QUAST is fairly easy to run on personal computers, it is much easier to run in the place you already have your reference (which hopefully is CARC!). QUAST is not currently a module on any machine, so we'll install it using conda. First, if conda isn't initialized, you'll need to load a module for miniconda (with this specific module being the one on wheeler). Then, you'll make a new environment with QUAST. Unfortunately, as of writing this QuickByte, QUAST has dependency issues with other programs, and can't be in the same environment as BUSCO: module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik conda create --name quast-env --channel bioconda quast Now for actually running QUAST! This is simple enough, but there are a few key options to know about (you can read them all here ). First, if you are working with an assembly over 100 Mbp, you should use the --large option to improve accuracy and speed. It automatically applies some relevant flags, too (e.g. sets the minimum evaluated contig size to 3000). Second, you'll want to use the --threads option to use the built in parallelization. This simplest version would look like: quast /path/to/focal_reference.fa -o /path/to/quast_output --large --threads 8 Here are two other options that may be of interest. One option for scaffolded genomes it \"--split scaffolds\", where it will add a second output directory evaluating contigs instead of scaffolds in your reference. Also, if you'd like to compare to a better, preexisting reference genome (needed for some functions like putative missassemblies), use the -r flag to specify the path to a different reference. One final option that is more computationally intensive is gene finding with --glimmer, which uses GlimmerHMM to identify possible protein coding genes. Note that the options using GeneMark (e.g. --gene-finding) are not availible on the bioconda distribution of this software. This advanced version would look like: quast /path/to/focal_reference.fa -o /path/to/quast_output -r /path/to/high_qual_refernece --large --glimmer --split-scaffolds --threads 8 We'll run QUAST with a submission script, here using PBS variables. This is also made for Wheeler, being run on one core. We will store the output in a subdirectory in the submission directory. This examples runs the basic stats above on a scaffolded reference genome used in a different QuickByte borrowed from a great paper about conservation genomics . module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik source activate quast-env # sample command using a reference genome quast /projects/shared/tutorials/quickbytes/GATK/sagegrouse_reference.fa -o $PBS_O_WORKDIR/quast_output --large --threads 8 There are a few ways to assess the output in the output directory, with the most convenient being report.txt. A sample report for running the above command is below. Major points of interest include the total length, N50 and L50 , total length in scaffolds of given sizes (e.g. >= 50000 bp), and number of contigs (counting scaffolds as contigs) of given lengths. You can also transfer a report PDF (report.pdf) to a desktop to view it some nice figures illustrating important info like cumulative reference size for given numbers of contigs. All statistics are based on contigs of size >= 3000 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs). Assembly sagegrouse_reference # contigs (>= 0 bp) 985 # contigs (>= 1000 bp) 985 # contigs (>= 5000 bp) 983 # contigs (>= 10000 bp) 967 # contigs (>= 25000 bp) 698 # contigs (>= 50000 bp) 527 Total length (>= 0 bp) 1000043310 Total length (>= 1000 bp) 1000043310 Total length (>= 5000 bp) 1000035416 Total length (>= 10000 bp) 999906513 Total length (>= 25000 bp) 995198919 Total length (>= 50000 bp) 989315597 # contigs 985 Largest contig 19076553 Total length 1000043310 GC (%) 41.52 N50 3849570 N75 2122777 L50 81 L75 165 # N's per 100 kbp 0.43 Assessing completeness with BUSCO The next step will be to assess how complete our genome is by seeing how many single-copy orthologs are detected. This is important, because obviously if our genome is missing important portions, those regions won't be detected in analyses. For this, we will use BUSCO. As it stands, there are some issues installing BUSCO with conda (new versions fail to solve), and it doesn't work well as a spack module, so I'll provide a conda .yml for you. First, copy the file from our shared tutorial directory like \"cp /projects/shared/tutorials/quickbytes/busco-env.yml ~/path/to/directory\" or exapand this bit and manually copy its contents: Text for the .yml file name: busco-env channels: - bioconda - ursky - conda-forge - defaults dependencies: - _libgcc_mutex=0.1=conda_forge - _openmp_mutex=4.5=1_gnu - _r-mutex=1.0.1=anacondar_1 - augustus=3.4.0=pl5262hb677c0c_1 - bamtools=2.5.1=h9a82719_9 - binutils_impl_linux-64=2.35.1=h193b22a_2 - binutils_linux-64=2.35=h67ddf6f_30 - biopython=1.79=py39h3811e60_0 - blast=2.11.0=pl5262h3289130_1 - boost=1.74.0=py39h5472131_3 - boost-cpp=1.74.0=h312852a_4 - busco=5.1.3=pyhdfd78af_0 - bwidget=1.9.14=ha770c72_0 - bzip2=1.0.8=h7f98852_4 - c-ares=1.17.1=h7f98852_1 - ca-certificates=2021.5.30=ha878542_0 - cairo=1.16.0=h6cf1ce9_1008 - cdbtools=0.99=h9a82719_6 - certifi=2021.5.30=py39hf3d152e_0 - curl=7.77.0=hea6ffbf_0 - diamond=2.0.9=hdcc8f71_0 - findutils=4.6.0=h14c3975_1000 - font-ttf-dejavu-sans-mono=2.37=hab24e00_0 - font-ttf-inconsolata=3.000=h77eed37_0 - font-ttf-source-code-pro=2.038=h77eed37_0 - font-ttf-ubuntu=0.83=hab24e00_0 - fontconfig=2.13.1=hba837de_1005 - fonts-conda-ecosystem=1=0 - fonts-conda-forge=1=0 - freetype=2.10.4=h0708190_1 - fribidi=1.0.10=h36c2ea0_0 - gcc_impl_linux-64=9.3.0=h70c0ae5_19 - gcc_linux-64=9.3.0=hf25ea35_30 - gettext=0.19.8.1=h0b5b191_1005 - gfortran_impl_linux-64=9.3.0=hc4a2995_19 - gfortran_linux-64=9.3.0=hdc58fab_30 - gmp=6.2.1=h58526e2_0 - graphite2=1.3.13=h58526e2_1001 - gsl=2.6=he838d99_2 - gxx_impl_linux-64=9.3.0=hd87eabc_19 - gxx_linux-64=9.3.0=h3fbe746_30 - harfbuzz=2.8.1=h83ec7ef_0 - hmmer=3.1b2=3 - htslib=1.12=h9093b5e_1 - icu=68.1=h58526e2_0 - jbig=2.1=h7f98852_2003 - jpeg=9d=h36c2ea0_0 - kernel-headers_linux-64=2.6.32=h77966d4_13 - krb5=1.19.1=hcc1bbae_0 - ld_impl_linux-64=2.35.1=hea4e1c9_2 - lerc=2.2.1=h9c3ff4c_0 - libblas=3.9.0=9_openblas - libcblas=3.9.0=9_openblas - libcurl=7.77.0=h2574ce0_0 - libdeflate=1.7=h7f98852_5 - libedit=3.1.20191231=he28a2e2_2 - libev=4.33=h516909a_1 - libffi=3.3=h58526e2_2 - libgcc=7.2.0=h69d50b8_2 - libgcc-devel_linux-64=9.3.0=h7864c58_19 - libgcc-ng=9.3.0=h2828fa1_19 - libgfortran-ng=9.3.0=hff62375_19 - libgfortran5=9.3.0=hff62375_19 - libglib=2.68.3=h3e27bee_0 - libgomp=9.3.0=h2828fa1_19 - libiconv=1.16=h516909a_0 - libidn2=2.3.1=h7f98852_0 - liblapack=3.9.0=9_openblas - libnghttp2=1.43.0=h812cca2_0 - libopenblas=0.3.15=pthreads_h8fe5266_1 - libpng=1.6.37=h21135ba_2 - libssh2=1.9.0=ha56f1ee_6 - libstdcxx-devel_linux-64=9.3.0=hb016644_19 - libstdcxx-ng=9.3.0=h6de172a_19 - libtiff=4.3.0=hf544144_1 - libunistring=0.9.10=h14c3975_0 - libuuid=2.32.1=h7f98852_1000 - libwebp-base=1.2.0=h7f98852_2 - libxcb=1.13=h7f98852_1003 - libxml2=2.9.12=h72842e0_0 - links=1.8.7=pl5262h7d875b9_2 - lp_solve=5.5.2.5=h14c3975_1001 - lz4-c=1.9.3=h9c3ff4c_0 - make=4.3=hd18ef5c_1 - metis=5.1.0=h58526e2_1006 - mpfr=4.0.2=he80fd80_1 - mysql-connector-c=6.1.11=h6eb9d5d_1007 - ncurses=6.2=h58526e2_4 - numpy=1.20.3=py39hdbf815f_1 - openssl=1.1.1k=h7f98852_0 - pango=1.48.5=hb8ff022_0 - pcre=8.45=h9c3ff4c_0 - pcre2=10.36=h032f7d1_1 - perl=5.26.2=h36c2ea0_1008 - perl-apache-test=1.40=pl526_1 - perl-app-cpanminus=1.7044=pl526_1 - perl-archive-tar=2.32=pl526_0 - perl-base=2.23=pl526_1 - perl-carp=1.38=pl526_3 - perl-class-load=0.25=pl526_0 - perl-class-load-xs=0.10=pl526h6bb024c_2 - perl-class-method-modifiers=2.12=pl526_0 - perl-constant=1.33=pl526_1 - perl-cpan-meta=2.150010=pl526_0 - perl-cpan-meta-requirements=2.140=pl526_0 - perl-cpan-meta-yaml=0.018=pl526_0 - perl-data-dumper=2.173=pl526_0 - perl-data-optlist=0.110=pl526_2 - perl-dbi=1.642=pl526_0 - perl-devel-globaldestruction=0.14=pl526_0 - perl-devel-overloadinfo=0.005=pl526_0 - perl-devel-stacktrace=2.04=pl526_0 - perl-dist-checkconflicts=0.11=pl526_2 - perl-encode=2.88=pl526_1 - perl-encode-locale=1.05=pl526_6 - perl-eval-closure=0.14=pl526h6bb024c_4 - perl-exporter=5.72=pl526_1 - perl-exporter-tiny=1.002001=pl526_0 - perl-extutils-cbuilder=0.280230=pl526_1 - perl-extutils-makemaker=7.36=pl526_1 - perl-extutils-manifest=1.72=pl526_0 - perl-extutils-parsexs=3.35=pl526_0 - perl-file-listing=6.04=pl526_1 - perl-file-path=2.16=pl526_0 - perl-file-temp=0.2304=pl526_2 - perl-file-which=1.23=pl526_0 - perl-getopt-long=2.50=pl526_1 - perl-ipc-cmd=1.02=pl526_0 - perl-json-pp=4.04=pl526_0 - perl-locale-maketext-simple=0.21=pl526_2 - perl-module-build=0.4224=pl526_3 - perl-module-corelist=5.20190524=pl526_0 - perl-module-implementation=0.09=pl526_2 - perl-module-load=0.32=pl526_1 - perl-module-load-conditional=0.68=pl526_2 - perl-module-metadata=1.000036=pl526_0 - perl-module-runtime=0.016=pl526_1 - perl-module-runtime-conflicts=0.003=pl526_0 - perl-moo=2.003004=pl526_0 - perl-moose=2.2011=pl526hf484d3e_1 - perl-mro-compat=0.13=pl526_0 - perl-package-deprecationmanager=0.17=pl526_0 - perl-package-stash=0.38=pl526hf484d3e_1 - perl-package-stash-xs=0.28=pl526hf484d3e_1 - perl-parallel-forkmanager=2.02=pl526_0 - perl-params-check=0.38=pl526_1 - perl-params-util=1.07=pl526h6bb024c_4 - perl-parent=0.236=pl526_1 - perl-pathtools=3.75=pl526h14c3975_1 - perl-perl-ostype=1.010=pl526_1 - perl-role-tiny=2.000008=pl526_0 - perl-scalar-list-utils=1.52=pl526h516909a_0 - perl-storable=3.15=pl526h14c3975_0 - perl-sub-exporter=0.987=pl526_2 - perl-sub-exporter-progressive=0.001013=pl526_0 - perl-sub-identify=0.14=pl526h14c3975_0 - perl-sub-install=0.928=pl526_2 - perl-sub-name=0.21=pl526_1 - perl-sub-quote=2.006003=pl526_1 - perl-text-abbrev=1.02=pl526_0 - perl-text-parsewords=3.30=pl526_0 - perl-try-tiny=0.30=pl526_1 - perl-version=0.9924=pl526_0 - perl-xsloader=0.24=pl526_0 - perl-yaml=1.29=pl526_0 - pip=21.1.2=pyhd8ed1ab_0 - pixman=0.40.0=h36c2ea0_0 - pthread-stubs=0.4=h36c2ea0_1001 - python=3.9.5=h49503c6_0_cpython - python_abi=3.9=1_cp39 - pytz=2021.1=pyhd8ed1ab_0 - r-assertthat=0.2.1=r40hc72bb7e_2 - r-backports=1.2.1=r40hcfec24a_0 - r-base=4.0.5=h9e01966_1 - r-brio=1.1.2=r40hcfec24a_0 - r-callr=3.7.0=r40hc72bb7e_0 - r-cli=2.5.0=r40hc72bb7e_0 - r-colorspace=2.0_1=r40hcfec24a_0 - r-crayon=1.4.1=r40hc72bb7e_0 - r-desc=1.3.0=r40hc72bb7e_0 - r-diffobj=0.3.4=r40hcfec24a_0 - r-digest=0.6.27=r40h03ef668_0 - r-ellipsis=0.3.2=r40hcfec24a_0 - r-evaluate=0.14=r40hc72bb7e_2 - r-fansi=0.5.0=r40hcfec24a_0 - r-farver=2.1.0=r40h03ef668_0 - r-ggplot2=3.3.4=r40hc72bb7e_0 - r-glue=1.4.2=r40hcfec24a_0 - r-gtable=0.3.0=r40hc72bb7e_3 - r-isoband=0.2.4=r40h03ef668_0 - r-jsonlite=1.7.2=r40hcfec24a_0 - r-labeling=0.4.2=r40hc72bb7e_0 - r-lattice=0.20_44=r40hcfec24a_0 - r-lifecycle=1.0.0=r40hc72bb7e_0 - r-magrittr=2.0.1=r40hcfec24a_1 - r-mass=7.3_54=r40hcfec24a_0 - r-matrix=1.3_4=r40he454529_0 - r-mgcv=1.8_36=r40he454529_0 - r-munsell=0.5.0=r40hc72bb7e_1003 - r-nlme=3.1_152=r40h859d828_0 - r-pillar=1.6.1=r40hc72bb7e_0 - r-pkgconfig=2.0.3=r40hc72bb7e_1 - r-pkgload=1.2.1=r40h03ef668_0 - r-praise=1.0.0=r40hc72bb7e_1004 - r-processx=3.5.2=r40hcfec24a_0 - r-ps=1.6.0=r40hcfec24a_0 - r-r6=2.5.0=r40hc72bb7e_0 - r-rcolorbrewer=1.1_2=r40h785f33e_1003 - r-rcpp=1.0.6=r40h03ef668_0 - r-rematch2=2.1.2=r40hc72bb7e_1 - r-rlang=0.4.11=r40hcfec24a_0 - r-rprojroot=2.0.2=r40hc72bb7e_0 - r-rstudioapi=0.13=r40hc72bb7e_0 - r-scales=1.1.1=r40hc72bb7e_0 - r-testthat=3.0.3=r40h03ef668_0 - r-tibble=3.1.2=r40hcfec24a_0 - r-utf8=1.2.1=r40hcfec24a_0 - r-vctrs=0.3.8=r40hcfec24a_1 - r-viridislite=0.4.0=r40hc72bb7e_0 - r-waldo=0.2.5=r40hc72bb7e_0 - r-withr=2.4.2=r40hc72bb7e_0 - readline=8.1=h46c0cb4_0 - sed=4.8=he412f7d_0 - sepp=4.4.0=py39_0 - setuptools=49.6.0=py39hf3d152e_3 - sqlite=3.35.5=h74cdb3f_0 - suitesparse=5.10.1=hd8046ac_0 - sysroot_linux-64=2.12=h77966d4_13 - tbb=2020.2=h4bd325d_4 - tigmint=1.2.3=py39h39abbe0_0 - tk=8.6.10=h21135ba_1 - tktable=2.10=hb7b940f_3 - tzdata=2021a=he74cb21_0 - ucsc-fatotwobit=377=h0b8a92a_4 - ucsc-twobitinfo=377=h0b8a92a_2 - wheel=0.36.2=pyhd3deb0d_0 - xorg-kbproto=1.0.7=h7f98852_1002 - xorg-libice=1.0.10=h7f98852_0 - xorg-libsm=1.2.3=hd9c2040_1000 - xorg-libx11=1.7.2=h7f98852_0 - xorg-libxau=1.0.9=h7f98852_0 - xorg-libxdmcp=1.1.3=h7f98852_0 - xorg-libxext=1.3.4=h7f98852_1 - xorg-libxrender=0.9.10=h7f98852_1003 - xorg-libxt=1.2.1=h7f98852_2 - xorg-renderproto=0.11.1=h7f98852_1002 - xorg-xextproto=7.3.0=h7f98852_1002 - xorg-xproto=7.0.31=h7f98852_1007 - xz=5.2.5=h516909a_1 - zlib=1.2.11=h516909a_1010 - zstd=1.5.0=ha95c52a_0 prefix: ~/.conda/envs/busco-env You then install it like this: module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik # commented command is how this was made # conda create --name busco-env --channel bioconda --channel conda-forge busco=5.1.3 conda env create -f busco-env.yml Then, you'll need to find which dataset of genes you want to use. To do this, either activate the conda environment (if you have it) or load the module and run \"busco --list-datasets\". You'll see an output like this, with a taxonomic hierarchy. You should choose the one most representative of your dataset. We'll do an example for our Sage Grouse genome again. First, we'll scroll down to eurkaryotes (eukaryota_odb10), then animals (metazoa_odb10), and then vertrabrates (vertabrata_odb10) below. The smallest group for us to use is the set for all birds, which is \"aves_odb10\". You can see it used in our sample command below. - vertebrata_odb10 - actinopterygii_odb10 - cyprinodontiformes_odb10 - tetrapoda_odb10 - mammalia_odb10 - eutheria_odb10 - euarchontoglires_odb10 - glires_odb10 - primates_odb10 - laurasiatheria_odb10 - carnivora_odb10 - cetartiodactyla_odb10 - sauropsida_odb10 - aves_odb10 - passeriformes_odb10 Then you'll run BUSCO, which has a manual with full info on options here . It has the odd quick of requiring modifying a config file to have its output anywhere but the current directory, so we'll just have everything in the submission directory (note that if you already have that directory made, you need the --force or -f flag to overwrite it). The first two flags after our input genome (-i) and output directory (-o) are the number of cores to use (--cpus) and the mode. Because we are evaluating a reference genome, we'll use \"--mode genome\". Finally, we have the lineage dataset (-l), which is the \"aves_odb10\" from above. cd $PBS_O_WORKDIR module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik source activate busco-test busco -i /projects/shared/tutorials/quickbytes/GATK/sagegrouse_reference.fa -o busco_test --cpu 8 --mode genome -l aves_odb10 The output from this is pretty straightforward, in the short summary file in the output directory (something like \"short_summary.specific.aves_odb10.busco_test.txt\"). An example of what to expect from the above command is below: C:95.1%[S:94.4%,D:0.7%],F:1.0%,M:3.9%,n:10844 10319 Complete BUSCOs (C) 10242 Complete and single-copy BUSCOs (S) 77 Complete and duplicated BUSCOs (D) 113 Fragmented BUSCOs (F) 412 Missing BUSCOs (M) 10844 Total BUSCO groups searched Note that we have 95.1% complete BUSCOs, which is greater than the ideal 95%! Don't worry if yours is a bit lower. Other things to note, some of these complete BUSCOs are single-copy and even more are fragmented, so only 3.9% are fully missing. Citations Gurevich, A., Saveliev, V., Vyahhi, N., & Tesler, G. (2013). QUAST: Quality assessment tool for genome assemblies. Bioinformatics, 29(8), 1072\u20131075. https://doi.org/10.1093/bioinformatics/btt086 Sim\u00e3o, F. A., Waterhouse, R. M., Ioannidis, P., Kriventseva, E. V., & Zdobnov, E. M. (2015). BUSCO: Assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics, 31(19), 3210\u20133212. https://doi.org/10.1093/bioinformatics/btv351","title":"Genome evaluation with QUAST and BUSCO"},{"location":"genome_evaluation/#reference-genome-evaluation-with-quast-and-busco","text":"Whether you're using an already assembled reference genome from a database like NCBI or evaluating one you've assembled for completeness, it's important to check the contiguity and completeness of your reference before using it. Here we'll discuss tools for assessing these metrics: QUAST (Quality Assessment Tool for Genome Assemblies) for contiguity and BUSCO (Benchmarking Universal Single-Copy Orthologs) for completeness. Here we'll go over how to run these two programs on CARC and optionally install them with conda.","title":"Reference genome evaluation with QUAST and BUSCO"},{"location":"genome_evaluation/#assessing-contiguity-with-quast","text":"Although QUAST is fairly easy to run on personal computers, it is much easier to run in the place you already have your reference (which hopefully is CARC!). QUAST is not currently a module on any machine, so we'll install it using conda. First, if conda isn't initialized, you'll need to load a module for miniconda (with this specific module being the one on wheeler). Then, you'll make a new environment with QUAST. Unfortunately, as of writing this QuickByte, QUAST has dependency issues with other programs, and can't be in the same environment as BUSCO: module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik conda create --name quast-env --channel bioconda quast Now for actually running QUAST! This is simple enough, but there are a few key options to know about (you can read them all here ). First, if you are working with an assembly over 100 Mbp, you should use the --large option to improve accuracy and speed. It automatically applies some relevant flags, too (e.g. sets the minimum evaluated contig size to 3000). Second, you'll want to use the --threads option to use the built in parallelization. This simplest version would look like: quast /path/to/focal_reference.fa -o /path/to/quast_output --large --threads 8 Here are two other options that may be of interest. One option for scaffolded genomes it \"--split scaffolds\", where it will add a second output directory evaluating contigs instead of scaffolds in your reference. Also, if you'd like to compare to a better, preexisting reference genome (needed for some functions like putative missassemblies), use the -r flag to specify the path to a different reference. One final option that is more computationally intensive is gene finding with --glimmer, which uses GlimmerHMM to identify possible protein coding genes. Note that the options using GeneMark (e.g. --gene-finding) are not availible on the bioconda distribution of this software. This advanced version would look like: quast /path/to/focal_reference.fa -o /path/to/quast_output -r /path/to/high_qual_refernece --large --glimmer --split-scaffolds --threads 8 We'll run QUAST with a submission script, here using PBS variables. This is also made for Wheeler, being run on one core. We will store the output in a subdirectory in the submission directory. This examples runs the basic stats above on a scaffolded reference genome used in a different QuickByte borrowed from a great paper about conservation genomics . module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik source activate quast-env # sample command using a reference genome quast /projects/shared/tutorials/quickbytes/GATK/sagegrouse_reference.fa -o $PBS_O_WORKDIR/quast_output --large --threads 8 There are a few ways to assess the output in the output directory, with the most convenient being report.txt. A sample report for running the above command is below. Major points of interest include the total length, N50 and L50 , total length in scaffolds of given sizes (e.g. >= 50000 bp), and number of contigs (counting scaffolds as contigs) of given lengths. You can also transfer a report PDF (report.pdf) to a desktop to view it some nice figures illustrating important info like cumulative reference size for given numbers of contigs. All statistics are based on contigs of size >= 3000 bp, unless otherwise noted (e.g., \"# contigs (>= 0 bp)\" and \"Total length (>= 0 bp)\" include all contigs). Assembly sagegrouse_reference # contigs (>= 0 bp) 985 # contigs (>= 1000 bp) 985 # contigs (>= 5000 bp) 983 # contigs (>= 10000 bp) 967 # contigs (>= 25000 bp) 698 # contigs (>= 50000 bp) 527 Total length (>= 0 bp) 1000043310 Total length (>= 1000 bp) 1000043310 Total length (>= 5000 bp) 1000035416 Total length (>= 10000 bp) 999906513 Total length (>= 25000 bp) 995198919 Total length (>= 50000 bp) 989315597 # contigs 985 Largest contig 19076553 Total length 1000043310 GC (%) 41.52 N50 3849570 N75 2122777 L50 81 L75 165 # N's per 100 kbp 0.43","title":"Assessing contiguity with QUAST"},{"location":"genome_evaluation/#assessing-completeness-with-busco","text":"The next step will be to assess how complete our genome is by seeing how many single-copy orthologs are detected. This is important, because obviously if our genome is missing important portions, those regions won't be detected in analyses. For this, we will use BUSCO. As it stands, there are some issues installing BUSCO with conda (new versions fail to solve), and it doesn't work well as a spack module, so I'll provide a conda .yml for you. First, copy the file from our shared tutorial directory like \"cp /projects/shared/tutorials/quickbytes/busco-env.yml ~/path/to/directory\" or exapand this bit and manually copy its contents: Text for the .yml file name: busco-env channels: - bioconda - ursky - conda-forge - defaults dependencies: - _libgcc_mutex=0.1=conda_forge - _openmp_mutex=4.5=1_gnu - _r-mutex=1.0.1=anacondar_1 - augustus=3.4.0=pl5262hb677c0c_1 - bamtools=2.5.1=h9a82719_9 - binutils_impl_linux-64=2.35.1=h193b22a_2 - binutils_linux-64=2.35=h67ddf6f_30 - biopython=1.79=py39h3811e60_0 - blast=2.11.0=pl5262h3289130_1 - boost=1.74.0=py39h5472131_3 - boost-cpp=1.74.0=h312852a_4 - busco=5.1.3=pyhdfd78af_0 - bwidget=1.9.14=ha770c72_0 - bzip2=1.0.8=h7f98852_4 - c-ares=1.17.1=h7f98852_1 - ca-certificates=2021.5.30=ha878542_0 - cairo=1.16.0=h6cf1ce9_1008 - cdbtools=0.99=h9a82719_6 - certifi=2021.5.30=py39hf3d152e_0 - curl=7.77.0=hea6ffbf_0 - diamond=2.0.9=hdcc8f71_0 - findutils=4.6.0=h14c3975_1000 - font-ttf-dejavu-sans-mono=2.37=hab24e00_0 - font-ttf-inconsolata=3.000=h77eed37_0 - font-ttf-source-code-pro=2.038=h77eed37_0 - font-ttf-ubuntu=0.83=hab24e00_0 - fontconfig=2.13.1=hba837de_1005 - fonts-conda-ecosystem=1=0 - fonts-conda-forge=1=0 - freetype=2.10.4=h0708190_1 - fribidi=1.0.10=h36c2ea0_0 - gcc_impl_linux-64=9.3.0=h70c0ae5_19 - gcc_linux-64=9.3.0=hf25ea35_30 - gettext=0.19.8.1=h0b5b191_1005 - gfortran_impl_linux-64=9.3.0=hc4a2995_19 - gfortran_linux-64=9.3.0=hdc58fab_30 - gmp=6.2.1=h58526e2_0 - graphite2=1.3.13=h58526e2_1001 - gsl=2.6=he838d99_2 - gxx_impl_linux-64=9.3.0=hd87eabc_19 - gxx_linux-64=9.3.0=h3fbe746_30 - harfbuzz=2.8.1=h83ec7ef_0 - hmmer=3.1b2=3 - htslib=1.12=h9093b5e_1 - icu=68.1=h58526e2_0 - jbig=2.1=h7f98852_2003 - jpeg=9d=h36c2ea0_0 - kernel-headers_linux-64=2.6.32=h77966d4_13 - krb5=1.19.1=hcc1bbae_0 - ld_impl_linux-64=2.35.1=hea4e1c9_2 - lerc=2.2.1=h9c3ff4c_0 - libblas=3.9.0=9_openblas - libcblas=3.9.0=9_openblas - libcurl=7.77.0=h2574ce0_0 - libdeflate=1.7=h7f98852_5 - libedit=3.1.20191231=he28a2e2_2 - libev=4.33=h516909a_1 - libffi=3.3=h58526e2_2 - libgcc=7.2.0=h69d50b8_2 - libgcc-devel_linux-64=9.3.0=h7864c58_19 - libgcc-ng=9.3.0=h2828fa1_19 - libgfortran-ng=9.3.0=hff62375_19 - libgfortran5=9.3.0=hff62375_19 - libglib=2.68.3=h3e27bee_0 - libgomp=9.3.0=h2828fa1_19 - libiconv=1.16=h516909a_0 - libidn2=2.3.1=h7f98852_0 - liblapack=3.9.0=9_openblas - libnghttp2=1.43.0=h812cca2_0 - libopenblas=0.3.15=pthreads_h8fe5266_1 - libpng=1.6.37=h21135ba_2 - libssh2=1.9.0=ha56f1ee_6 - libstdcxx-devel_linux-64=9.3.0=hb016644_19 - libstdcxx-ng=9.3.0=h6de172a_19 - libtiff=4.3.0=hf544144_1 - libunistring=0.9.10=h14c3975_0 - libuuid=2.32.1=h7f98852_1000 - libwebp-base=1.2.0=h7f98852_2 - libxcb=1.13=h7f98852_1003 - libxml2=2.9.12=h72842e0_0 - links=1.8.7=pl5262h7d875b9_2 - lp_solve=5.5.2.5=h14c3975_1001 - lz4-c=1.9.3=h9c3ff4c_0 - make=4.3=hd18ef5c_1 - metis=5.1.0=h58526e2_1006 - mpfr=4.0.2=he80fd80_1 - mysql-connector-c=6.1.11=h6eb9d5d_1007 - ncurses=6.2=h58526e2_4 - numpy=1.20.3=py39hdbf815f_1 - openssl=1.1.1k=h7f98852_0 - pango=1.48.5=hb8ff022_0 - pcre=8.45=h9c3ff4c_0 - pcre2=10.36=h032f7d1_1 - perl=5.26.2=h36c2ea0_1008 - perl-apache-test=1.40=pl526_1 - perl-app-cpanminus=1.7044=pl526_1 - perl-archive-tar=2.32=pl526_0 - perl-base=2.23=pl526_1 - perl-carp=1.38=pl526_3 - perl-class-load=0.25=pl526_0 - perl-class-load-xs=0.10=pl526h6bb024c_2 - perl-class-method-modifiers=2.12=pl526_0 - perl-constant=1.33=pl526_1 - perl-cpan-meta=2.150010=pl526_0 - perl-cpan-meta-requirements=2.140=pl526_0 - perl-cpan-meta-yaml=0.018=pl526_0 - perl-data-dumper=2.173=pl526_0 - perl-data-optlist=0.110=pl526_2 - perl-dbi=1.642=pl526_0 - perl-devel-globaldestruction=0.14=pl526_0 - perl-devel-overloadinfo=0.005=pl526_0 - perl-devel-stacktrace=2.04=pl526_0 - perl-dist-checkconflicts=0.11=pl526_2 - perl-encode=2.88=pl526_1 - perl-encode-locale=1.05=pl526_6 - perl-eval-closure=0.14=pl526h6bb024c_4 - perl-exporter=5.72=pl526_1 - perl-exporter-tiny=1.002001=pl526_0 - perl-extutils-cbuilder=0.280230=pl526_1 - perl-extutils-makemaker=7.36=pl526_1 - perl-extutils-manifest=1.72=pl526_0 - perl-extutils-parsexs=3.35=pl526_0 - perl-file-listing=6.04=pl526_1 - perl-file-path=2.16=pl526_0 - perl-file-temp=0.2304=pl526_2 - perl-file-which=1.23=pl526_0 - perl-getopt-long=2.50=pl526_1 - perl-ipc-cmd=1.02=pl526_0 - perl-json-pp=4.04=pl526_0 - perl-locale-maketext-simple=0.21=pl526_2 - perl-module-build=0.4224=pl526_3 - perl-module-corelist=5.20190524=pl526_0 - perl-module-implementation=0.09=pl526_2 - perl-module-load=0.32=pl526_1 - perl-module-load-conditional=0.68=pl526_2 - perl-module-metadata=1.000036=pl526_0 - perl-module-runtime=0.016=pl526_1 - perl-module-runtime-conflicts=0.003=pl526_0 - perl-moo=2.003004=pl526_0 - perl-moose=2.2011=pl526hf484d3e_1 - perl-mro-compat=0.13=pl526_0 - perl-package-deprecationmanager=0.17=pl526_0 - perl-package-stash=0.38=pl526hf484d3e_1 - perl-package-stash-xs=0.28=pl526hf484d3e_1 - perl-parallel-forkmanager=2.02=pl526_0 - perl-params-check=0.38=pl526_1 - perl-params-util=1.07=pl526h6bb024c_4 - perl-parent=0.236=pl526_1 - perl-pathtools=3.75=pl526h14c3975_1 - perl-perl-ostype=1.010=pl526_1 - perl-role-tiny=2.000008=pl526_0 - perl-scalar-list-utils=1.52=pl526h516909a_0 - perl-storable=3.15=pl526h14c3975_0 - perl-sub-exporter=0.987=pl526_2 - perl-sub-exporter-progressive=0.001013=pl526_0 - perl-sub-identify=0.14=pl526h14c3975_0 - perl-sub-install=0.928=pl526_2 - perl-sub-name=0.21=pl526_1 - perl-sub-quote=2.006003=pl526_1 - perl-text-abbrev=1.02=pl526_0 - perl-text-parsewords=3.30=pl526_0 - perl-try-tiny=0.30=pl526_1 - perl-version=0.9924=pl526_0 - perl-xsloader=0.24=pl526_0 - perl-yaml=1.29=pl526_0 - pip=21.1.2=pyhd8ed1ab_0 - pixman=0.40.0=h36c2ea0_0 - pthread-stubs=0.4=h36c2ea0_1001 - python=3.9.5=h49503c6_0_cpython - python_abi=3.9=1_cp39 - pytz=2021.1=pyhd8ed1ab_0 - r-assertthat=0.2.1=r40hc72bb7e_2 - r-backports=1.2.1=r40hcfec24a_0 - r-base=4.0.5=h9e01966_1 - r-brio=1.1.2=r40hcfec24a_0 - r-callr=3.7.0=r40hc72bb7e_0 - r-cli=2.5.0=r40hc72bb7e_0 - r-colorspace=2.0_1=r40hcfec24a_0 - r-crayon=1.4.1=r40hc72bb7e_0 - r-desc=1.3.0=r40hc72bb7e_0 - r-diffobj=0.3.4=r40hcfec24a_0 - r-digest=0.6.27=r40h03ef668_0 - r-ellipsis=0.3.2=r40hcfec24a_0 - r-evaluate=0.14=r40hc72bb7e_2 - r-fansi=0.5.0=r40hcfec24a_0 - r-farver=2.1.0=r40h03ef668_0 - r-ggplot2=3.3.4=r40hc72bb7e_0 - r-glue=1.4.2=r40hcfec24a_0 - r-gtable=0.3.0=r40hc72bb7e_3 - r-isoband=0.2.4=r40h03ef668_0 - r-jsonlite=1.7.2=r40hcfec24a_0 - r-labeling=0.4.2=r40hc72bb7e_0 - r-lattice=0.20_44=r40hcfec24a_0 - r-lifecycle=1.0.0=r40hc72bb7e_0 - r-magrittr=2.0.1=r40hcfec24a_1 - r-mass=7.3_54=r40hcfec24a_0 - r-matrix=1.3_4=r40he454529_0 - r-mgcv=1.8_36=r40he454529_0 - r-munsell=0.5.0=r40hc72bb7e_1003 - r-nlme=3.1_152=r40h859d828_0 - r-pillar=1.6.1=r40hc72bb7e_0 - r-pkgconfig=2.0.3=r40hc72bb7e_1 - r-pkgload=1.2.1=r40h03ef668_0 - r-praise=1.0.0=r40hc72bb7e_1004 - r-processx=3.5.2=r40hcfec24a_0 - r-ps=1.6.0=r40hcfec24a_0 - r-r6=2.5.0=r40hc72bb7e_0 - r-rcolorbrewer=1.1_2=r40h785f33e_1003 - r-rcpp=1.0.6=r40h03ef668_0 - r-rematch2=2.1.2=r40hc72bb7e_1 - r-rlang=0.4.11=r40hcfec24a_0 - r-rprojroot=2.0.2=r40hc72bb7e_0 - r-rstudioapi=0.13=r40hc72bb7e_0 - r-scales=1.1.1=r40hc72bb7e_0 - r-testthat=3.0.3=r40h03ef668_0 - r-tibble=3.1.2=r40hcfec24a_0 - r-utf8=1.2.1=r40hcfec24a_0 - r-vctrs=0.3.8=r40hcfec24a_1 - r-viridislite=0.4.0=r40hc72bb7e_0 - r-waldo=0.2.5=r40hc72bb7e_0 - r-withr=2.4.2=r40hc72bb7e_0 - readline=8.1=h46c0cb4_0 - sed=4.8=he412f7d_0 - sepp=4.4.0=py39_0 - setuptools=49.6.0=py39hf3d152e_3 - sqlite=3.35.5=h74cdb3f_0 - suitesparse=5.10.1=hd8046ac_0 - sysroot_linux-64=2.12=h77966d4_13 - tbb=2020.2=h4bd325d_4 - tigmint=1.2.3=py39h39abbe0_0 - tk=8.6.10=h21135ba_1 - tktable=2.10=hb7b940f_3 - tzdata=2021a=he74cb21_0 - ucsc-fatotwobit=377=h0b8a92a_4 - ucsc-twobitinfo=377=h0b8a92a_2 - wheel=0.36.2=pyhd3deb0d_0 - xorg-kbproto=1.0.7=h7f98852_1002 - xorg-libice=1.0.10=h7f98852_0 - xorg-libsm=1.2.3=hd9c2040_1000 - xorg-libx11=1.7.2=h7f98852_0 - xorg-libxau=1.0.9=h7f98852_0 - xorg-libxdmcp=1.1.3=h7f98852_0 - xorg-libxext=1.3.4=h7f98852_1 - xorg-libxrender=0.9.10=h7f98852_1003 - xorg-libxt=1.2.1=h7f98852_2 - xorg-renderproto=0.11.1=h7f98852_1002 - xorg-xextproto=7.3.0=h7f98852_1002 - xorg-xproto=7.0.31=h7f98852_1007 - xz=5.2.5=h516909a_1 - zlib=1.2.11=h516909a_1010 - zstd=1.5.0=ha95c52a_0 prefix: ~/.conda/envs/busco-env You then install it like this: module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik # commented command is how this was made # conda create --name busco-env --channel bioconda --channel conda-forge busco=5.1.3 conda env create -f busco-env.yml Then, you'll need to find which dataset of genes you want to use. To do this, either activate the conda environment (if you have it) or load the module and run \"busco --list-datasets\". You'll see an output like this, with a taxonomic hierarchy. You should choose the one most representative of your dataset. We'll do an example for our Sage Grouse genome again. First, we'll scroll down to eurkaryotes (eukaryota_odb10), then animals (metazoa_odb10), and then vertrabrates (vertabrata_odb10) below. The smallest group for us to use is the set for all birds, which is \"aves_odb10\". You can see it used in our sample command below. - vertebrata_odb10 - actinopterygii_odb10 - cyprinodontiformes_odb10 - tetrapoda_odb10 - mammalia_odb10 - eutheria_odb10 - euarchontoglires_odb10 - glires_odb10 - primates_odb10 - laurasiatheria_odb10 - carnivora_odb10 - cetartiodactyla_odb10 - sauropsida_odb10 - aves_odb10 - passeriformes_odb10 Then you'll run BUSCO, which has a manual with full info on options here . It has the odd quick of requiring modifying a config file to have its output anywhere but the current directory, so we'll just have everything in the submission directory (note that if you already have that directory made, you need the --force or -f flag to overwrite it). The first two flags after our input genome (-i) and output directory (-o) are the number of cores to use (--cpus) and the mode. Because we are evaluating a reference genome, we'll use \"--mode genome\". Finally, we have the lineage dataset (-l), which is the \"aves_odb10\" from above. cd $PBS_O_WORKDIR module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik source activate busco-test busco -i /projects/shared/tutorials/quickbytes/GATK/sagegrouse_reference.fa -o busco_test --cpu 8 --mode genome -l aves_odb10 The output from this is pretty straightforward, in the short summary file in the output directory (something like \"short_summary.specific.aves_odb10.busco_test.txt\"). An example of what to expect from the above command is below: C:95.1%[S:94.4%,D:0.7%],F:1.0%,M:3.9%,n:10844 10319 Complete BUSCOs (C) 10242 Complete and single-copy BUSCOs (S) 77 Complete and duplicated BUSCOs (D) 113 Fragmented BUSCOs (F) 412 Missing BUSCOs (M) 10844 Total BUSCO groups searched Note that we have 95.1% complete BUSCOs, which is greater than the ideal 95%! Don't worry if yours is a bit lower. Other things to note, some of these complete BUSCOs are single-copy and even more are fragmented, so only 3.9% are fully missing.","title":"Assessing completeness with BUSCO"},{"location":"genome_evaluation/#citations","text":"Gurevich, A., Saveliev, V., Vyahhi, N., & Tesler, G. (2013). QUAST: Quality assessment tool for genome assemblies. Bioinformatics, 29(8), 1072\u20131075. https://doi.org/10.1093/bioinformatics/btt086 Sim\u00e3o, F. A., Waterhouse, R. M., Ioannidis, P., Kriventseva, E. V., & Zdobnov, E. M. (2015). BUSCO: Assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics, 31(19), 3210\u20133212. https://doi.org/10.1093/bioinformatics/btv351","title":"Citations"},{"location":"haskell/","text":"Haskell at CARC Haskell is a strongly typed functional language. In this QuickByte you will learn how to setup Haskell at CARC and create a simple matrix multiplication project using stack. On Wheeler: First we will install and load the stack environment: 1) module load miniconda3 2) conda create -n haskell stack -c conda-forge 3) source activate haskell Or, alternatively, load the spack module: module load haskell-stack-stable-gcc-4.8.5-exczvin Please note that you should choose one method for using Haskell-Stack, Anaconda or the installed module, as you can run into compatability issues switching between methods. Now create a new stack project: 4) stack new matmul new-template 5) cd matmul Edit the main source file with your preferred text editor: 6) emacs app/Main.hs The following code creates and multiplies two 4x4 matrices: module Main where import Lib import Data.Matrix main :: IO () m1 = matrix 4 4 $ \\(i,j) -> 2*i - j m2 = matrix 4 4 $ \\(i,j) -> 2*i - j test = multStd m1 m2 main = do putStrLn (show test) Add the dependency on the matrix package to the project: 7) emacs package.yaml 8) Add \"- matrix\" under dependencies in the executables section so it reads: dependencies: - matmul - matrix Build your project: 9) stack build Run your compiled program: (haskell) mfricke@wheeler-sn:~/matmul$ .stack-work/install/x86_64-linux/lts-13.17/8.6.4/bin/matmul-exe \u250c \u2510 \u2502 -18 -16 -14 -12 \u2502 \u2502 14 8 2 -4 \u2502 \u2502 46 32 18 4 \u2502 \u2502 78 56 34 12 \u2502 \u2514 \u2518","title":"Haskell at CARC"},{"location":"haskell/#haskell-at-carc","text":"Haskell is a strongly typed functional language. In this QuickByte you will learn how to setup Haskell at CARC and create a simple matrix multiplication project using stack. On Wheeler: First we will install and load the stack environment: 1) module load miniconda3 2) conda create -n haskell stack -c conda-forge 3) source activate haskell Or, alternatively, load the spack module: module load haskell-stack-stable-gcc-4.8.5-exczvin Please note that you should choose one method for using Haskell-Stack, Anaconda or the installed module, as you can run into compatability issues switching between methods. Now create a new stack project: 4) stack new matmul new-template 5) cd matmul Edit the main source file with your preferred text editor: 6) emacs app/Main.hs The following code creates and multiplies two 4x4 matrices: module Main where import Lib import Data.Matrix main :: IO () m1 = matrix 4 4 $ \\(i,j) -> 2*i - j m2 = matrix 4 4 $ \\(i,j) -> 2*i - j test = multStd m1 m2 main = do putStrLn (show test) Add the dependency on the matrix package to the project: 7) emacs package.yaml 8) Add \"- matrix\" under dependencies in the executables section so it reads: dependencies: - matmul - matrix Build your project: 9) stack build Run your compiled program: (haskell) mfricke@wheeler-sn:~/matmul$ .stack-work/install/x86_64-linux/lts-13.17/8.6.4/bin/matmul-exe \u250c \u2510 \u2502 -18 -16 -14 -12 \u2502 \u2502 14 8 2 -4 \u2502 \u2502 46 32 18 4 \u2502 \u2502 78 56 34 12 \u2502 \u2514 \u2518","title":"Haskell at CARC"},{"location":"install_perl_libraries/","text":"Installing Perl Libraries to your home directory Perl libraries can be installed to your home directory. First, load a perl module. Use module spider perl to view available perl modules on the cluster you are using. In this example we will use a perl module available on the Hopper cluster. $> module load intel/20.0.4 $> module load perl/5.32.0-sw3s Next, start an interactive cpan shell: $> perl -MCPAN -e shell Direct cpan to your home directory: cpan[1]> o conf makepl_arg INSTALL_BASE=~/perl5 Commit the changes to cpan: cpan[1]> o conf commit Exit cpan: cpan[1]> exit Now, you need to modify your environment variables. This can be done in the terminal using the following four commands, but would need to be done once per login session before installing perl libraries. Alternatively, you can add these commands to your .bashrc file to have them happen automatically upon logging in. $> export PERL_MM_OPT=\"INSTALL_BASE=$HOME/perl5\" $> export PERL5LIB=\"$HOME/perl5/lib/perl5:$PERL5LIB\" $> export PATH=\"$HOME/perl5/bin:$PATH\" $> eval $(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib) You can now use the cpan commands to install perl modules/libraries: $> cpan <library> Some examples are: $> cpan YAML $> cpan Math::Utils $> cpan Thread::Queue","title":"Installing Perl Libraries to Your Home Directory"},{"location":"install_perl_libraries/#installing-perl-libraries-to-your-home-directory","text":"Perl libraries can be installed to your home directory. First, load a perl module. Use module spider perl to view available perl modules on the cluster you are using. In this example we will use a perl module available on the Hopper cluster. $> module load intel/20.0.4 $> module load perl/5.32.0-sw3s Next, start an interactive cpan shell: $> perl -MCPAN -e shell Direct cpan to your home directory: cpan[1]> o conf makepl_arg INSTALL_BASE=~/perl5 Commit the changes to cpan: cpan[1]> o conf commit Exit cpan: cpan[1]> exit Now, you need to modify your environment variables. This can be done in the terminal using the following four commands, but would need to be done once per login session before installing perl libraries. Alternatively, you can add these commands to your .bashrc file to have them happen automatically upon logging in. $> export PERL_MM_OPT=\"INSTALL_BASE=$HOME/perl5\" $> export PERL5LIB=\"$HOME/perl5/lib/perl5:$PERL5LIB\" $> export PATH=\"$HOME/perl5/bin:$PATH\" $> eval $(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib) You can now use the cpan commands to install perl modules/libraries: $> cpan <library> Some examples are: $> cpan YAML $> cpan Math::Utils $> cpan Thread::Queue","title":"Installing Perl Libraries to your home directory"},{"location":"intro_conda_usage/","text":"Working with Conda Using Conda to create a new environment Let's start by logging in to Wheeler and create an empty environment. After creating a new environment we will then install Python 2.7 our newly created environment and explore what Conda is doing. To do this we must first load the module for Anaconda, in this case we will be using Anaconda3 which uses Python 3 by default. If you know that you are going to be using a lot of code written in Python 2 then you can load Anaconda instead of Anaconda3. $ module load anaconda3 $ conda create --name py-2.7 Solving environment: done ## Package Plan ## environment location: /users/yourusername/.conda/envs/py-2.7 Proceed ([y]/n)? Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # # $ conda activate MyFirstEnvironment # # To deactivate an active environment, use # # $ conda deactivate We now have a new Conda environment that we can populate with whatever software we require for the analyses we wish to run. Installing packages with Conda Now that we have our empty environment let's install Python 2.7 in it. Make sure you have the Anaconda3 module loaded and then type the following command: $ conda install --name py-2.7 python=2.7 You should see the following print to stdout ## Package Plan ## environment location: /users/yourusername/.conda/envs/py-2.7 added / updated specs: - python=2.7 The following packages will be downloaded: package | build ---------------------------|----------------- pip-10.0.1 | py27_0 1.7 MB certifi-2018.8.24 | py27_1 139 KB python-2.7.15 | h1571d57_0 12.1 MB setuptools-40.2.0 | py27_0 585 KB wheel-0.31.1 | py27_0 62 KB tk-8.6.8 | hbc83047_0 3.1 MB readline-7.0 | h7b6447c_5 392 KB ------------------------------------------------------------ Total: 18.1 MB The following NEW packages will be INSTALLED: ca-certificates: 2018.03.07-0 certifi: 2018.8.24-py27_1 libedit: 3.1.20170329-h6b74fdf_2 libffi: 3.2.1-hd88cf55_4 libgcc-ng: 8.2.0-hdf63c60_1 libstdcxx-ng: 8.2.0-hdf63c60_1 ncurses: 6.1-hf484d3e_0 openssl: 1.0.2p-h14c3975_0 pip: 10.0.1-py27_0 python: 2.7.15-h1571d57_0 readline: 7.0-h7b6447c_5 setuptools: 40.2.0-py27_0 sqlite: 3.24.0-h84994c4_0 tk: 8.6.8-hbc83047_0 wheel: 0.31.1-py27_0 zlib: 1.2.11-ha838bed_2 Proceed ([y]/n)? Downloading and Extracting Packages pip-10.0.1 | 1.7 MB | ####################################### | 100% certifi-2018.8.24 | 139 KB | ####################################### | 100% python-2.7.15 | 12.1 MB | ####################################### | 100% setuptools-40.2.0 | 585 KB | ####################################### | 100% wheel-0.31.1 | 62 KB | ####################################### | 100% tk-8.6.8 | 3.1 MB | ####################################### | 100% readline-7.0 | 392 KB | ####################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done Now our py-2.7 environment actually has something in it that we can use. There are other ways to install packages in an environment, but they all use the conda install command. You can do what we did above and specify which environment you want to install in to, or you can activate the environment first and then install the package, as shown below: $ py-2.7 (py-2.7)$ conda install python=2.7 You will notice that the name of the currently active environment now precedes your command prompt. We can also save time by installing our packages while we create our environment by listing the packages we want installed after the name of the environment we are creating, as shown below: $ conda create --name py-2.7 python=2.7 These all accomplish the same goal of populating an environment with software, but what exactly is a Conda environment and what is it doing? What is a Conda environment? What Conda does when it creates an environment is generate an isolated directory where software packages are installed, then, upon activation of that environment, it prepends our PATH to direct the computer to search in that environment directory first. To help visualize and understand what Conda is doing when it is creating an environment let's run a couple of Bash commands. Run the following commands, which python , python --version , and echo $PATH while you have the anaconda3 module loaded, but no environment activated. You should see the following print to stdout : $ module load anaconda3 $ which python /opt/local/anaconda3/5.2.0/bin/python $ python --version Python 3.6.5 :: Anaconda, Inc. $ echo $PATH /opt/local/anaconda3/5.2.0/bin:/users/yourusername/bin You can see that we are using Python version 3.6.5 distributed by Anaconda which is located in the Anaconda root directory. You can also see that at the beginning of our PATH is that root directory. Now let's activate our py-2.7 environment and see how things change. $ source activate py-2.7 (py-2.7)$ which python ~/.conda/envs/py-2.7/bin/python (py-2.7)$ python --version Python 2.7.15 :: Anaconda, Inc. (py-2.7)$ echo $PATH /users/yourusername/.conda/envs/py-2.7/bin:/opt/local/anaconda3/5.2.0/bin:/users/yourusername/bin As you can see we are now accessing Python version 2.7.15 distributed by Anaconda which is installed in the py-2.7 environment directory located in our home directory. By comparing the PATH before and after activating our environment you can see that Conda prepends our PATH to direct to your environment. This is fundamentally how Conda controls and manages environments. Once you deactivate an environment your PATH variable returns to its previous state. For more information on managing Conda environments please visit the Conda help documentation at this link .","title":"Working with Conda"},{"location":"intro_conda_usage/#working-with-conda","text":"","title":"Working with Conda"},{"location":"intro_conda_usage/#using-conda-to-create-a-new-environment","text":"Let's start by logging in to Wheeler and create an empty environment. After creating a new environment we will then install Python 2.7 our newly created environment and explore what Conda is doing. To do this we must first load the module for Anaconda, in this case we will be using Anaconda3 which uses Python 3 by default. If you know that you are going to be using a lot of code written in Python 2 then you can load Anaconda instead of Anaconda3. $ module load anaconda3 $ conda create --name py-2.7 Solving environment: done ## Package Plan ## environment location: /users/yourusername/.conda/envs/py-2.7 Proceed ([y]/n)? Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # # $ conda activate MyFirstEnvironment # # To deactivate an active environment, use # # $ conda deactivate We now have a new Conda environment that we can populate with whatever software we require for the analyses we wish to run.","title":"Using Conda to create a new environment"},{"location":"intro_conda_usage/#installing-packages-with-conda","text":"Now that we have our empty environment let's install Python 2.7 in it. Make sure you have the Anaconda3 module loaded and then type the following command: $ conda install --name py-2.7 python=2.7 You should see the following print to stdout ## Package Plan ## environment location: /users/yourusername/.conda/envs/py-2.7 added / updated specs: - python=2.7 The following packages will be downloaded: package | build ---------------------------|----------------- pip-10.0.1 | py27_0 1.7 MB certifi-2018.8.24 | py27_1 139 KB python-2.7.15 | h1571d57_0 12.1 MB setuptools-40.2.0 | py27_0 585 KB wheel-0.31.1 | py27_0 62 KB tk-8.6.8 | hbc83047_0 3.1 MB readline-7.0 | h7b6447c_5 392 KB ------------------------------------------------------------ Total: 18.1 MB The following NEW packages will be INSTALLED: ca-certificates: 2018.03.07-0 certifi: 2018.8.24-py27_1 libedit: 3.1.20170329-h6b74fdf_2 libffi: 3.2.1-hd88cf55_4 libgcc-ng: 8.2.0-hdf63c60_1 libstdcxx-ng: 8.2.0-hdf63c60_1 ncurses: 6.1-hf484d3e_0 openssl: 1.0.2p-h14c3975_0 pip: 10.0.1-py27_0 python: 2.7.15-h1571d57_0 readline: 7.0-h7b6447c_5 setuptools: 40.2.0-py27_0 sqlite: 3.24.0-h84994c4_0 tk: 8.6.8-hbc83047_0 wheel: 0.31.1-py27_0 zlib: 1.2.11-ha838bed_2 Proceed ([y]/n)? Downloading and Extracting Packages pip-10.0.1 | 1.7 MB | ####################################### | 100% certifi-2018.8.24 | 139 KB | ####################################### | 100% python-2.7.15 | 12.1 MB | ####################################### | 100% setuptools-40.2.0 | 585 KB | ####################################### | 100% wheel-0.31.1 | 62 KB | ####################################### | 100% tk-8.6.8 | 3.1 MB | ####################################### | 100% readline-7.0 | 392 KB | ####################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done Now our py-2.7 environment actually has something in it that we can use. There are other ways to install packages in an environment, but they all use the conda install command. You can do what we did above and specify which environment you want to install in to, or you can activate the environment first and then install the package, as shown below: $ py-2.7 (py-2.7)$ conda install python=2.7 You will notice that the name of the currently active environment now precedes your command prompt. We can also save time by installing our packages while we create our environment by listing the packages we want installed after the name of the environment we are creating, as shown below: $ conda create --name py-2.7 python=2.7 These all accomplish the same goal of populating an environment with software, but what exactly is a Conda environment and what is it doing?","title":"Installing packages with Conda"},{"location":"intro_conda_usage/#what-is-a-conda-environment","text":"What Conda does when it creates an environment is generate an isolated directory where software packages are installed, then, upon activation of that environment, it prepends our PATH to direct the computer to search in that environment directory first. To help visualize and understand what Conda is doing when it is creating an environment let's run a couple of Bash commands. Run the following commands, which python , python --version , and echo $PATH while you have the anaconda3 module loaded, but no environment activated. You should see the following print to stdout : $ module load anaconda3 $ which python /opt/local/anaconda3/5.2.0/bin/python $ python --version Python 3.6.5 :: Anaconda, Inc. $ echo $PATH /opt/local/anaconda3/5.2.0/bin:/users/yourusername/bin You can see that we are using Python version 3.6.5 distributed by Anaconda which is located in the Anaconda root directory. You can also see that at the beginning of our PATH is that root directory. Now let's activate our py-2.7 environment and see how things change. $ source activate py-2.7 (py-2.7)$ which python ~/.conda/envs/py-2.7/bin/python (py-2.7)$ python --version Python 2.7.15 :: Anaconda, Inc. (py-2.7)$ echo $PATH /users/yourusername/.conda/envs/py-2.7/bin:/opt/local/anaconda3/5.2.0/bin:/users/yourusername/bin As you can see we are now accessing Python version 2.7.15 distributed by Anaconda which is installed in the py-2.7 environment directory located in our home directory. By comparing the PATH before and after activating our environment you can see that Conda prepends our PATH to direct to your environment. This is fundamentally how Conda controls and manages environments. Once you deactivate an environment your PATH variable returns to its previous state. For more information on managing Conda environments please visit the Conda help documentation at this link .","title":"What is a Conda environment?"},{"location":"julia_with_jupyterhub/","text":"Julia with Jupyter Hub It is possible to use a Julia environment in the CARC Jupyter Hub. To get Julia to appear as a Jupyter Notebook option, you must install the IJulia package to your home directory using a terminal. Follow the setup steps below to install the package. Once the package is installed, you will not need to repeat the setup steps. Setup First you must load the julia module. To see a list of available julia versions enter module spider julia . $> module load julia Next, start an interactive Julia session: $> julia Enter two commands into Julia to download the IJulia package and build it: julia> using Pkg julia> Pkg.add(\"IJulia\") julia> Pkg.build(\"IJulia\") Exit Julia: julia> exit() Julia should now appear as an option in JupyterHub. Selecting Julia in JupyterHub Start a JupyterHub job. In the upper right portion of the screen, click on the 'new' drop-down menu. Select the Julia option.","title":"Using Julia in JupyterHub"},{"location":"julia_with_jupyterhub/#julia-with-jupyter-hub","text":"It is possible to use a Julia environment in the CARC Jupyter Hub. To get Julia to appear as a Jupyter Notebook option, you must install the IJulia package to your home directory using a terminal. Follow the setup steps below to install the package. Once the package is installed, you will not need to repeat the setup steps.","title":"Julia with Jupyter Hub"},{"location":"julia_with_jupyterhub/#setup","text":"First you must load the julia module. To see a list of available julia versions enter module spider julia . $> module load julia Next, start an interactive Julia session: $> julia Enter two commands into Julia to download the IJulia package and build it: julia> using Pkg julia> Pkg.add(\"IJulia\") julia> Pkg.build(\"IJulia\") Exit Julia: julia> exit() Julia should now appear as an option in JupyterHub.","title":"Setup"},{"location":"julia_with_jupyterhub/#selecting-julia-in-jupyterhub","text":"Start a JupyterHub job. In the upper right portion of the screen, click on the 'new' drop-down menu. Select the Julia option.","title":"Selecting Julia in JupyterHub"},{"location":"learning_linux/","text":"Linux All CARC systems run Linux as their operating system and are interacted with in a command-line interface (CLI) as opposed to a graphical user interface (GUI) where you can point at and click on things with a mouse.In order to effectively navigate and efficiently utilize CARC systems we recommend you become familiar with basic Linux commands and navigation in a CLI. Although the command line environment can look intimidating at first, you will soon learn that because Linux/Unix were written by scientists for scientists, these tools are extremely powerful and you can become more productive using them than you thought possible. There are many resources freely available for learning this material, and since they have been written better than CARC could hope to do, we will now simply direct you to some of our favorite learning resources. Learning Linux Below are some great resources for learning the basics of operating in a Linux CLI environment: Software Carpentry - This website has lessons that help develop a strong foundation in Linux, especially lesson 1, but the other lessons are helpful in learning some very powerful tools found in Linux. Linux Journey - There is a lot of overlap between Linux Journey and Software Carpentry, but some may find the approach of Linux Journey more accessible. Greg's Bash Wiki - One of the classic resources on utilizing Bash, the default shell for interacting with a Linux system. Learning programming languages Once you are confident operating within Linux you may want to dabble in writing some of your own programs. One of the great things about computers is that we can automate them to perform tedious and menial tasks that would take us hours or days to complete, and a computer can do it almost instantaneously and without error. Below are resources for learning some of the more common programming languages. Python - Python is one of the most popular languages for scientific computing and is a great skill to learn, even if you are just trying to debug someone else's code. Rosalind has some really great lessons focused on solving actual problems in order to learn to program in python. Lessons range in difficulty from very simple to rather advanced. Julia - Julie is another powerful programing language popular for scientific computing. Learn X in Y Minutes Julia by Example Go - Go is a programming language developed by Google. Learn X in Y minutes Go by example In addition to these more modern languages, a decent foundation in the classics like Perl, C, C++, and Fortran can be beneficial to learn as a lot of software, both old and current, are written in these languages, and a basic understanding of how to read the code can help when troubleshooting issues.","title":"Linux"},{"location":"learning_linux/#linux","text":"All CARC systems run Linux as their operating system and are interacted with in a command-line interface (CLI) as opposed to a graphical user interface (GUI) where you can point at and click on things with a mouse.In order to effectively navigate and efficiently utilize CARC systems we recommend you become familiar with basic Linux commands and navigation in a CLI. Although the command line environment can look intimidating at first, you will soon learn that because Linux/Unix were written by scientists for scientists, these tools are extremely powerful and you can become more productive using them than you thought possible. There are many resources freely available for learning this material, and since they have been written better than CARC could hope to do, we will now simply direct you to some of our favorite learning resources.","title":"Linux"},{"location":"learning_linux/#learning-linux","text":"Below are some great resources for learning the basics of operating in a Linux CLI environment: Software Carpentry - This website has lessons that help develop a strong foundation in Linux, especially lesson 1, but the other lessons are helpful in learning some very powerful tools found in Linux. Linux Journey - There is a lot of overlap between Linux Journey and Software Carpentry, but some may find the approach of Linux Journey more accessible. Greg's Bash Wiki - One of the classic resources on utilizing Bash, the default shell for interacting with a Linux system.","title":"Learning Linux"},{"location":"learning_linux/#learning-programming-languages","text":"Once you are confident operating within Linux you may want to dabble in writing some of your own programs. One of the great things about computers is that we can automate them to perform tedious and menial tasks that would take us hours or days to complete, and a computer can do it almost instantaneously and without error. Below are resources for learning some of the more common programming languages. Python - Python is one of the most popular languages for scientific computing and is a great skill to learn, even if you are just trying to debug someone else's code. Rosalind has some really great lessons focused on solving actual problems in order to learn to program in python. Lessons range in difficulty from very simple to rather advanced. Julia - Julie is another powerful programing language popular for scientific computing. Learn X in Y Minutes Julia by Example Go - Go is a programming language developed by Google. Learn X in Y minutes Go by example In addition to these more modern languages, a decent foundation in the classics like Perl, C, C++, and Fortran can be beneficial to learn as a lot of software, both old and current, are written in these languages, and a basic understanding of how to read the code can help when troubleshooting issues.","title":"Learning programming languages"},{"location":"linux_intro/","text":"Linux Tutorial The goal of this tutorial is to familiarize you with the minimal set of commands needed to effectively negotiate the Unix operating system and file management system in an X11 windowing environment such as cygwin. At the end of this tutorial you will find a list of more advanced tools that you may wish to explore as you become familiar with Unix and its various flavors. These include the text editors vi and emacs, plotting packages (xmgr, gnuplot, xgrace), and image/file display packages (ghostscript, for postscript; acroread, for PDF files; and xv, for displaying .gif, .jpeg, and other image formats.) In this tutorial, we use the generic term Unix to refer to any operating system (OS) based on the original Unix OS developed in the 1970s (1969 is typically cited as the year that Unix was \u201cborn.\u201d) Although numerous flavors and variants of the OS have appeared in the intervening years, they are all based on the same underlying design, and preserve a core set of commands in common. What this means is that if you are familiar with any contemporary Unix-like system, you will be able to immediately work on any other Unix-like system, and it will be very easy for you to come up to speed on the idiosyncrasies of the new environment. Examples of Unix-based systems that you may have heard of include Linux, SGI IRIX, Mac OS X, Sun Solaris, and SunOS. This tutorial provides step-by-step instructions for exploring the Unix-like operating systems running on UNM IT and CARC machines, such as linux.unm.edu and wheeler.alliance.unm.edu . Commands that you should enter at the prompt (also file, directory, and application names) will appear as a code block, while names of files will appear in bold . You may wish to take a look at the IT documentation for Linux and X Windows available at http://it.unm.edu/quickrefs/unix.html . Numerous \u201ccheat sheets\u201d for Linux and for editors such as vi (see below) can also be found online. NOTE: The program that interprets your commands on a Unix-based computer is called the shell. Shells come in different varieties, such as csh (C shell), tsch (Tenex C shell), ksh (Korn shell), and bash (Bourne-Again Shell). Many of the basic commands are the same across shells, but there are also some important differences. Begin by ssh-ing into wheeler.alliance.unm.edu from an XTerm. Type in your username, followed by your wheeler password. (Note: you can also ssh into IT Linux machines by connecting to linux.unm.edu and using your UNM IT password (different from your CARC wheeler password). Note that when you login to a Unix machine, there is always a prompt. The default prompt is $ , but sometimes it will appear as the machine name, followed by a number which tells you how many commands you have entered up to that point in the session, e.g. username@wheeler-sn:~$ , username@xena:~$ or username@gibbs:~$ . All commands in Unix are issued sequentially, each at a new prompt (there are ways to combine commands, but we won\u2019t worry about this for now). If your password is new, you should change it immediately to one that is known only to you. If you have not done so already, type passwd at the Unix prompt, and follow the instructions. You can change your password as often as you like in this fashion. Verify your username by typing whoami . This command is useful in computer labs if you come across an unoccupied terminal that appears to be in use, and would like to know who is logged in. Also, it gives you an idea of the current user load on the machine. See who else is using the machine you are on by typing who . Reality check: type date . Check that you are in your home directory by typing pwd (print working directory). Create a new directory called Lab1 by using the mkdir command: mkdir Lab1 . Type cd Lab1 (change directory) to move to this directory. Create two subdirectories, take1 and take2, and change directories to take1. (NOTE: Any files or directories that you create remain on the Linux filesystem until you specifically choose to delete them. For example, if you login to linux.unm.edu from a second XTerm while still connected via the first, you will be able to see (and manipulate) all of your directories and files stored on the filesystem.) Brief text editor review. Two editors that are commonly used in the Unix environment are vi and emacs . emacs is more sophisticated, but requires more effort to master. If you want to try it out, type emacs at the Unix prompt, and follow the instructions to work through the tutorial. For right now, we will use vi . (Note: there are a number of useful online quick-references for vi. See, for example: http://it.unm.edu/quickrefs/vi.html ). To invoke the editor, type vi <filename> . (The notation \u201c \u201d just means \u201cany filename you like.\u201d For example, you might type vi genomes.txt ). Note that in Unix, long filenames are deprecated; it is customary to use the suffix .txt to denote plain text files, and filenames may not include spaces. In lieu of spaces, it is common to use the underscore character \u201c_\u201d, or strategically placed capitals, thus: MyLongFilename.txt . Once you are in the editor, you can use the following commands indicated in boldface to differentiate commands within the editor versus those at the command prompt: i to insert text at cursor location. type i and then type your text; hit (the \u201cEsc\u201d key on your keyboard) to stop entering text a to add text at location of cursor ( to end text entry) o to open a new line and begin inserting text ( to end text entry). If you are not sure what state vi is in, it\u2019s always safe to type to reset. j to move down a line k to move up a line D to delete from cursor location to end of line dd to delete entire line : to get into editor mode. From here you can type w (followed by the Enter key) to write your file (this will overwrite whatever file you said you were editing when you invoked vi ), q to quit vi ; and q! to exit without writing over your file. Another very useful editor-mode command is to type u at the colon; this undoes your last command in case you made a mess of your editing. Type G in text-entry mode to go to line number . $ refers to the last line in your file, so $G will take you to the last line in your Also in text entry mode, type -G (this is the Ctrl key on your keyboard, held down at the same time as the G key) to find out what line you are at. Now that you are an expert at editing, create a new file called junk.txt containing the single line \"Hello World.\" You may use vi or the Unix editor of your choice in order to do this. Copy junk.txt to junk2.txt using the cp command: cp junk.txt junk2.txt . Edit junk2.txt and add the line: \"Now is the time for all good men to come to the aid of their PARTY.\" (Yes, PARTY should be capitalized). Save the file and exit to the Unix prompt by typing : wq . Check which file contains the word party by doing a case-insensitive grep ; type grep -i party *.txt . The asterisk is a wild-card character that matches both junk and junk2 . Do a pwd ; then get a listing of the files in the current directory by typing ls . where \u2018.\u2019 is a shorthand for \u201cthe current directory.\u201d See how much space is left on the disk where your directory resides by typing df . Type man ls to access the Unix on-line manual page for the ls command. Find the option that will produce a long listing, and re-type the ls command using this option. One of the most important features of Unix is the ability to re-direct output. You can re-direct the output of your ls command to a file named list.txt by typing > list.txt on the same line as the ls command, and immediately afterwards. Try this now. Make sure that your re-direction worked by typing cat list.txt to look at the contents of list.txt . Another useful tool is the diff command, which allows you to compare the contents of two files, line by line. Do a diff junk.txt junk2.txt . The >> symbol allows you to append output to the end of a file. Repeat the diff command, followed by >> list.txt to append the output of diff to the list.txt file. Use cat list.txt to verify that the results are what you expected. Move junk2.txt one directory up by typing mv junk2.txt .. where \"..\u2019 is shorthand for \u201cone directory up from where I am now.\u201d Jump to your home directory by typing cd without any arguments. Go back to directory take1 by typing cd Lab1/take1 . Now go back to your home directory by typing cd ../.. Use the man page for ls again to find out how to display hidden (\u2018dot\u2019) files, and do a long listing, including dotfiles, in your home directory. Change to directory Lab1 , and remove directory take2 by typing by rmdir take2 . Now try to remove directory take1 in the same way. The reason take1 could not be removed is that it still has a file in it. Change directories to take1 and do a rm junk.txt . Notice the different command for removing a file vs. removing a directory. Go back up one directory and remove directory take1 . Change to your home directory and remove directory Lab1 . To logout, type exit at the prompt. You can kill any jobs executing before you logout, or they will get killed automatically when you logout. Note that Unix is a much more robust OS than Windows. There is virtually no way that you, as an ordinary user, can crash the operating system while others are using it. This is one of the many reasons why you will want to learn to use (and master) the Unix operating system environment, and why it is used on supercomputers and clusters at centers around the world. Congratulations! You\u2019ve survived the Unix tutorial. Here is a list of some particularly useful X windows tools included on many (not necessarily all, however) Unix installations. These are listed according to the Unix command used to invoke them. At your leisure, please take the time to explore some of them, and see that the appropriate GUI (Graphical User Interface) comes up correctly when they are invoked. Many of these applications are self-explanatory; simply start them up and try them out. xmgr : plotting program gnuplot plotting program xwd : X window dump (man xwd). xv Useful visualizer that can handle a wide range of graphics formats, and convert between them (.rgb, .xwd, .tiff, etc.). xgrace : more advanced (sophisticated) plotting program ghostview : postscript previewer. acroread : Acrobat (PDF) previewer. LaTeX: : Scientific text formatting language, including previewing package. Far superior to Word for heavily mathematical documents. A LaTeX tutorial/reference book such as the one by Leslie Lamport is useful for getting started. vi : Standard Unix line-editor. An introduction to vi was included as part of this Lab; see online reference there. emacs, xemacs : A highly flexible editing environment, with special editing modes for particular file types (.F Fortran source code, .tex LaTeX files, etc.), facilities for multiple buffers, and much, much more. See Learning GNU Emacs (O\u2019Reilly and Associates) or the emacs online help for a detailed tutorial.","title":"Linux Introduction"},{"location":"linux_intro/#linux-tutorial","text":"The goal of this tutorial is to familiarize you with the minimal set of commands needed to effectively negotiate the Unix operating system and file management system in an X11 windowing environment such as cygwin. At the end of this tutorial you will find a list of more advanced tools that you may wish to explore as you become familiar with Unix and its various flavors. These include the text editors vi and emacs, plotting packages (xmgr, gnuplot, xgrace), and image/file display packages (ghostscript, for postscript; acroread, for PDF files; and xv, for displaying .gif, .jpeg, and other image formats.) In this tutorial, we use the generic term Unix to refer to any operating system (OS) based on the original Unix OS developed in the 1970s (1969 is typically cited as the year that Unix was \u201cborn.\u201d) Although numerous flavors and variants of the OS have appeared in the intervening years, they are all based on the same underlying design, and preserve a core set of commands in common. What this means is that if you are familiar with any contemporary Unix-like system, you will be able to immediately work on any other Unix-like system, and it will be very easy for you to come up to speed on the idiosyncrasies of the new environment. Examples of Unix-based systems that you may have heard of include Linux, SGI IRIX, Mac OS X, Sun Solaris, and SunOS. This tutorial provides step-by-step instructions for exploring the Unix-like operating systems running on UNM IT and CARC machines, such as linux.unm.edu and wheeler.alliance.unm.edu . Commands that you should enter at the prompt (also file, directory, and application names) will appear as a code block, while names of files will appear in bold . You may wish to take a look at the IT documentation for Linux and X Windows available at http://it.unm.edu/quickrefs/unix.html . Numerous \u201ccheat sheets\u201d for Linux and for editors such as vi (see below) can also be found online. NOTE: The program that interprets your commands on a Unix-based computer is called the shell. Shells come in different varieties, such as csh (C shell), tsch (Tenex C shell), ksh (Korn shell), and bash (Bourne-Again Shell). Many of the basic commands are the same across shells, but there are also some important differences. Begin by ssh-ing into wheeler.alliance.unm.edu from an XTerm. Type in your username, followed by your wheeler password. (Note: you can also ssh into IT Linux machines by connecting to linux.unm.edu and using your UNM IT password (different from your CARC wheeler password). Note that when you login to a Unix machine, there is always a prompt. The default prompt is $ , but sometimes it will appear as the machine name, followed by a number which tells you how many commands you have entered up to that point in the session, e.g. username@wheeler-sn:~$ , username@xena:~$ or username@gibbs:~$ . All commands in Unix are issued sequentially, each at a new prompt (there are ways to combine commands, but we won\u2019t worry about this for now). If your password is new, you should change it immediately to one that is known only to you. If you have not done so already, type passwd at the Unix prompt, and follow the instructions. You can change your password as often as you like in this fashion. Verify your username by typing whoami . This command is useful in computer labs if you come across an unoccupied terminal that appears to be in use, and would like to know who is logged in. Also, it gives you an idea of the current user load on the machine. See who else is using the machine you are on by typing who . Reality check: type date . Check that you are in your home directory by typing pwd (print working directory). Create a new directory called Lab1 by using the mkdir command: mkdir Lab1 . Type cd Lab1 (change directory) to move to this directory. Create two subdirectories, take1 and take2, and change directories to take1. (NOTE: Any files or directories that you create remain on the Linux filesystem until you specifically choose to delete them. For example, if you login to linux.unm.edu from a second XTerm while still connected via the first, you will be able to see (and manipulate) all of your directories and files stored on the filesystem.) Brief text editor review. Two editors that are commonly used in the Unix environment are vi and emacs . emacs is more sophisticated, but requires more effort to master. If you want to try it out, type emacs at the Unix prompt, and follow the instructions to work through the tutorial. For right now, we will use vi . (Note: there are a number of useful online quick-references for vi. See, for example: http://it.unm.edu/quickrefs/vi.html ). To invoke the editor, type vi <filename> . (The notation \u201c \u201d just means \u201cany filename you like.\u201d For example, you might type vi genomes.txt ). Note that in Unix, long filenames are deprecated; it is customary to use the suffix .txt to denote plain text files, and filenames may not include spaces. In lieu of spaces, it is common to use the underscore character \u201c_\u201d, or strategically placed capitals, thus: MyLongFilename.txt . Once you are in the editor, you can use the following commands indicated in boldface to differentiate commands within the editor versus those at the command prompt: i to insert text at cursor location. type i and then type your text; hit (the \u201cEsc\u201d key on your keyboard) to stop entering text a to add text at location of cursor ( to end text entry) o to open a new line and begin inserting text ( to end text entry). If you are not sure what state vi is in, it\u2019s always safe to type to reset. j to move down a line k to move up a line D to delete from cursor location to end of line dd to delete entire line : to get into editor mode. From here you can type w (followed by the Enter key) to write your file (this will overwrite whatever file you said you were editing when you invoked vi ), q to quit vi ; and q! to exit without writing over your file. Another very useful editor-mode command is to type u at the colon; this undoes your last command in case you made a mess of your editing. Type G in text-entry mode to go to line number . $ refers to the last line in your file, so $G will take you to the last line in your Also in text entry mode, type -G (this is the Ctrl key on your keyboard, held down at the same time as the G key) to find out what line you are at. Now that you are an expert at editing, create a new file called junk.txt containing the single line \"Hello World.\" You may use vi or the Unix editor of your choice in order to do this. Copy junk.txt to junk2.txt using the cp command: cp junk.txt junk2.txt . Edit junk2.txt and add the line: \"Now is the time for all good men to come to the aid of their PARTY.\" (Yes, PARTY should be capitalized). Save the file and exit to the Unix prompt by typing : wq . Check which file contains the word party by doing a case-insensitive grep ; type grep -i party *.txt . The asterisk is a wild-card character that matches both junk and junk2 . Do a pwd ; then get a listing of the files in the current directory by typing ls . where \u2018.\u2019 is a shorthand for \u201cthe current directory.\u201d See how much space is left on the disk where your directory resides by typing df . Type man ls to access the Unix on-line manual page for the ls command. Find the option that will produce a long listing, and re-type the ls command using this option. One of the most important features of Unix is the ability to re-direct output. You can re-direct the output of your ls command to a file named list.txt by typing > list.txt on the same line as the ls command, and immediately afterwards. Try this now. Make sure that your re-direction worked by typing cat list.txt to look at the contents of list.txt . Another useful tool is the diff command, which allows you to compare the contents of two files, line by line. Do a diff junk.txt junk2.txt . The >> symbol allows you to append output to the end of a file. Repeat the diff command, followed by >> list.txt to append the output of diff to the list.txt file. Use cat list.txt to verify that the results are what you expected. Move junk2.txt one directory up by typing mv junk2.txt .. where \"..\u2019 is shorthand for \u201cone directory up from where I am now.\u201d Jump to your home directory by typing cd without any arguments. Go back to directory take1 by typing cd Lab1/take1 . Now go back to your home directory by typing cd ../.. Use the man page for ls again to find out how to display hidden (\u2018dot\u2019) files, and do a long listing, including dotfiles, in your home directory. Change to directory Lab1 , and remove directory take2 by typing by rmdir take2 . Now try to remove directory take1 in the same way. The reason take1 could not be removed is that it still has a file in it. Change directories to take1 and do a rm junk.txt . Notice the different command for removing a file vs. removing a directory. Go back up one directory and remove directory take1 . Change to your home directory and remove directory Lab1 . To logout, type exit at the prompt. You can kill any jobs executing before you logout, or they will get killed automatically when you logout. Note that Unix is a much more robust OS than Windows. There is virtually no way that you, as an ordinary user, can crash the operating system while others are using it. This is one of the many reasons why you will want to learn to use (and master) the Unix operating system environment, and why it is used on supercomputers and clusters at centers around the world. Congratulations! You\u2019ve survived the Unix tutorial. Here is a list of some particularly useful X windows tools included on many (not necessarily all, however) Unix installations. These are listed according to the Unix command used to invoke them. At your leisure, please take the time to explore some of them, and see that the appropriate GUI (Graphical User Interface) comes up correctly when they are invoked. Many of these applications are self-explanatory; simply start them up and try them out. xmgr : plotting program gnuplot plotting program xwd : X window dump (man xwd). xv Useful visualizer that can handle a wide range of graphics formats, and convert between them (.rgb, .xwd, .tiff, etc.). xgrace : more advanced (sophisticated) plotting program ghostview : postscript previewer. acroread : Acrobat (PDF) previewer. LaTeX: : Scientific text formatting language, including previewing package. Far superior to Word for heavily mathematical documents. A LaTeX tutorial/reference book such as the one by Leslie Lamport is useful for getting started. vi : Standard Unix line-editor. An introduction to vi was included as part of this Lab; see online reference there. emacs, xemacs : A highly flexible editing environment, with special editing modes for particular file types (.F Fortran source code, .tex LaTeX files, etc.), facilities for multiple buffers, and much, much more. See Learning GNU Emacs (O\u2019Reilly and Associates) or the emacs online help for a detailed tutorial.","title":"Linux Tutorial"},{"location":"logging_in/","text":"Logging in To log in to the CARC systems and start computing you will need a terminal that can log in to remote systems using Secure Shell (SSH). If you are using a Linux or Mac machine you are in luck as they come bundled with a terminal that is ready to use. The terminal application packaged with your OS is great, but if you would like something with a bit more customization, utility, and flexibility there are other free options available. Below are a couple of options that CARC recommends. Note that iterm2 is only available on Mac, and MobaXterm is only available on Windows. iTerm2 MobaXterm If you are on windows, you can start with using Powershell, however it is recommended to install MobaXterm for a better overall experience. Now that you have your terminal open you can log in. To do this type the following from the terminal prompt: ssh <CARC-USERNAME>@<MACHINE-NAME>.alliance.unm.edu Where CARC-USERNAME is the username you were assigned once your account was approved. MACHINE-NAME will be one of our carc systems; hopper , wheeler , or xena , for example. If this is your first time logging in, you will get a prompt asking you to accept your computer as a new authorized host. You can accept this first prompt, and you will then be prompted for your password. If you are unsure of your current password, please reference the password reset quickbyte. This quickbyte was validated on 5/21/2024","title":"Logging in"},{"location":"logging_in/#logging-in","text":"To log in to the CARC systems and start computing you will need a terminal that can log in to remote systems using Secure Shell (SSH). If you are using a Linux or Mac machine you are in luck as they come bundled with a terminal that is ready to use. The terminal application packaged with your OS is great, but if you would like something with a bit more customization, utility, and flexibility there are other free options available. Below are a couple of options that CARC recommends. Note that iterm2 is only available on Mac, and MobaXterm is only available on Windows. iTerm2 MobaXterm If you are on windows, you can start with using Powershell, however it is recommended to install MobaXterm for a better overall experience. Now that you have your terminal open you can log in. To do this type the following from the terminal prompt: ssh <CARC-USERNAME>@<MACHINE-NAME>.alliance.unm.edu Where CARC-USERNAME is the username you were assigned once your account was approved. MACHINE-NAME will be one of our carc systems; hopper , wheeler , or xena , for example. If this is your first time logging in, you will get a prompt asking you to accept your computer as a new authorized host. You can accept this first prompt, and you will then be prompted for your password. If you are unsure of your current password, please reference the password reset quickbyte. This quickbyte was validated on 5/21/2024","title":"Logging in"},{"location":"module_management/","text":"Managing software models Modules There are many software packages installed on CARC systems, as well as standard built-in functions native to Unix. In order to manage these additional software packages, the CARC systems use modules. These modules set the appropriate environment variables and dependencies for software optimization and to avoid conflicts with other software. For more information, visit this page , or use the command module man . Using modules for setting application environments Modules are used to set environment variables and dependencies for the purpose of managing access to applications and libraries on CARC systems. The command module avail lists all the modules available on the system you are logged in to. To load a module, use the module load command. For example, to load the module for the Intel compiler, use the command: module load intel Another useful command related to module management is module spider . For example, if we issue the command module spider intel you will see the output: ---------------------------------------------------------------------------------------------------------------------------------------------------------- intel: ---------------------------------------------------------------------------------------------------------------------------------------------------------- Description: Intel Compiler Family (C/C++/Fortran for x86_64) Versions: intel/17.0.3.191 intel/17.0.4.196 intel/17.0.5.239 intel/18.0.0.128 intel/18.0.1.163 intel/18.0.2.199 Other possible modules matches: abinit-8.2.2-intel-17.0.4-impi-mkl-xz32k53 abinit-8.2.2-intel-18.0.0-impi-mkl-lck65q7 abyss-2.0.2-intel-17.0.4-openmpi-pf2axsd abyss-2.0.2-intel-17.0.4-pf2axsd ... ---------------------------------------------------------------------------------------------------------------------------------------------------------- To find other possible module matches execute: $ module -r spider '.*intel.*' ---------------------------------------------------------------------------------------------------------------------------------------------------------- For detailed information about a specific \"intel\" module (including how to load the modules) use the module's full name. For example: $ module spider intel/18.0.2.199 ---------------------------------------------------------------------------------------------------------------------------------------------------------- This command returns much more detailed information on a module of interest. You can see that there are actually multiple versions of the Intel compilers available for use, as is the case for most software installed on CARC systems. To see all currently loaded modules use the command module list . As an example, lets load the software module for the genome assembler Abyss, and then use module list : module load abyss-2.0.2-intel-18.0.2-openmpi-mag4oti module list Currently Loaded Modules: 1) ncurses-6.0-intel-18.0.2-crfixrx 5) libpciaccess-0.13.5-intel-18.0.2-etjmw6m 9) sparsehash-2.0.3-intel-18.0.2-wkrpmec 2) readline-7.0-intel-18.0.2-v73wsy6 6) hwloc-1.11.8-intel-18.0.2-fjspqwm 10) bzip2-1.0.6-intel-18.0.2-fsqwhjw 3) sqlite-3.21.0-intel-18.0.2-c66lylp 7) openmpi-3.0.0-intel-18.0.2-7ejspct 11) boost-1.66.0-intel-18.0.2-eoio7oh 4) libxml2-2.9.4-intel-18.0.2-gyxifwh 8) libtool-2.4.6-intel-18.0.2-h4zy3we 12) abyss-2.0.2-intel-18.0.2-openmpi-mag4oti As you can see there are many more modules loaded than just Abyss. These are the libraries and applications that are dependencies of Abyss. Usually, modules are loaded as part of PBS script and subsequently unloaded automatically after the completion of that Job, so module avail and module load are the main commands you will be using. However, if you are working on a node interactively you may need to unload modules manually. The command module unload modulename will unload modules one at a time, for example module unload ncurses-6.0-intel-18.0.2-crfixrx only unloads ncurses but leaves the rest of the modules still loaded. To unload all modules use the command module purge .","title":"Managing modules"},{"location":"module_management/#managing-software-models","text":"","title":"Managing software models"},{"location":"module_management/#modules","text":"There are many software packages installed on CARC systems, as well as standard built-in functions native to Unix. In order to manage these additional software packages, the CARC systems use modules. These modules set the appropriate environment variables and dependencies for software optimization and to avoid conflicts with other software. For more information, visit this page , or use the command module man .","title":"Modules"},{"location":"module_management/#using-modules-for-setting-application-environments","text":"Modules are used to set environment variables and dependencies for the purpose of managing access to applications and libraries on CARC systems. The command module avail lists all the modules available on the system you are logged in to. To load a module, use the module load command. For example, to load the module for the Intel compiler, use the command: module load intel Another useful command related to module management is module spider . For example, if we issue the command module spider intel you will see the output: ---------------------------------------------------------------------------------------------------------------------------------------------------------- intel: ---------------------------------------------------------------------------------------------------------------------------------------------------------- Description: Intel Compiler Family (C/C++/Fortran for x86_64) Versions: intel/17.0.3.191 intel/17.0.4.196 intel/17.0.5.239 intel/18.0.0.128 intel/18.0.1.163 intel/18.0.2.199 Other possible modules matches: abinit-8.2.2-intel-17.0.4-impi-mkl-xz32k53 abinit-8.2.2-intel-18.0.0-impi-mkl-lck65q7 abyss-2.0.2-intel-17.0.4-openmpi-pf2axsd abyss-2.0.2-intel-17.0.4-pf2axsd ... ---------------------------------------------------------------------------------------------------------------------------------------------------------- To find other possible module matches execute: $ module -r spider '.*intel.*' ---------------------------------------------------------------------------------------------------------------------------------------------------------- For detailed information about a specific \"intel\" module (including how to load the modules) use the module's full name. For example: $ module spider intel/18.0.2.199 ---------------------------------------------------------------------------------------------------------------------------------------------------------- This command returns much more detailed information on a module of interest. You can see that there are actually multiple versions of the Intel compilers available for use, as is the case for most software installed on CARC systems. To see all currently loaded modules use the command module list . As an example, lets load the software module for the genome assembler Abyss, and then use module list : module load abyss-2.0.2-intel-18.0.2-openmpi-mag4oti module list Currently Loaded Modules: 1) ncurses-6.0-intel-18.0.2-crfixrx 5) libpciaccess-0.13.5-intel-18.0.2-etjmw6m 9) sparsehash-2.0.3-intel-18.0.2-wkrpmec 2) readline-7.0-intel-18.0.2-v73wsy6 6) hwloc-1.11.8-intel-18.0.2-fjspqwm 10) bzip2-1.0.6-intel-18.0.2-fsqwhjw 3) sqlite-3.21.0-intel-18.0.2-c66lylp 7) openmpi-3.0.0-intel-18.0.2-7ejspct 11) boost-1.66.0-intel-18.0.2-eoio7oh 4) libxml2-2.9.4-intel-18.0.2-gyxifwh 8) libtool-2.4.6-intel-18.0.2-h4zy3we 12) abyss-2.0.2-intel-18.0.2-openmpi-mag4oti As you can see there are many more modules loaded than just Abyss. These are the libraries and applications that are dependencies of Abyss. Usually, modules are loaded as part of PBS script and subsequently unloaded automatically after the completion of that Job, so module avail and module load are the main commands you will be using. However, if you are working on a node interactively you may need to unload modules manually. The command module unload modulename will unload modules one at a time, for example module unload ncurses-6.0-intel-18.0.2-crfixrx only unloads ncurses but leaves the rest of the modules still loaded. To unload all modules use the command module purge .","title":"Using modules for setting application environments"},{"location":"mpiCASA/","text":"Using CASA on CARC A Bit About CASA CASA is the premier software for reducing radio data coming off of a variety of telescopes around the world, including the Jansky Very Large Array (VLA) and Atacama Large Millimeter Array (ALMA). Getting Some Data to Play With Going to use the new Archive . Find something small and have them download it manually, like a few GB at most. Getting Set Up First off get some nodes srun --partition singleGPU --nodes 2 --tasks-per-node 2 --pty bash Do we want singleGPU? Can request more nodes obviously. tasks-per-node we don't need to set unless CASA demands slots Might want to create aliases for casa and mpi casa, just to make things quick alias casa='/users/sbruzew/xena-scratch/casa-blah-blah/bin/casa' alias mpicasa='/users/sbruzew/xena-scratch/casa-blah-blah/bin/mpicasa' Actually it doesn't like the alias when you run the command 2) It will create a nodefile for us at $PBS_NODEFILE If CASA doesn't need slots, we can use this, and all the nodes If CASA does need slots, we'll need to make a script that can reference $PBS_NUM_PPN and add slots 3) Run something like 'path_to_casa_bin/mpicasa -hostfile $PBS_NODEFILE path_to_casa_bin/casa ' Probably want to do --nogui and --log2term Puts us into a CASA environment 4) Run python script that does all the fun stuff Could run mpicasa call with '-c myscript.py' Can also do 'exec(open('./filename').read())' Shortcut as execfile 'filename.py'","title":"CASA Radio Astronomy"},{"location":"mpiCASA/#using-casa-on-carc","text":"","title":"Using CASA on CARC"},{"location":"mpiCASA/#a-bit-about-casa","text":"CASA is the premier software for reducing radio data coming off of a variety of telescopes around the world, including the Jansky Very Large Array (VLA) and Atacama Large Millimeter Array (ALMA).","title":"A Bit About CASA"},{"location":"mpiCASA/#getting-some-data-to-play-with","text":"Going to use the new Archive . Find something small and have them download it manually, like a few GB at most.","title":"Getting Some Data to Play With"},{"location":"mpiCASA/#getting-set-up","text":"First off get some nodes srun --partition singleGPU --nodes 2 --tasks-per-node 2 --pty bash Do we want singleGPU? Can request more nodes obviously. tasks-per-node we don't need to set unless CASA demands slots Might want to create aliases for casa and mpi casa, just to make things quick alias casa='/users/sbruzew/xena-scratch/casa-blah-blah/bin/casa' alias mpicasa='/users/sbruzew/xena-scratch/casa-blah-blah/bin/mpicasa' Actually it doesn't like the alias when you run the command 2) It will create a nodefile for us at $PBS_NODEFILE If CASA doesn't need slots, we can use this, and all the nodes If CASA does need slots, we'll need to make a script that can reference $PBS_NUM_PPN and add slots 3) Run something like 'path_to_casa_bin/mpicasa -hostfile $PBS_NODEFILE path_to_casa_bin/casa ' Probably want to do --nogui and --log2term Puts us into a CASA environment 4) Run python script that does all the fun stuff Could run mpicasa call with '-c myscript.py' Can also do 'exec(open('./filename').read())' Shortcut as execfile 'filename.py'","title":"Getting Set Up"},{"location":"msprime_quickbyte/","text":"Running Population Genetic Simulations with MSPrime Many microevolutionary questions invoke population genetic processes to explain them, from population genetic summary statistics to selection on specific loci of interest. In turn, population genetic simulations are becoming increasingly important to publish in major journals to validate that empirical result are consistent with the processes authors say the findings represent. There are many programs used for these simulations, that fall into two general categories: forward (slow, simulate all individuals in the populations) and backward (fast, work from a number of samples backward to simulate genealogies). Prominent examples of these two are SLiM and msprime respectively. Here, we'll give a basic introduction to msprime, give a simple example of two diverging populations, and show how to parallelize replicates on CARC. The example is one I used to establish how F ST , an estimate of population divergence , changes over time based on population size. In particular, I used this to test if observed values of F ST between islands connected during the Pleistocene became panmictic at those times, or if there was still reduced gene flow between given islands (with the system being one worked on in the past in papers like Smith & Filardi 2007 ). This QuickByte describes msprime 1.0, which is a major update from the widely used earlier versions. How msprime Works The paper describing msprime has been cited hundreds of time, but represents an extension of the simulation program ms , which has been cited a couple thousand times. The goal of this extension is to scale simulations up to the large sample sizes of individuals and loci used in modern day genomic studies. Msprime's fast speed and ease of analysis is achieved by by adding novel ways of keeping track of geneologies being analyzed through sparse trees and coalescence records, collectively refer to ass tree sequences. How to run msprime Msprime is a python package, with reccomended download via conda. We reccomend you use miniconda for this, as we've had some bugs with installing this through anaconda in the past. We'll make an environment with it and a couple other important modules: conda create -n msp1-env -c conda-forge msprime scikit-allel numpy Then we'll activate the environment with \"conda activate\", but note that it needs to be \"source activate\" in a script: conda activate msp1-env In a script or in python command line, we'll import msprime as msp for convenience: import msprime as msp The simulations themselves, at a base level, are very simple. All that's needed is a sample size of individuals \"tracked\" backwards. Note that these are assumed diploid unless ploidy is specified: trees = msp.sim_ancestry(samples=10) Mutations can be added using sim_mutations, ideally changing the default mutation rate (here to the per-year songbird mutation rate): mutations = msp.sim_mutations(trees, rate=2.3e-9) This is just one locus, but we gain a lot more information by adding sequence length and recombination rates. trees = msp.sim_ancestry(samples=10, recombination_rate=1e-8, sequence_length=1e6) Unlike the older versions of msprime, outside of pre-made demographic scenarios, you need to add things like populations and merge events to a demography object. Migration rates are also set there. First, we'll initialize the demography and add two populations: demo = msp.Demography() demo.add_population(name=\"pop1\", initial_size=10000) demo.add_population(name=\"pop2\", initial_size=20000) Next, we'll set up an ancestral population and add a population split time (the size is equal to the size before a bottleneck we implement below): demo.add_population(name=\"anc_pop12\", initial_size=50000) demo.add_population_split(time=5700, derived=[\"pop1\",\"pop2\"], ancestral=\"anc_pop12\") Finally, we'll add symmetric migration (i.e. gene flow) between the two focal populations: demo.set_symmetric_migration_rate(['pop1', 'pop2'], 0.001) We can have the migration rate change over time too, here we increase the migration rate during the period between the split time (5700 generations ago) and 4000 generations go: demo.add_symmetric_migration_rate_change(4000, ['pop1', 'pop2'], 0.01) The final common demographic event is population size change, which we'll set to have a bottleneck three quarters of the way through the simulation. That is, we set the population size of population 1 to be three times the present population 2500 years: demo.add_population_parameters_change(time=2500, initial_size=30000, population=\"pop1\") Finally, we need to sort these events, as we added them out of order for instructional purposes: demo.sort_events() A really helpful debugging feature is the print function for demography objects, it lists and explains the events and migration matrix! print(demo) Bringing this all together, our simulation will look like: trees = msp.sim_ancestry(samples={\"pop1\":10, \"pop2\":10}, demography=demo, recombination_rate=1e-8, sequence_length=1e7) And we'll add mutations like this, using a yearly rate of 2.3e-9 and a generation time of 2.55 years (same as the case study): mutations = msp.sim_mutations(trees, rate=5.9e-9) Running MSPrime replicates on CARC It is best practice to run many replicates of any simulation you run to assess the robustness of any estimates you make. You can use GNU Parallel (specifically env_parallel) to run these simulations, and can add these replicates directly to an output file. The following examples your python simulation script takes population size (popsize) and population divergence time (divtime) and have an output file like \"$popsize_$divtime.out\" that the script writes to. First we'll run 30 replicates. Note that the \"echo {}\" just to deal with the parallel iterator, so GNU parallel doesn't append it to the end of our python call by default. env_parallel --sshloginfile $PBS_NODEFILE \\ 'echo {}; /path/to/python msprime_script.py --popsize 2000 --divtime 10000 \\ --output ./outputs/1000_10000.out' ::: {1..30} You could also use GNU parallel to iterate over multiple parameter combinations, here we test multiple population sizes (2000, 3000, and 5000) and divergence times (1000, 5000, and 10000 generations). We'll assume the script has replicates coded into it. env_parallel --sshloginfile $PBS_NODEFILE \\ '/path/to/python msprime_script.py --popsize {1} --divtime {2} \\ --output ./outputs/{1}_{2}' ::: 2000 3000 5000 ::: 1000 5000 10000 Case study: Divergence between connected islands Here we look at three islands interconnected at glacial maxima, modeled after Choiseul, Isabel, and Guadalcanal of the Solomon Islands. These islands are arranged from west to east, with Guadalcanal arguably not being fully connected to the rest. Our goal is to assess F ST between islands, for which which will use the scikit-allel package. Empirically, we find that F ST between Isabel and Choiseul is much higher than between Guadalcanal and Isabel, consistent with the slight break between those two islands. However, F ST increases faster after divergence with lower population size, so we want to know what density of birds on Guadalcanal would be required to produce this result given knowledge that a density of 25 birds/km 2 produced the empirical F ST between Isabel and Choiseul. We will test 5 densities for Guadalcanal (5, 10, 15, 20, 25, and 30), each with 30 replicates, holding the population density on Isabel as a constant 25 birds/km 2 . First, we have to write our python script. We'll use the argparse module to hand our arguments. Note that we'll set Isabel as the first population and Guadalcanal as the second (in a more complete version we can take island names as input and have a function to determine their sizes). It will output the population size of the first (Isabel) and second (Guadalcanal) island along with the F ST . import os, sys, msprime as msp, numpy as np, allel, re, argparse def main(): # set up argparse parse = argparse.ArgumentParser(description = \"Get simulation parameters\") # two population densities for generality parse.add_argument(\"-d1\", \"--density1\", type=float, help=\"Density of first population\") parse.add_argument(\"-d2\", \"--density2\", type=float, help=\"Density of second population\") parse.add_argument(\"-o\", \"--output\", type=str, help=\"Path to output file\") args = parse.parse_args() # assign argparse values to variables dens1, dens2, outfile = args.density1, args.density2, args.output # calculate population sizes, with Isabel and Guadalcanal being 2999 and 5302 km<sup>2</sup> respectively size1 = dens1 * 2999 size2 = dens2 * 5302 # set up number of samples and demography samples = 30 demography = msp.Demography() demography.add_population(name=\"pop1\", initial_size=size1) demography.add_population(name=\"pop2\", initial_size=size2) demography.add_population(name=\"anc_pop12\", initial_size=size1+size2) demography.add_population_split(time=5700, derived=[\"pop1\",\"pop2\"], ancestral=\"anc_pop12\") # run simulation for 10 megabases trees = msp.sim_ancestry(samples={\"pop1\":samples, \"pop2\":samples}, demography=demography, recombination_rate=1e-8, sequence_length=1e7) # add mutations with a common estimate of mutation rate in birds mutations = msp.sim_mutations(trees, rate=5.9e-9) # get haplotypes from simulation haplotypes = np.array(mutations.genotype_matrix()) genotypes = allel.HaplotypeArray(haplotypes).to_genotypes(ploidy=2) # calculate fst, assumes even sample size fst = allel.stats.fst.average_weir_cockerham_fst(genotypes,[list(range(0,samples)),list(range(samples,samples*2))],10)[0] # write output to file output = open(outfile, \"a\") output.write(str(int(size1))+\"\\t\"+str(int(size2))+\"\\t\"+str(fst)+\"\\n\") output.close() if __name__ == '__main__': main() Now that we have our scripted simulation, we'll write a PBS script to run it in parallel! We assumes you have a directory in your working directory called \"output\" and named your script \"island_msp_twopop.py\". We'll name files based on the denisty on Guadalcanal. We don't have it written in the script, but if you already have something in \"output\", this just appends to those files (i.e. \"rm output\\/*\" beforehand). Note that this script excludes the header. # prepare GNU parallel module load parallel-20170322-gcc-4.8.5-2ycpx7e source $(which env_parallel.bash) # load our environment module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik source activate msp1-env # make a shortcut for our working directory, where we assume all scripts are located. dir=$PBS_O_WORKDIR env_parallel --sshloginfile $PBS_NODEFILE \\ 'echo {2}; python $dir/island_msp_twopop.py -d1 25 -d2 {1} -o $dir/output/density_{1}.out' \\ ::: 3 4 5 10 15 20 25 30 ::: {1..30} For interpretting these results, the empirical F ST value between Guadalcanal and Isabel is 0.077, and the best density match was 4 birds/km 2 (F ST =0.79). That means that to get the observed F ST , Guadalcanal would have to have 16% of the population density inferred on the other islands. Genetic diversity doesn't support this, suggesting that the gap between them formed an excess of population structure compared to the other islands! Citation Below are citations for msprime and GNU parallel. Remember to cite programs you run whenever possible! Kelleher, J., Etheridge, A. M., & McVean, G. (2016). Efficient Coalescent Simulation and Genealogical Analysis for Large Sample Sizes. PLoS Computational Biology, 12(5), 1004842. https://doi.org/10.1371/journal.pcbi.1004842 Tange, O. (2018). GNU Parallel 2018 [Computer software]. https://doi.org/10.5281/zenodo.1146014.","title":"Population genetic simulations with msprime (backwards time"},{"location":"msprime_quickbyte/#running-population-genetic-simulations-with-msprime","text":"Many microevolutionary questions invoke population genetic processes to explain them, from population genetic summary statistics to selection on specific loci of interest. In turn, population genetic simulations are becoming increasingly important to publish in major journals to validate that empirical result are consistent with the processes authors say the findings represent. There are many programs used for these simulations, that fall into two general categories: forward (slow, simulate all individuals in the populations) and backward (fast, work from a number of samples backward to simulate genealogies). Prominent examples of these two are SLiM and msprime respectively. Here, we'll give a basic introduction to msprime, give a simple example of two diverging populations, and show how to parallelize replicates on CARC. The example is one I used to establish how F ST , an estimate of population divergence , changes over time based on population size. In particular, I used this to test if observed values of F ST between islands connected during the Pleistocene became panmictic at those times, or if there was still reduced gene flow between given islands (with the system being one worked on in the past in papers like Smith & Filardi 2007 ). This QuickByte describes msprime 1.0, which is a major update from the widely used earlier versions.","title":"Running Population Genetic Simulations with MSPrime"},{"location":"msprime_quickbyte/#how-msprime-works","text":"The paper describing msprime has been cited hundreds of time, but represents an extension of the simulation program ms , which has been cited a couple thousand times. The goal of this extension is to scale simulations up to the large sample sizes of individuals and loci used in modern day genomic studies. Msprime's fast speed and ease of analysis is achieved by by adding novel ways of keeping track of geneologies being analyzed through sparse trees and coalescence records, collectively refer to ass tree sequences.","title":"How msprime Works"},{"location":"msprime_quickbyte/#how-to-run-msprime","text":"Msprime is a python package, with reccomended download via conda. We reccomend you use miniconda for this, as we've had some bugs with installing this through anaconda in the past. We'll make an environment with it and a couple other important modules: conda create -n msp1-env -c conda-forge msprime scikit-allel numpy Then we'll activate the environment with \"conda activate\", but note that it needs to be \"source activate\" in a script: conda activate msp1-env In a script or in python command line, we'll import msprime as msp for convenience: import msprime as msp The simulations themselves, at a base level, are very simple. All that's needed is a sample size of individuals \"tracked\" backwards. Note that these are assumed diploid unless ploidy is specified: trees = msp.sim_ancestry(samples=10) Mutations can be added using sim_mutations, ideally changing the default mutation rate (here to the per-year songbird mutation rate): mutations = msp.sim_mutations(trees, rate=2.3e-9) This is just one locus, but we gain a lot more information by adding sequence length and recombination rates. trees = msp.sim_ancestry(samples=10, recombination_rate=1e-8, sequence_length=1e6) Unlike the older versions of msprime, outside of pre-made demographic scenarios, you need to add things like populations and merge events to a demography object. Migration rates are also set there. First, we'll initialize the demography and add two populations: demo = msp.Demography() demo.add_population(name=\"pop1\", initial_size=10000) demo.add_population(name=\"pop2\", initial_size=20000) Next, we'll set up an ancestral population and add a population split time (the size is equal to the size before a bottleneck we implement below): demo.add_population(name=\"anc_pop12\", initial_size=50000) demo.add_population_split(time=5700, derived=[\"pop1\",\"pop2\"], ancestral=\"anc_pop12\") Finally, we'll add symmetric migration (i.e. gene flow) between the two focal populations: demo.set_symmetric_migration_rate(['pop1', 'pop2'], 0.001) We can have the migration rate change over time too, here we increase the migration rate during the period between the split time (5700 generations ago) and 4000 generations go: demo.add_symmetric_migration_rate_change(4000, ['pop1', 'pop2'], 0.01) The final common demographic event is population size change, which we'll set to have a bottleneck three quarters of the way through the simulation. That is, we set the population size of population 1 to be three times the present population 2500 years: demo.add_population_parameters_change(time=2500, initial_size=30000, population=\"pop1\") Finally, we need to sort these events, as we added them out of order for instructional purposes: demo.sort_events() A really helpful debugging feature is the print function for demography objects, it lists and explains the events and migration matrix! print(demo) Bringing this all together, our simulation will look like: trees = msp.sim_ancestry(samples={\"pop1\":10, \"pop2\":10}, demography=demo, recombination_rate=1e-8, sequence_length=1e7) And we'll add mutations like this, using a yearly rate of 2.3e-9 and a generation time of 2.55 years (same as the case study): mutations = msp.sim_mutations(trees, rate=5.9e-9)","title":"How to run msprime"},{"location":"msprime_quickbyte/#running-msprime-replicates-on-carc","text":"It is best practice to run many replicates of any simulation you run to assess the robustness of any estimates you make. You can use GNU Parallel (specifically env_parallel) to run these simulations, and can add these replicates directly to an output file. The following examples your python simulation script takes population size (popsize) and population divergence time (divtime) and have an output file like \"$popsize_$divtime.out\" that the script writes to. First we'll run 30 replicates. Note that the \"echo {}\" just to deal with the parallel iterator, so GNU parallel doesn't append it to the end of our python call by default. env_parallel --sshloginfile $PBS_NODEFILE \\ 'echo {}; /path/to/python msprime_script.py --popsize 2000 --divtime 10000 \\ --output ./outputs/1000_10000.out' ::: {1..30} You could also use GNU parallel to iterate over multiple parameter combinations, here we test multiple population sizes (2000, 3000, and 5000) and divergence times (1000, 5000, and 10000 generations). We'll assume the script has replicates coded into it. env_parallel --sshloginfile $PBS_NODEFILE \\ '/path/to/python msprime_script.py --popsize {1} --divtime {2} \\ --output ./outputs/{1}_{2}' ::: 2000 3000 5000 ::: 1000 5000 10000","title":"Running MSPrime replicates on CARC"},{"location":"msprime_quickbyte/#case-study-divergence-between-connected-islands","text":"Here we look at three islands interconnected at glacial maxima, modeled after Choiseul, Isabel, and Guadalcanal of the Solomon Islands. These islands are arranged from west to east, with Guadalcanal arguably not being fully connected to the rest. Our goal is to assess F ST between islands, for which which will use the scikit-allel package. Empirically, we find that F ST between Isabel and Choiseul is much higher than between Guadalcanal and Isabel, consistent with the slight break between those two islands. However, F ST increases faster after divergence with lower population size, so we want to know what density of birds on Guadalcanal would be required to produce this result given knowledge that a density of 25 birds/km 2 produced the empirical F ST between Isabel and Choiseul. We will test 5 densities for Guadalcanal (5, 10, 15, 20, 25, and 30), each with 30 replicates, holding the population density on Isabel as a constant 25 birds/km 2 . First, we have to write our python script. We'll use the argparse module to hand our arguments. Note that we'll set Isabel as the first population and Guadalcanal as the second (in a more complete version we can take island names as input and have a function to determine their sizes). It will output the population size of the first (Isabel) and second (Guadalcanal) island along with the F ST . import os, sys, msprime as msp, numpy as np, allel, re, argparse def main(): # set up argparse parse = argparse.ArgumentParser(description = \"Get simulation parameters\") # two population densities for generality parse.add_argument(\"-d1\", \"--density1\", type=float, help=\"Density of first population\") parse.add_argument(\"-d2\", \"--density2\", type=float, help=\"Density of second population\") parse.add_argument(\"-o\", \"--output\", type=str, help=\"Path to output file\") args = parse.parse_args() # assign argparse values to variables dens1, dens2, outfile = args.density1, args.density2, args.output # calculate population sizes, with Isabel and Guadalcanal being 2999 and 5302 km<sup>2</sup> respectively size1 = dens1 * 2999 size2 = dens2 * 5302 # set up number of samples and demography samples = 30 demography = msp.Demography() demography.add_population(name=\"pop1\", initial_size=size1) demography.add_population(name=\"pop2\", initial_size=size2) demography.add_population(name=\"anc_pop12\", initial_size=size1+size2) demography.add_population_split(time=5700, derived=[\"pop1\",\"pop2\"], ancestral=\"anc_pop12\") # run simulation for 10 megabases trees = msp.sim_ancestry(samples={\"pop1\":samples, \"pop2\":samples}, demography=demography, recombination_rate=1e-8, sequence_length=1e7) # add mutations with a common estimate of mutation rate in birds mutations = msp.sim_mutations(trees, rate=5.9e-9) # get haplotypes from simulation haplotypes = np.array(mutations.genotype_matrix()) genotypes = allel.HaplotypeArray(haplotypes).to_genotypes(ploidy=2) # calculate fst, assumes even sample size fst = allel.stats.fst.average_weir_cockerham_fst(genotypes,[list(range(0,samples)),list(range(samples,samples*2))],10)[0] # write output to file output = open(outfile, \"a\") output.write(str(int(size1))+\"\\t\"+str(int(size2))+\"\\t\"+str(fst)+\"\\n\") output.close() if __name__ == '__main__': main() Now that we have our scripted simulation, we'll write a PBS script to run it in parallel! We assumes you have a directory in your working directory called \"output\" and named your script \"island_msp_twopop.py\". We'll name files based on the denisty on Guadalcanal. We don't have it written in the script, but if you already have something in \"output\", this just appends to those files (i.e. \"rm output\\/*\" beforehand). Note that this script excludes the header. # prepare GNU parallel module load parallel-20170322-gcc-4.8.5-2ycpx7e source $(which env_parallel.bash) # load our environment module load miniconda3-4.7.12.1-gcc-4.8.5-lmtvtik source activate msp1-env # make a shortcut for our working directory, where we assume all scripts are located. dir=$PBS_O_WORKDIR env_parallel --sshloginfile $PBS_NODEFILE \\ 'echo {2}; python $dir/island_msp_twopop.py -d1 25 -d2 {1} -o $dir/output/density_{1}.out' \\ ::: 3 4 5 10 15 20 25 30 ::: {1..30} For interpretting these results, the empirical F ST value between Guadalcanal and Isabel is 0.077, and the best density match was 4 birds/km 2 (F ST =0.79). That means that to get the observed F ST , Guadalcanal would have to have 16% of the population density inferred on the other islands. Genetic diversity doesn't support this, suggesting that the gap between them formed an excess of population structure compared to the other islands!","title":"Case study: Divergence between connected islands"},{"location":"msprime_quickbyte/#citation","text":"Below are citations for msprime and GNU parallel. Remember to cite programs you run whenever possible! Kelleher, J., Etheridge, A. M., & McVean, G. (2016). Efficient Coalescent Simulation and Genealogical Analysis for Large Sample Sizes. PLoS Computational Biology, 12(5), 1004842. https://doi.org/10.1371/journal.pcbi.1004842 Tange, O. (2018). GNU Parallel 2018 [Computer software]. https://doi.org/10.5281/zenodo.1146014.","title":"Citation"},{"location":"orca_wheeler_taos/","text":"Using Orca on Wheeler and Taos Submitting an Orca script on Wheeler Wheeler uses PBS ( P ortable B atch S ystem) to submit jobs to the queue for execution. Below is an example script that can be used on Wheeler named orca_submission.pbs : #!/usr/bin/bash ## Setup your qsub flags here requesting resources. #PBS -l walltime=10:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_orca_job #PBS -M <YourNetID>@unm.edu #PBS -m bae ## Change to the submission directory ## and load the Orca software module cd $PBS_O_WORKDIR module load orca/4.0.1 ## Set your input file for Orca input_file=my_orca_input.inp # Orca needs the full path when running in parallel full_orca_path=$(which orca) # Run Orca $full_orca_path $input_file Now you can simply submit your Orca job to the queue with qsub orca_submission.pbs . Submitting on Orca script on Taos Taos uses Slurm ( S imple L inux U tility for R esource M anagement) to submit jobs and manage resources. Slurm provides greater control over resource management and utilization which means one has to be more explicit in their submission script. Specifically, it is necessary to request sufficient memory for your task when submitting your job. Below is a sample script for submitting an Orca job on Taos named orca_submission.sh : #!/usr/bin/bash ## Set your slurm flags here requesting resources. #SBATCH --job-name=orca_test #SBATCH --output=test.out #SBATCH --ntasks=8 #SBATCH --cpus-per-task=1 ## This depends on which queue you have access to. #SBATCH --partition=my_partition #SBATCH --mem-per-cpu=6GB #SBATCH --mail-type=begin # send email when job begins #SBATCH --mail-type=end # send email when job ends #SBATCH --mail-type=fail # send email if job fails #SBATCH --mail-user=<YourNetID>@unm.edu module load openmpi-3.1.5-gcc-5.4.0-f6ikvl6 module load orca/4.2.1 ## Set your input and output file names input_file=my_orca_input.inp output_file=my_orca_output.log prefix=$(echo $input_file | cut -f1 -d\".\") # Set the scratch directory path scratch_dir=/taos/scratch/$USER/ # Set the input and output paths on the scratch file system mkdir $scratch_dir$prefix TEMP_DIR=$scratch_dir$prefix output_scratch_path=$TEMP_DIR/$output_file input_scratch_path=$TEMP_DIR/$input_file # Create directory for additional files mkdir $SLURM_SUBMIT_DIR/$prefix add_files_dir=$SLURM_SUBMIT_DIR/$prefix # Copy the input file from the submission directory to the scratch directory cp $SLURM_SUBMIT_DIR/$input_file $TEMP_DIR/ # Orca needs the full path when running in parallel full_orca_path=$(which orca) # Run Orca $full_orca_path $input_scratch_path > $output_scratch_path # Orca finished so copy the output file on scratch to the submission directory and clean the scratch directory cp $output_scratch_path $SLURM_SUBMIT_DIR/$output_file cp $TEMP_DIR/$prefix* $add_files_dir rm $TEMP_DIR Now you can simply submit your job to the queue with sbatch orca_submission.sh .","title":"Orca on Wheeler and Taos"},{"location":"orca_wheeler_taos/#using-orca-on-wheeler-and-taos","text":"","title":"Using Orca on Wheeler and Taos"},{"location":"orca_wheeler_taos/#submitting-an-orca-script-on-wheeler","text":"Wheeler uses PBS ( P ortable B atch S ystem) to submit jobs to the queue for execution. Below is an example script that can be used on Wheeler named orca_submission.pbs : #!/usr/bin/bash ## Setup your qsub flags here requesting resources. #PBS -l walltime=10:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_orca_job #PBS -M <YourNetID>@unm.edu #PBS -m bae ## Change to the submission directory ## and load the Orca software module cd $PBS_O_WORKDIR module load orca/4.0.1 ## Set your input file for Orca input_file=my_orca_input.inp # Orca needs the full path when running in parallel full_orca_path=$(which orca) # Run Orca $full_orca_path $input_file Now you can simply submit your Orca job to the queue with qsub orca_submission.pbs .","title":"Submitting an Orca script on Wheeler"},{"location":"orca_wheeler_taos/#submitting-on-orca-script-on-taos","text":"Taos uses Slurm ( S imple L inux U tility for R esource M anagement) to submit jobs and manage resources. Slurm provides greater control over resource management and utilization which means one has to be more explicit in their submission script. Specifically, it is necessary to request sufficient memory for your task when submitting your job. Below is a sample script for submitting an Orca job on Taos named orca_submission.sh : #!/usr/bin/bash ## Set your slurm flags here requesting resources. #SBATCH --job-name=orca_test #SBATCH --output=test.out #SBATCH --ntasks=8 #SBATCH --cpus-per-task=1 ## This depends on which queue you have access to. #SBATCH --partition=my_partition #SBATCH --mem-per-cpu=6GB #SBATCH --mail-type=begin # send email when job begins #SBATCH --mail-type=end # send email when job ends #SBATCH --mail-type=fail # send email if job fails #SBATCH --mail-user=<YourNetID>@unm.edu module load openmpi-3.1.5-gcc-5.4.0-f6ikvl6 module load orca/4.2.1 ## Set your input and output file names input_file=my_orca_input.inp output_file=my_orca_output.log prefix=$(echo $input_file | cut -f1 -d\".\") # Set the scratch directory path scratch_dir=/taos/scratch/$USER/ # Set the input and output paths on the scratch file system mkdir $scratch_dir$prefix TEMP_DIR=$scratch_dir$prefix output_scratch_path=$TEMP_DIR/$output_file input_scratch_path=$TEMP_DIR/$input_file # Create directory for additional files mkdir $SLURM_SUBMIT_DIR/$prefix add_files_dir=$SLURM_SUBMIT_DIR/$prefix # Copy the input file from the submission directory to the scratch directory cp $SLURM_SUBMIT_DIR/$input_file $TEMP_DIR/ # Orca needs the full path when running in parallel full_orca_path=$(which orca) # Run Orca $full_orca_path $input_scratch_path > $output_scratch_path # Orca finished so copy the output file on scratch to the submission directory and clean the scratch directory cp $output_scratch_path $SLURM_SUBMIT_DIR/$output_file cp $TEMP_DIR/$prefix* $add_files_dir rm $TEMP_DIR Now you can simply submit your job to the queue with sbatch orca_submission.sh .","title":"Submitting on Orca script on Taos"},{"location":"parallel_jupyterhub_with_dask_and_scikit-learn/","text":"Example of Parallelization with JupyterHub using Dask and SciKit-learn Dask uses existing Python APIs and data structures to make it easy to switch between Numpy, Pandas, Scikit-learn to their Dask-powered equivalents. SciKit-learn is a machine learning tool for Python. Log in to JupyterHub On a computer connected to ethernet on main campus, open an internet browser go to https://wheeler.alliance.unm.edu:8000 where you will be asked to log in. Use your carc username and password. This logs you into a compute node where your programs in Jupyter notebook will be running. Because it is beginning an interactive job it may not be instant depending on resources available at the time. Once logged in, you can see all the files in your home directory. To be kind to other users when you are finished with JupyterHub for the day, please be sure to go to \"control panel\" in the top righthand corner and click \"stop my server\". This will free up the node for other users. Otherwise, the default walltime is 12 hours. Setup cluster resources with Dask-jobqueue from dask_jobqueue import PBSCluster from dask.distributed import Client, progress import time cluster = PBSCluster(memory=\"42GB\",cores=8, resource_spec=\"nodes=1:ppn=8\", queue=\"default\", walltime='01:00:00') print(cluster.job_script()) #!/usr/bin/env bash #PBS -N dask-worker #PBS -q default #PBS -l nodes=1:ppn=8 #PBS -l walltime=01:00:00 JOB_ID=${PBS_JOBID%%.*} /opt/local/anaconda3/envs/jupyterhub/bin/python -m distributed.cli.dask_worker tcp://172.16.2.42:46451 --nthreads 8 --memory-limit 42.00GB --name name --nanny --death-timeout 60 cluster.scale(4) A loop to check when all the resources are ready for x in range(10): print(cluster) time.sleep(5) PBSCluster('tcp://172.16.2.42:46451', workers=0, threads=0, memory=0 B) PBSCluster('tcp://172.16.2.42:46451', workers=1, threads=8, memory=42.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) client = Client(cluster) Run a simple parallel program to test functionality def slow_increment(x): time.sleep(1) return x + 1 futures = client.map(slow_increment, range(5000)) progress(futures) VBox() Demonstrate how dask integrates with Scikit-Learn # Scikit-learn bundles joblib, so you need to import from # `sklearn.externals.joblib` instead of `joblib` directly from sklearn.externals.joblib import parallel_backend from sklearn.datasets import load_digits from sklearn.model_selection import RandomizedSearchCV from sklearn.svm import SVC import numpy as np digits = load_digits() param_space = { 'C': np.logspace(-6, 6, 13), 'gamma': np.logspace(-8, 8, 17), 'tol': np.logspace(-4, -1, 4), 'class_weight': [None, 'balanced'], } model = SVC(kernel='rbf') search = RandomizedSearchCV(model, param_space, cv=3, n_iter=50, verbose=10) # Serialize the training data only once to each worker with parallel_backend('dask', scatter=[digits.data, digits.target]): search.fit(digits.data, digits.target) Fitting 3 folds for each of 50 candidates, totalling 150 fits [Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 32 concurrent workers. [Parallel(n_jobs=-1)]: Done 8 tasks | elapsed: 2.2s [Parallel(n_jobs=-1)]: Done 21 tasks | elapsed: 2.9s [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 3.5s [Parallel(n_jobs=-1)]: Done 49 tasks | elapsed: 3.8s [Parallel(n_jobs=-1)]: Done 64 tasks | elapsed: 4.1s [Parallel(n_jobs=-1)]: Done 81 tasks | elapsed: 4.6s [Parallel(n_jobs=-1)]: Done 103 out of 150 | elapsed: 5.0s remaining: 2.3s [Parallel(n_jobs=-1)]: Done 119 out of 150 | elapsed: 5.3s remaining: 1.4s [Parallel(n_jobs=-1)]: Done 135 out of 150 | elapsed: 5.7s remaining: 0.6s [Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 6.1s finished /opt/local/anaconda3/envs/jupyterhub/lib/python3.6/site-packages/sklearn/model_selection/_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) print(search) RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto_deprecated', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid='warn', n_iter=50, n_jobs=None, param_distributions={'C': array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]), 'gamma': array([1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08]), 'tol': array([0.0001, 0.001 , 0.01 , 0.1 ]), 'class_weight': [None, 'balanced']}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score='warn', scoring=None, verbose=10)","title":"Parallelization with JupyterHub using Dask and SciKit-learn"},{"location":"parallel_jupyterhub_with_dask_and_scikit-learn/#example-of-parallelization-with-jupyterhub-using-dask-and-scikit-learn","text":"Dask uses existing Python APIs and data structures to make it easy to switch between Numpy, Pandas, Scikit-learn to their Dask-powered equivalents. SciKit-learn is a machine learning tool for Python.","title":"Example of Parallelization with JupyterHub using Dask and SciKit-learn"},{"location":"parallel_jupyterhub_with_dask_and_scikit-learn/#log-in-to-jupyterhub","text":"On a computer connected to ethernet on main campus, open an internet browser go to https://wheeler.alliance.unm.edu:8000 where you will be asked to log in. Use your carc username and password. This logs you into a compute node where your programs in Jupyter notebook will be running. Because it is beginning an interactive job it may not be instant depending on resources available at the time. Once logged in, you can see all the files in your home directory. To be kind to other users when you are finished with JupyterHub for the day, please be sure to go to \"control panel\" in the top righthand corner and click \"stop my server\". This will free up the node for other users. Otherwise, the default walltime is 12 hours.","title":"Log in to JupyterHub"},{"location":"parallel_jupyterhub_with_dask_and_scikit-learn/#setup-cluster-resources-with-dask-jobqueue","text":"from dask_jobqueue import PBSCluster from dask.distributed import Client, progress import time cluster = PBSCluster(memory=\"42GB\",cores=8, resource_spec=\"nodes=1:ppn=8\", queue=\"default\", walltime='01:00:00') print(cluster.job_script()) #!/usr/bin/env bash #PBS -N dask-worker #PBS -q default #PBS -l nodes=1:ppn=8 #PBS -l walltime=01:00:00 JOB_ID=${PBS_JOBID%%.*} /opt/local/anaconda3/envs/jupyterhub/bin/python -m distributed.cli.dask_worker tcp://172.16.2.42:46451 --nthreads 8 --memory-limit 42.00GB --name name --nanny --death-timeout 60 cluster.scale(4)","title":"Setup cluster resources with Dask-jobqueue"},{"location":"parallel_jupyterhub_with_dask_and_scikit-learn/#a-loop-to-check-when-all-the-resources-are-ready","text":"for x in range(10): print(cluster) time.sleep(5) PBSCluster('tcp://172.16.2.42:46451', workers=0, threads=0, memory=0 B) PBSCluster('tcp://172.16.2.42:46451', workers=1, threads=8, memory=42.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) PBSCluster('tcp://172.16.2.42:46451', workers=4, threads=32, memory=168.00 GB) client = Client(cluster)","title":"A loop to check when all the resources are ready"},{"location":"parallel_jupyterhub_with_dask_and_scikit-learn/#run-a-simple-parallel-program-to-test-functionality","text":"def slow_increment(x): time.sleep(1) return x + 1 futures = client.map(slow_increment, range(5000)) progress(futures) VBox()","title":"Run a simple parallel program to test functionality"},{"location":"parallel_jupyterhub_with_dask_and_scikit-learn/#demonstrate-how-dask-integrates-with-scikit-learn","text":"# Scikit-learn bundles joblib, so you need to import from # `sklearn.externals.joblib` instead of `joblib` directly from sklearn.externals.joblib import parallel_backend from sklearn.datasets import load_digits from sklearn.model_selection import RandomizedSearchCV from sklearn.svm import SVC import numpy as np digits = load_digits() param_space = { 'C': np.logspace(-6, 6, 13), 'gamma': np.logspace(-8, 8, 17), 'tol': np.logspace(-4, -1, 4), 'class_weight': [None, 'balanced'], } model = SVC(kernel='rbf') search = RandomizedSearchCV(model, param_space, cv=3, n_iter=50, verbose=10) # Serialize the training data only once to each worker with parallel_backend('dask', scatter=[digits.data, digits.target]): search.fit(digits.data, digits.target) Fitting 3 folds for each of 50 candidates, totalling 150 fits [Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 32 concurrent workers. [Parallel(n_jobs=-1)]: Done 8 tasks | elapsed: 2.2s [Parallel(n_jobs=-1)]: Done 21 tasks | elapsed: 2.9s [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 3.5s [Parallel(n_jobs=-1)]: Done 49 tasks | elapsed: 3.8s [Parallel(n_jobs=-1)]: Done 64 tasks | elapsed: 4.1s [Parallel(n_jobs=-1)]: Done 81 tasks | elapsed: 4.6s [Parallel(n_jobs=-1)]: Done 103 out of 150 | elapsed: 5.0s remaining: 2.3s [Parallel(n_jobs=-1)]: Done 119 out of 150 | elapsed: 5.3s remaining: 1.4s [Parallel(n_jobs=-1)]: Done 135 out of 150 | elapsed: 5.7s remaining: 0.6s [Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 6.1s finished /opt/local/anaconda3/envs/jupyterhub/lib/python3.6/site-packages/sklearn/model_selection/_search.py:842: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) print(search) RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto_deprecated', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid='warn', n_iter=50, n_jobs=None, param_distributions={'C': array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]), 'gamma': array([1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08]), 'tol': array([0.0001, 0.001 , 0.01 , 0.1 ]), 'class_weight': [None, 'balanced']}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score='warn', scoring=None, verbose=10)","title":"Demonstrate how dask integrates with Scikit-Learn"},{"location":"parallelization_with_Jupyterhub_using_mpi/","text":"Parallelization with JupyterHub using MPI The following steps will show you the steps to use MPI through ipython's ipyparallel interface. Create a PBS profile on CARC Once you are logged in at carc run these steps: cd /projects/systems/shared/ipython_cluster_profiles cp -r profile_pbs ~/.ipython/ Then check the files copied. cd ~/.ipython/profile_pbs Now on JupyterHub go to the IPython Clusters tab (refresh if already open) and you should see a pbs profile now available to you. Click the JupyterHub icon in the upper left of your screen if you can't see the clusters tab. You can start a job by setting number of engine in the 'pbs' cluster profile and clicking start under actions. For this example we will request 8 ipython compute engines. [Optional] Since ipython's ipyparallel system is requesting compute nodes through the torque PBS system you will have to wait until the nodes are running before you can run code on them. Check that the job is running in terminal with watch qstat -tn -u <username> You should see something like the following: Every 2.0s: qstat -t -n -u $USER Wed Oct 23 09:15:14 2019 wheeler-sn.alliance.unm.edu: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 258370.wheeler-sn.alli mfricke default jupyterhub 21730 1 1 -- 08:00:00 R 00:06:45 wheeler291/1 258371.wheeler-sn.alli mfricke default ipython_controll 22553 1 1 -- 01:00:00 R 00:06:11 wheeler291/2 258372.wheeler-sn.alli mfricke default ipython_engine 3213 2 16 -- 01:00:00 R 00:06:11 wheeler176/0-7+wheeler175/0-7 Notice the ipython engines are running with status 'R'. You can also check to see whether the compute engines are ready in your python notebook (see below). To exit the watch command use control-C To change the walltime of your profile, in the ~/.ipython/profile_pbs directory edit the pbs.engine.template and the pbs.controller.template to fit the requirments for your job. By editing these files you can also change from the default to debug queue as you are testing your program. Now you can open a Jupyter notebook and follow the remainder of this tutorial. Creating an example function that uses MPI Create a new file in your home directory and name it psum.py. Enter the following into psum.py and save the file. from mpi4py import MPI import numpy as np def psum(a): locsum = np.sum(a) rcvBuf = np.array(0.0,'d') MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE], [rcvBuf, MPI.DOUBLE], op=MPI.SUM) return rcvBuf This function performs a distributed sum over all the nodes on the MPI communications group. Create a Jupyter Notebook to Call Our MPI Function Create a new Python 3 notebook in Jupyterhub and name it mpi_test.ipynb. Enter the following into cells of your notebook. Many of the commands are run on the MPI cluster and so are asynchronous. To check whether an operation has completed we check the status with \".wait_interactive()\". When the status reports \"done\" you can move on to the next step. Load required packages for ipyparallel and MPI import ipyparallel as ipp from mpi4py import MPI import numpy as np Create a cluster to use the CPUs allocated thrugh PBS cluster = ipp.Client(profile='pbs') Check if the cluster is ready. We are looking for 8 ids since we asked for 8 engines. Engines in ipparallel parlence are the same as processes or workers in other parallel systems. cluster.ids [0, 1, 2, 3, 4, 5, 6, 7] len(cluster[:]) 8 Assign the engines to a variable named \"view\" to allow us to interact with them view = cluster[:] Enable ipython `magics\u00b4. These are ipython helper functions such as % view.activate() Check to see if the MPI communication world is of the expected size. It should be size 8 since we have 8 engines. Note we are running the Get_size command on each engine to make sure they all see the same MPI comm world. %px simply executes the code following it on each compute engine in parallel. status_mpi_size=%px size = MPI.COMM_WORLD.Get_size() status_mpi_size.wait_interactive() done The output of viewing the size variable should be an array with the same number of entries as engines, and each entry should be the number of engines requested. view['size'] [8, 8, 8, 8, 8, 8, 8, 8] Run the external python code in \u00b4psum.py\u00b4 on all the engines. Recall that psum.py just loads the MPI libraries and defines the distributed sum function, psum. We are not actually calling the psum function yet. status_psum_run=view.run('psum.py') status_psum_run.wait_interactive() done Send data to all nodes to by summed The scatter command sends 32 values from 0 to 31 to the 8 compute engines. Each compute engine gets 32/8=4 values. This is the ipyparallel scatter command, not an MPI scatter command. status_scatter=view.scatter('a',np.arange(32,dtype='float')) done We can view the variable 'a' on all the compute engines. The value of 'a' for each compute engine is an element of the return array. In this case each value is itself an array. view['a'] [array([0., 1., 2., 3.]), array([4., 5., 6., 7.]), array([ 8., 9., 10., 11.]), array([12., 13., 14., 15.]), array([16., 17., 18., 19.]), array([20., 21., 22., 23.]), array([24., 25., 26., 27.]), array([28., 29., 30., 31.])] Execute the psum function on all the compute engines and store the result in totalsum MPI code has to be executed on each compute engine so they can each perform the MPI reduce. This is accomplished by running calling the psum function on all the compute engines simultaniosly. MPI will allow them to communicate with each other to calculate the sum. status_psum_call=%px totalsum = psum(a) status_psum_call.wait_interactive() done Check the value of totalsum on each node Total should be equal to 31(31+1)/2=496 view['totalsum'] [array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.)] Each compute engine calculated the sum of all the values. Since we ran this MPI function on all the compute engines they report the same value. Defining functions in the notebook Rather than loading psum from file we can define it in the notebook using the ipython function decorator '@'. @view.remote(block = True) def inlinesum(): from mpi4py import MPI import numpy as np locsum = np.sum(a) rcvBuf = np.array(0.0,'d') MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE], [rcvBuf, MPI.DOUBLE], op=MPI.SUM) return rcvBuf Now we can call inlinesum and it is automatically run on every compute engine. The call is through ipyparallels but the computation is still using MPI. inlinesum() [array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.)]","title":"JupyterHub Parallel Processing with MPI"},{"location":"parallelization_with_Jupyterhub_using_mpi/#parallelization-with-jupyterhub-using-mpi","text":"The following steps will show you the steps to use MPI through ipython's ipyparallel interface.","title":"Parallelization with JupyterHub using MPI"},{"location":"parallelization_with_Jupyterhub_using_mpi/#create-a-pbs-profile-on-carc","text":"Once you are logged in at carc run these steps: cd /projects/systems/shared/ipython_cluster_profiles cp -r profile_pbs ~/.ipython/ Then check the files copied. cd ~/.ipython/profile_pbs Now on JupyterHub go to the IPython Clusters tab (refresh if already open) and you should see a pbs profile now available to you. Click the JupyterHub icon in the upper left of your screen if you can't see the clusters tab. You can start a job by setting number of engine in the 'pbs' cluster profile and clicking start under actions. For this example we will request 8 ipython compute engines. [Optional] Since ipython's ipyparallel system is requesting compute nodes through the torque PBS system you will have to wait until the nodes are running before you can run code on them. Check that the job is running in terminal with watch qstat -tn -u <username> You should see something like the following: Every 2.0s: qstat -t -n -u $USER Wed Oct 23 09:15:14 2019 wheeler-sn.alliance.unm.edu: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time ----------------------- ----------- -------- ---------------- ------ ----- ------ --------- --------- - --------- 258370.wheeler-sn.alli mfricke default jupyterhub 21730 1 1 -- 08:00:00 R 00:06:45 wheeler291/1 258371.wheeler-sn.alli mfricke default ipython_controll 22553 1 1 -- 01:00:00 R 00:06:11 wheeler291/2 258372.wheeler-sn.alli mfricke default ipython_engine 3213 2 16 -- 01:00:00 R 00:06:11 wheeler176/0-7+wheeler175/0-7 Notice the ipython engines are running with status 'R'. You can also check to see whether the compute engines are ready in your python notebook (see below). To exit the watch command use control-C To change the walltime of your profile, in the ~/.ipython/profile_pbs directory edit the pbs.engine.template and the pbs.controller.template to fit the requirments for your job. By editing these files you can also change from the default to debug queue as you are testing your program. Now you can open a Jupyter notebook and follow the remainder of this tutorial.","title":"Create a PBS profile on CARC"},{"location":"parallelization_with_Jupyterhub_using_mpi/#creating-an-example-function-that-uses-mpi","text":"Create a new file in your home directory and name it psum.py. Enter the following into psum.py and save the file. from mpi4py import MPI import numpy as np def psum(a): locsum = np.sum(a) rcvBuf = np.array(0.0,'d') MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE], [rcvBuf, MPI.DOUBLE], op=MPI.SUM) return rcvBuf This function performs a distributed sum over all the nodes on the MPI communications group.","title":"Creating an example function that uses MPI"},{"location":"parallelization_with_Jupyterhub_using_mpi/#create-a-jupyter-notebook-to-call-our-mpi-function","text":"Create a new Python 3 notebook in Jupyterhub and name it mpi_test.ipynb. Enter the following into cells of your notebook. Many of the commands are run on the MPI cluster and so are asynchronous. To check whether an operation has completed we check the status with \".wait_interactive()\". When the status reports \"done\" you can move on to the next step.","title":"Create a Jupyter Notebook to Call Our MPI Function"},{"location":"parallelization_with_Jupyterhub_using_mpi/#load-required-packages-for-ipyparallel-and-mpi","text":"import ipyparallel as ipp from mpi4py import MPI import numpy as np","title":"Load required packages for ipyparallel and MPI"},{"location":"parallelization_with_Jupyterhub_using_mpi/#create-a-cluster-to-use-the-cpus-allocated-thrugh-pbs","text":"cluster = ipp.Client(profile='pbs')","title":"Create a cluster to use the CPUs allocated thrugh PBS"},{"location":"parallelization_with_Jupyterhub_using_mpi/#check-if-the-cluster-is-ready-we-are-looking-for-8-ids-since-we-asked-for-8-engines","text":"Engines in ipparallel parlence are the same as processes or workers in other parallel systems. cluster.ids [0, 1, 2, 3, 4, 5, 6, 7] len(cluster[:]) 8","title":"Check if the cluster is ready. We are looking for 8 ids since we asked for 8 engines."},{"location":"parallelization_with_Jupyterhub_using_mpi/#assign-the-engines-to-a-variable-named-view-to-allow-us-to-interact-with-them","text":"view = cluster[:] Enable ipython `magics\u00b4. These are ipython helper functions such as % view.activate()","title":"Assign the engines to a variable named \"view\" to allow us to interact with them"},{"location":"parallelization_with_Jupyterhub_using_mpi/#check-to-see-if-the-mpi-communication-world-is-of-the-expected-size-it-should-be-size-8-since-we-have-8-engines","text":"Note we are running the Get_size command on each engine to make sure they all see the same MPI comm world. %px simply executes the code following it on each compute engine in parallel. status_mpi_size=%px size = MPI.COMM_WORLD.Get_size() status_mpi_size.wait_interactive() done The output of viewing the size variable should be an array with the same number of entries as engines, and each entry should be the number of engines requested. view['size'] [8, 8, 8, 8, 8, 8, 8, 8]","title":"Check to see if the MPI communication world is of the expected size. It should be size 8 since we have 8 engines."},{"location":"parallelization_with_Jupyterhub_using_mpi/#run-the-external-python-code-in-psumpy-on-all-the-engines","text":"Recall that psum.py just loads the MPI libraries and defines the distributed sum function, psum. We are not actually calling the psum function yet. status_psum_run=view.run('psum.py') status_psum_run.wait_interactive() done","title":"Run the external python code in \u00b4psum.py\u00b4 on all the engines."},{"location":"parallelization_with_Jupyterhub_using_mpi/#send-data-to-all-nodes-to-by-summed","text":"The scatter command sends 32 values from 0 to 31 to the 8 compute engines. Each compute engine gets 32/8=4 values. This is the ipyparallel scatter command, not an MPI scatter command. status_scatter=view.scatter('a',np.arange(32,dtype='float')) done We can view the variable 'a' on all the compute engines. The value of 'a' for each compute engine is an element of the return array. In this case each value is itself an array. view['a'] [array([0., 1., 2., 3.]), array([4., 5., 6., 7.]), array([ 8., 9., 10., 11.]), array([12., 13., 14., 15.]), array([16., 17., 18., 19.]), array([20., 21., 22., 23.]), array([24., 25., 26., 27.]), array([28., 29., 30., 31.])]","title":"Send data to all nodes to by summed"},{"location":"parallelization_with_Jupyterhub_using_mpi/#execute-the-psum-function-on-all-the-compute-engines-and-store-the-result-in-totalsum","text":"MPI code has to be executed on each compute engine so they can each perform the MPI reduce. This is accomplished by running calling the psum function on all the compute engines simultaniosly. MPI will allow them to communicate with each other to calculate the sum. status_psum_call=%px totalsum = psum(a) status_psum_call.wait_interactive() done","title":"Execute the psum function on all the compute engines and store the result in totalsum"},{"location":"parallelization_with_Jupyterhub_using_mpi/#check-the-value-of-totalsum-on-each-node","text":"Total should be equal to 31(31+1)/2=496 view['totalsum'] [array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.)] Each compute engine calculated the sum of all the values. Since we ran this MPI function on all the compute engines they report the same value.","title":"Check the value of totalsum on each node"},{"location":"parallelization_with_Jupyterhub_using_mpi/#defining-functions-in-the-notebook","text":"Rather than loading psum from file we can define it in the notebook using the ipython function decorator '@'. @view.remote(block = True) def inlinesum(): from mpi4py import MPI import numpy as np locsum = np.sum(a) rcvBuf = np.array(0.0,'d') MPI.COMM_WORLD.Allreduce([locsum, MPI.DOUBLE], [rcvBuf, MPI.DOUBLE], op=MPI.SUM) return rcvBuf Now we can call inlinesum and it is automatically run on every compute engine. The call is through ipyparallels but the computation is still using MPI. inlinesum() [array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.), array(496.)]","title":"Defining functions in the notebook"},{"location":"paraview/","text":"How to: Remote Visualization using ParaView in parallel. ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView's batch processing capabilities. ParaView was developed to analyze extremely large datasets using distributed memory computing resources. It can be run on supercomputers to analyze datasets of petascale size. Overview: Paraview Connection and Documentation The process to connecto to ParaView is, in one terminal you will ask Wheeler or Gibbs to give you compute nodes, where you will run the ParaView server. Once the ParaView server is listening for connections, you will open an ssh tunnel in another terminal window (This process is from your local computer to one of the compute nodes you were assigned). Then, you will tell the ParaView client on your computer to connect to the tunnel and so to the compute nodes at CARC, where it will perform the rendering. ** NOTE: Be sure that the ParaView version installed on your local computer matches the same one that is installed on ether Wheeler or Gibbs cluster. Gibbs's ParaView version is 5.9.1, Wheeler's version is 5.11.0-RC1 and for Hopper Cluster Information, click here. Downloads: https://www.paraview.org/download/ ParaView 5.9.1 Guide: https://docs.paraview.org/en/v5.9.1/ Paraview 5.11.0-RC1 Guide: https://docs.paraview.org/en/v5.11.0/UsersGuide/index.html Wiki Page: https://www.paraview.org/Wiki/ParaView Method 1: Client - Server Mode (Gibbs Direct Connection) The most common approach to use ParaView on Gibbs is through the Client-Server mode support by ParaView, which requires an installation of ParaView on your local computer. This is a two-step process, requesting a compute node via SSH and opening an SSH tunnel to Gibbs's service node. The following examples assume you are using the Gibbs cluster. Terminal 1: SSH to Gibbs Accessing Gibbs and requesting one nodes with four cores. NOTE: Wait until Gibbs assigns you a compute node. ssh username@gibbs.alliance.unm.edu qsub -I -l nodes=1:ppn=4 Loading ParaView Module 5.9.1 in Gibbs module load paraview-5.9.1-gcc-10.2.0-enwua53 Run ParaView PVServer on Compute Nodes mpiexec -np 4 pvserver --mpi --use-offscreen-rendering --server-port=11111 Terminal 2: SSH Tunnel to a Gibbs Compute node Note: this step is from your local computer to Gibbs's compute node. Make sure to replace the \"compute_node_name\" from the bash command below to one of the compute nodes you were assigned by qsub. Example \"gibbs18\" or \"gibbs20\". ssh -L 11111:compute_node_name:11111 username@gibbs.alliance.unm.edu Opening ParaView 5.9.1 and Setup Server Configuration File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Gibbs Server Type: \"Client / Server\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the qdel command on the cluster head node. Method 2: Client - Server Mode (Wheeler: Reverse Connection) This process allows you to connect to wheeler service node. This process requires to know your localhost IP address \"local_host_IP\". Check your firewall setting if you are having firewall connectivity issues. Terminal 1: SSH to Wheeler Accessing Wheeler and requesting 2 nodes with 8 cores each. ssh username@wheeler.alliance.unm.edu qsub -I -l nodes=2:ppn=8 NOTE: Wait until wheeler assigns you two compute nodes. Load ParaView Module module load paraview/5.11.0-RC1 Run ParaView PVServer on the Compute Nodes mpiexec -np 16 pvserver --mpi --force-offscreen-rendering --rc --client-host=local_host_IP Opening ParaView 5.4.1 and Setup Server Configuration Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Wheeler RC Server Type: \"Client / Server (Reverse Connection)\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the qdel command on the cluster head node.","title":"Paraview Wheeler"},{"location":"paraview/#how-to-remote-visualization-using-paraview-in-parallel","text":"ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView's batch processing capabilities. ParaView was developed to analyze extremely large datasets using distributed memory computing resources. It can be run on supercomputers to analyze datasets of petascale size.","title":"How to: Remote Visualization using ParaView in parallel."},{"location":"paraview/#overview-paraview-connection-and-documentation","text":"The process to connecto to ParaView is, in one terminal you will ask Wheeler or Gibbs to give you compute nodes, where you will run the ParaView server. Once the ParaView server is listening for connections, you will open an ssh tunnel in another terminal window (This process is from your local computer to one of the compute nodes you were assigned). Then, you will tell the ParaView client on your computer to connect to the tunnel and so to the compute nodes at CARC, where it will perform the rendering. ** NOTE: Be sure that the ParaView version installed on your local computer matches the same one that is installed on ether Wheeler or Gibbs cluster. Gibbs's ParaView version is 5.9.1, Wheeler's version is 5.11.0-RC1 and for Hopper Cluster Information, click here. Downloads: https://www.paraview.org/download/ ParaView 5.9.1 Guide: https://docs.paraview.org/en/v5.9.1/ Paraview 5.11.0-RC1 Guide: https://docs.paraview.org/en/v5.11.0/UsersGuide/index.html Wiki Page: https://www.paraview.org/Wiki/ParaView","title":"Overview: Paraview Connection and Documentation"},{"location":"paraview/#method-1-client-server-mode-gibbs-direct-connection","text":"The most common approach to use ParaView on Gibbs is through the Client-Server mode support by ParaView, which requires an installation of ParaView on your local computer. This is a two-step process, requesting a compute node via SSH and opening an SSH tunnel to Gibbs's service node. The following examples assume you are using the Gibbs cluster.","title":"Method 1: Client - Server Mode (Gibbs Direct Connection)"},{"location":"paraview/#terminal-1-ssh-to-gibbs","text":"Accessing Gibbs and requesting one nodes with four cores. NOTE: Wait until Gibbs assigns you a compute node. ssh username@gibbs.alliance.unm.edu qsub -I -l nodes=1:ppn=4","title":"Terminal 1: SSH to Gibbs"},{"location":"paraview/#loading-paraview-module-591-in-gibbs","text":"module load paraview-5.9.1-gcc-10.2.0-enwua53","title":"Loading ParaView Module 5.9.1 in Gibbs"},{"location":"paraview/#run-paraview-pvserver-on-compute-nodes","text":"mpiexec -np 4 pvserver --mpi --use-offscreen-rendering --server-port=11111","title":"Run ParaView PVServer on Compute Nodes"},{"location":"paraview/#terminal-2-ssh-tunnel-to-a-gibbs-compute-node","text":"Note: this step is from your local computer to Gibbs's compute node. Make sure to replace the \"compute_node_name\" from the bash command below to one of the compute nodes you were assigned by qsub. Example \"gibbs18\" or \"gibbs20\". ssh -L 11111:compute_node_name:11111 username@gibbs.alliance.unm.edu","title":"Terminal 2: SSH Tunnel to a Gibbs Compute node"},{"location":"paraview/#opening-paraview-591-and-setup-server-configuration","text":"File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Gibbs Server Type: \"Client / Server\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the qdel command on the cluster head node.","title":"Opening ParaView 5.9.1 and Setup Server Configuration"},{"location":"paraview/#method-2-client-server-mode-wheeler-reverse-connection","text":"This process allows you to connect to wheeler service node. This process requires to know your localhost IP address \"local_host_IP\". Check your firewall setting if you are having firewall connectivity issues.","title":"Method 2: Client - Server Mode (Wheeler: Reverse Connection)"},{"location":"paraview/#terminal-1-ssh-to-wheeler","text":"Accessing Wheeler and requesting 2 nodes with 8 cores each. ssh username@wheeler.alliance.unm.edu qsub -I -l nodes=2:ppn=8 NOTE: Wait until wheeler assigns you two compute nodes.","title":"Terminal 1: SSH to Wheeler"},{"location":"paraview/#load-paraview-module","text":"module load paraview/5.11.0-RC1","title":"Load ParaView Module"},{"location":"paraview/#run-paraview-pvserver-on-the-compute-nodes","text":"mpiexec -np 16 pvserver --mpi --force-offscreen-rendering --rc --client-host=local_host_IP","title":"Run ParaView PVServer on the Compute Nodes"},{"location":"paraview/#opening-paraview-541-and-setup-server-configuration","text":"Note: To Verify, Client - Server setup, go to \"View\" and select \"Memory Inspector\" File --> Connect On the \"Choose Server Configuration\" window: Click on \"Add Server\" Name: Wheeler RC Server Type: \"Client / Server (Reverse Connection)\" Port: 11111 Click on \"Configure\" Startup Type: Manual Click on \"Save\" NOTE: When you are finished make sure to end the interactive job on the compute nodes. You can do this by exiting \"Exit\" the compute node or the qdel command on the cluster head node.","title":"Opening ParaView 5.4.1 and Setup Server Configuration"},{"location":"password_reset/","text":"Reset Your Password To reset your password, you can use the link here After entering your CARC username, you can follow the prompts and reset your password. You can also log in with the above link to find other information about your CARC account, see which groups you are a part of, activate two-factor authentication, and add SSH keys to your account. This quickbyte was validated on 5/21/2024","title":"Reset Your Password"},{"location":"password_reset/#reset-your-password","text":"To reset your password, you can use the link here After entering your CARC username, you can follow the prompts and reset your password. You can also log in with the above link to find other information about your CARC account, see which groups you are a part of, activate two-factor authentication, and add SSH keys to your account. This quickbyte was validated on 5/21/2024","title":"Reset Your Password"},{"location":"pbs-torque/","text":"PBS/TORQUE Portable Batch System (PBS) is a computer software which performs job (a unit of work or unit of execution) computational resource allocation in an HPC center. PBS is often used in conjunction with UNIX cluster environments, i.e., software modules. There are multiple different versions of software that utilize PBS architecture, including TORQUE, which is the flavor of PBS we use at CARC. Terascale Open-source Resource and QUEue manager (TORQUE) is a job scheduler/resource manager that employs PBS. Jobs can be run either interactively or as a submitted PVS batch script that is run non-interactively and subsequently controlled through TORQUE. In both cases resources are requested and jobs submitted through TORQUE which then places your request into a queue. At CARC, all batch jobs are submitted through the machine\u2019s head node via the PBS/TORQUE resource manager and are scheduled through the Maui scheduler. PBS Batch Scripts To submit jobs at CARC you submit a PBS batch script to the TORQUE resource manager. This PBS script starts by telling TORQUE what kind of resources you are requesting for your job. These lines in your script start with #PBS followed by flags that specify things like wall time, nodes, and processors requested. To get a complete list of options available type man qsub from the command prompt when logged in to a CARC machine. After your PBS instructions to TORQUE, you then load your software modules (refer to the help page \u2018Managing software modules\u2019 for more information\u2019) followed by software specific instructions. All PBS batch scripts take this same basic structure for job submission. For some example scripts refer to the help page \u2018Example PBS Scripts\u2019 to help you get started with computing at CARC.","title":"PBS/TORQUE"},{"location":"pbs-torque/#pbstorque","text":"Portable Batch System (PBS) is a computer software which performs job (a unit of work or unit of execution) computational resource allocation in an HPC center. PBS is often used in conjunction with UNIX cluster environments, i.e., software modules. There are multiple different versions of software that utilize PBS architecture, including TORQUE, which is the flavor of PBS we use at CARC. Terascale Open-source Resource and QUEue manager (TORQUE) is a job scheduler/resource manager that employs PBS. Jobs can be run either interactively or as a submitted PVS batch script that is run non-interactively and subsequently controlled through TORQUE. In both cases resources are requested and jobs submitted through TORQUE which then places your request into a queue. At CARC, all batch jobs are submitted through the machine\u2019s head node via the PBS/TORQUE resource manager and are scheduled through the Maui scheduler.","title":"PBS/TORQUE"},{"location":"pbs-torque/#pbs-batch-scripts","text":"To submit jobs at CARC you submit a PBS batch script to the TORQUE resource manager. This PBS script starts by telling TORQUE what kind of resources you are requesting for your job. These lines in your script start with #PBS followed by flags that specify things like wall time, nodes, and processors requested. To get a complete list of options available type man qsub from the command prompt when logged in to a CARC machine. After your PBS instructions to TORQUE, you then load your software modules (refer to the help page \u2018Managing software modules\u2019 for more information\u2019) followed by software specific instructions. All PBS batch scripts take this same basic structure for job submission. For some example scripts refer to the help page \u2018Example PBS Scripts\u2019 to help you get started with computing at CARC.","title":"PBS Batch Scripts"},{"location":"pbs2slurm/","text":"Conversion of PBS script to Slurm script Most of the machines at CARC uses PBS/TORQUE for scheduling jobs in HPC. However, Taos uses the Simple Linux Utility for Resource Management, or SLURM, for scheduling jobs. Slurm is a little different from PBS in terms of syntax, commands used for resource allocation, job submission and monitoring, and setting of environment variables. Detailed documentation of Slurm can be found in this link . For submitting a job on Taos, you have to submit a Slurm script. If you already have a PBS script, it can be easily converted to Slurm without worrying about the technical details of Slurm. More details of submitting a PBS and Slurm can be found in their quickbytes links. Conversion of PBS to Slurm For this purpose, we can use the cheet sheet tables below. The first table lists the most commonly used commands in PBS for submitting and monitoring jobs. PBS Command Slurm Command Command definition qsub \\ sbatch \\ Submit \\ to the queue qsub -I \\ salloc \\ Requesting interactive job qstat -u \\ squeue -u \\ Status of jobs submitted by \\ qstat -f \\ scontrol show job \\<job-id> Display details of \\ qdel \\ scancel \\ Delete the listed \\ pbsnodes \\ sinfo Display all nodes with their information Now let's take a look at the commands used for resource allocation which goes into the PBS/SLURM script. In both cases, the script has to be initialized by the shell interpreter. Bash shell can be initialized in both PBS and SLURM by #!/bin/bash/ The resource allocation in PBS is precceded by #PBS and #SBATCH in SLURM. Let's look at various commands used for allocating resources. PBS Command Slurm Command Command definition -N --job-name= \\ Name of the job to submit -l procs= \\ --ntasks= \\ N processes to run -l nodes=a:ppn=b --ntasks= \\ a*b processes to run -l walltime=\\ --time=\\ Maximum time required to finish the job -l mem=\\ --mem = \\ Memory required per node -l M \\ --mail-user=\\ Email ID for sending the job alerts to the user -l m \\<a,b,e> --mail-type=\\ Sending email alerts at different situations -o \\ --output=\\ Name of the output file -e \\ --error=\\ Name of the file to write out the error/warning during execution -j oe Default in Slurm Merge output and error files Let's look at different ways to set environment variable within the PBS/Slurm job. PBS Variable Slurm Variable Variable definition $PBS_O_HOST $SLURM_SUBMIT_HOST Hostname from which job was submitted $PBS_JOBID $SLURM_JOB_ID ID of the job sumitted $PBS_O_WORKDIR $SLURM_SUBMIT_DIR Name of the directory from which job was submited cat $PBS_NODEFILE $SLURM_JOB_NODELIST In PBS, a file containing allocated nodes/hostnames. In Slurm, a variable containing allocated nodes/hostnames. Now using the commands in the above tables, we can easily convert a PBS script to a Slurm script. First let us look at a sample PBS script to run a python script test.py. #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -l walltime=01:00:00 #PBS -N test #PBS -o test_out #PBS -e test_error #PBS -m bae #PBS -M user@unm.edu cd $PBS_O_WORKDIR/ python test.py The corresponding Slurm script for the above PBS script can be written as #!/bin/bash #SBATCH --ntasks=1 #SBATCH --time=01:00:00 #SBATCH --job-name=test #SBATCH --output=test.out #SBATCH --error=test.err #SBATCH --mail-type=BEGIN,FAIL,END #SBATCH --mail-user=user@unm.edu cd $SLURM_SUBMIT_DIR/ python test.py","title":"Converting PBS to Slurm"},{"location":"pbs2slurm/#conversion-of-pbs-script-to-slurm-script","text":"Most of the machines at CARC uses PBS/TORQUE for scheduling jobs in HPC. However, Taos uses the Simple Linux Utility for Resource Management, or SLURM, for scheduling jobs. Slurm is a little different from PBS in terms of syntax, commands used for resource allocation, job submission and monitoring, and setting of environment variables. Detailed documentation of Slurm can be found in this link . For submitting a job on Taos, you have to submit a Slurm script. If you already have a PBS script, it can be easily converted to Slurm without worrying about the technical details of Slurm. More details of submitting a PBS and Slurm can be found in their quickbytes links.","title":"Conversion of PBS script to Slurm script"},{"location":"pbs2slurm/#conversion-of-pbs-to-slurm","text":"For this purpose, we can use the cheet sheet tables below. The first table lists the most commonly used commands in PBS for submitting and monitoring jobs. PBS Command Slurm Command Command definition qsub \\ sbatch \\ Submit \\ to the queue qsub -I \\ salloc \\ Requesting interactive job qstat -u \\ squeue -u \\ Status of jobs submitted by \\ qstat -f \\ scontrol show job \\<job-id> Display details of \\ qdel \\ scancel \\ Delete the listed \\ pbsnodes \\ sinfo Display all nodes with their information Now let's take a look at the commands used for resource allocation which goes into the PBS/SLURM script. In both cases, the script has to be initialized by the shell interpreter. Bash shell can be initialized in both PBS and SLURM by #!/bin/bash/ The resource allocation in PBS is precceded by #PBS and #SBATCH in SLURM. Let's look at various commands used for allocating resources. PBS Command Slurm Command Command definition -N --job-name= \\ Name of the job to submit -l procs= \\ --ntasks= \\ N processes to run -l nodes=a:ppn=b --ntasks= \\ a*b processes to run -l walltime=\\ --time=\\ Maximum time required to finish the job -l mem=\\ --mem = \\ Memory required per node -l M \\ --mail-user=\\ Email ID for sending the job alerts to the user -l m \\<a,b,e> --mail-type=\\ Sending email alerts at different situations -o \\ --output=\\ Name of the output file -e \\ --error=\\ Name of the file to write out the error/warning during execution -j oe Default in Slurm Merge output and error files Let's look at different ways to set environment variable within the PBS/Slurm job. PBS Variable Slurm Variable Variable definition $PBS_O_HOST $SLURM_SUBMIT_HOST Hostname from which job was submitted $PBS_JOBID $SLURM_JOB_ID ID of the job sumitted $PBS_O_WORKDIR $SLURM_SUBMIT_DIR Name of the directory from which job was submited cat $PBS_NODEFILE $SLURM_JOB_NODELIST In PBS, a file containing allocated nodes/hostnames. In Slurm, a variable containing allocated nodes/hostnames. Now using the commands in the above tables, we can easily convert a PBS script to a Slurm script. First let us look at a sample PBS script to run a python script test.py. #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -l walltime=01:00:00 #PBS -N test #PBS -o test_out #PBS -e test_error #PBS -m bae #PBS -M user@unm.edu cd $PBS_O_WORKDIR/ python test.py The corresponding Slurm script for the above PBS script can be written as #!/bin/bash #SBATCH --ntasks=1 #SBATCH --time=01:00:00 #SBATCH --job-name=test #SBATCH --output=test.out #SBATCH --error=test.err #SBATCH --mail-type=BEGIN,FAIL,END #SBATCH --mail-user=user@unm.edu cd $SLURM_SUBMIT_DIR/ python test.py","title":"Conversion of PBS to Slurm"},{"location":"pbs_scripts2/","text":"Example PBS Scripts PBS Hello World: This example uses the \"Bash\u201d shell to print a simple \u201cHello World\u201d message. Note that it specifies the shell with the -S option. If you do not specify a shell using the -S option (either inside the PBS script or as an argument to qsub ), then your default shell will be used. Since this script uses built-in Bash commands no software modules are loaded. That will be introduced in the next PBS script. #!/bin/bash ## Introduction for writing a PBS script ## The next lines specify what resources you are requesting. ## Starting with 1 node, 8 processors per node, and 2 hours of walltime. ## Setup your qsub flags #PBS -l walltime=2:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_job #PBS -M myemailaddress@unm.edu #PBS -m bae ## All other instructions to TORQUE are here as well and are preceded by a single #, note that normal comments can also be preceded by a single # ## Specify the shell to be bash #PBS -S /bin/bash ## Change to directory the PBS script was submitted from cd $PBS_O_WORKDIR ## Print out a hello message indicating the host this is running on export THIS_HOST=$(hostname) echo Hello World from host $THIS_HOST #################################################### Note that the ppn (processors per node) value must always be less than or equal to the number of physical cores available on each node of the system on which you are running and is machine specific. For example, on Wheeler, ppn should be <=8, however, we recommend you always request the maximum number of processors per node to avoid multiple jobs on one node that have to share memory. For more information see CARC systems information. Multi-processor example script: #!/bin/bash ## Introductory Example ## Copyright (c) 2018 ## The Center for Advanced Research Computing ## at The University of New Mexico #################################################### ## Setup your qsub flags #PBS -l walltime=2:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_job #PBS -M myemailaddress@unm.edu #PBS -m bae # load the environment module to use OpenMPI built with the Intel compilers module load openmpi-3.1.1-intel-18.0.2-hlc45mq # Change to the directory where the PBS script was submitted from cd $PBS_O_WORKDIR # run the command \"hostname\" on ever CPU. Hostname prints the name of the computer is it running on. # $PBS_NP is the total number of CPUs requested. In this case 1 nodes x 8 CPUS per node = 8 mpirun -np $PBS_NP hostname #################################################### Multi-node example script: #!/bin/bash ## Introductory Example ## Copyright (c) 2018 ## The Center for Advanced Research Computing ## at The University of New Mexico #################################################### ## Setup your qsub flags #PBS -l walltime=2:00:00 #PBS -l nodes=4:ppn=8 #PBS -N my_job #PBS -M myemailaddress@unm.edu #PBS -m bae # Change to directory the PBS script was submitted from cd $PBS_O_WORKDIR # load the environment module to use OpenMPI built with the Intel compilers module load openmpi-3.1.1-intel-18.0.2-hlc45mq # print out a hello message from each of the processors on this host # run the command \"hostname\" on ever CPU. Hostname prints the name of the computer is it running on. # $PBS_NP is the total number of CPUs requested. In this case 4 nodes x 8 CPUS per node = 32 # Since we are running on multiple nodes (computers) we have to tell mpirun the names of the nodes we were assigned. Those names are in $PBS_NODEFILE. mpirun -np $PBS_NP -machinefile $PBS_NODEFILE hostname ###################################################","title":"Example PBS Scripts"},{"location":"pbs_scripts2/#example-pbs-scripts","text":"","title":"Example PBS Scripts"},{"location":"pbs_scripts2/#pbs-hello-world","text":"This example uses the \"Bash\u201d shell to print a simple \u201cHello World\u201d message. Note that it specifies the shell with the -S option. If you do not specify a shell using the -S option (either inside the PBS script or as an argument to qsub ), then your default shell will be used. Since this script uses built-in Bash commands no software modules are loaded. That will be introduced in the next PBS script. #!/bin/bash ## Introduction for writing a PBS script ## The next lines specify what resources you are requesting. ## Starting with 1 node, 8 processors per node, and 2 hours of walltime. ## Setup your qsub flags #PBS -l walltime=2:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_job #PBS -M myemailaddress@unm.edu #PBS -m bae ## All other instructions to TORQUE are here as well and are preceded by a single #, note that normal comments can also be preceded by a single # ## Specify the shell to be bash #PBS -S /bin/bash ## Change to directory the PBS script was submitted from cd $PBS_O_WORKDIR ## Print out a hello message indicating the host this is running on export THIS_HOST=$(hostname) echo Hello World from host $THIS_HOST #################################################### Note that the ppn (processors per node) value must always be less than or equal to the number of physical cores available on each node of the system on which you are running and is machine specific. For example, on Wheeler, ppn should be <=8, however, we recommend you always request the maximum number of processors per node to avoid multiple jobs on one node that have to share memory. For more information see CARC systems information.","title":"PBS Hello World:"},{"location":"pbs_scripts2/#multi-processor-example-script","text":"#!/bin/bash ## Introductory Example ## Copyright (c) 2018 ## The Center for Advanced Research Computing ## at The University of New Mexico #################################################### ## Setup your qsub flags #PBS -l walltime=2:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_job #PBS -M myemailaddress@unm.edu #PBS -m bae # load the environment module to use OpenMPI built with the Intel compilers module load openmpi-3.1.1-intel-18.0.2-hlc45mq # Change to the directory where the PBS script was submitted from cd $PBS_O_WORKDIR # run the command \"hostname\" on ever CPU. Hostname prints the name of the computer is it running on. # $PBS_NP is the total number of CPUs requested. In this case 1 nodes x 8 CPUS per node = 8 mpirun -np $PBS_NP hostname ####################################################","title":"Multi-processor example script:"},{"location":"pbs_scripts2/#multi-node-example-script","text":"#!/bin/bash ## Introductory Example ## Copyright (c) 2018 ## The Center for Advanced Research Computing ## at The University of New Mexico #################################################### ## Setup your qsub flags #PBS -l walltime=2:00:00 #PBS -l nodes=4:ppn=8 #PBS -N my_job #PBS -M myemailaddress@unm.edu #PBS -m bae # Change to directory the PBS script was submitted from cd $PBS_O_WORKDIR # load the environment module to use OpenMPI built with the Intel compilers module load openmpi-3.1.1-intel-18.0.2-hlc45mq # print out a hello message from each of the processors on this host # run the command \"hostname\" on ever CPU. Hostname prints the name of the computer is it running on. # $PBS_NP is the total number of CPUs requested. In this case 4 nodes x 8 CPUS per node = 32 # Since we are running on multiple nodes (computers) we have to tell mpirun the names of the nodes we were assigned. Those names are in $PBS_NODEFILE. mpirun -np $PBS_NP -machinefile $PBS_NODEFILE hostname ###################################################","title":"Multi-node example script:"},{"location":"psmc_quickbyte/","text":"Pairwise Sequentially Markovian Coalescent (PSMC) The pairwise sequentially Markovian coalescent model is a popular method of leveraging single high-quality diploid genomes to infer the demographic history of a lineage over thousands to hundreds of thousands of years. It can be a great exploratory tool for genomic data, and can help you understand and generate biogeographic and evolutionary hypotheses. It leverages heterozygosity information to estimate local times of most recent common ancestor across the genome, which is then used to reconstruct demographic \"stairway plots\". It is implemented by the authors of the original paper on GitHub , but the documentation is difficult to understand and the method of calling variants is a bit outdated. Here I'll outline a simple pipeline for generating a consensus sequence using high coverage (>18x) reads and sites with a depth of at least 10 reads and a reference genome. Then, I'll go over how to run PSMC and perform bootstrapping. Note that nothing but the bootstrapping can work across different nodes, so if you find bootstrapping takes too long you can run it as a seperate job with more nodes. The runtime and resource requirements will vary based on genome, but the only step that can work across nodes is bootstrap generation. Wheeler will work for some samples, but nodes with more cores may be needed for others due to wall time limits. Installation and setup Due to the inavailibility of PSMC on conda, high number of included utilities, and ease of installing locally, we suggest you install PSMC as shown below. You can install it anywhere, but we'll assume it's in the working directory you're using to run everything: git clone https://github.com/lh3/psmc.git cd psmc make cd utils make cd ../.. Then we'll install some dependencies with conda as below. conda create -n psmc-env -c bioconda -c conda-forge picard bcftools samtools bwa At the top of any scripts used for this, change to your working directory and activate the environment like: cd $SLURM_SUBMIT_DIR # if using PBS, 'cd $PBS_O_WORKDIR' module load miniconda3/4.8.2-pilj eval \"$(conda shell.bash hook)\" conda activate psmc-env Then, we'll make a subdirectory for future bootstraps with \"mkdir boot\". If you are testing multiple references for consistency, I suggest putting everything for each reference in its own subdirectory. Generating input Generating input is essentially a simplified version of GATK's widely used pipeline using bcftools to leverage its simplicity and ability to generate a consensus FASTA file with heterozygosity. First, reads are alligned to the reference with BWA. Then, bcftools' mpileup and call are used to obtain variant calls, which are filtered using bcftools' view command. Finally, a consensus sequence is generated and converted to a \"PSMC FASTA\" for use in PSMC itself. Code for running these steps is below, assuming your reference is \"reference.fa\" and read files are called \"sample_R1.fastq.gz\" and \"sample_R2.fastq.gz\". # Align with BWA bwa index -p reference reference.fa # assuming you have 8 threads, not that this .sam file will be huge, and we'll remove it at the end bwa mem -t 8 reference sample_R1.fastq.gz sample_R2.fastq.gz > bwa_alignment.sam # convert .sam to .bam samtools view -S -b bwa_alignment.sam > unsorted_alignment.bam # sort the .bam file picard SortSam I=unsorted_alignment.bam O=sorted_alignment.bam SORT_ORDER=coordinate # remove old files, comment out if you want to keep them to troubleshoot rm bwa_alignment.sam rm unsorted_alignment.bam # pipeline combining bcftool's mpileup and call (consensus mode) using 8 threads, then samtools's vcfutils.pl # the latter filters variants with a depth less than 10 or greater than 50, and those with quality score under 30 bcftools mpileup -Q 30 -q 30 -Ovu -f reference.fa sorted_alignment.bam --threads 8 | \\ bcftools call -c --threads 8 | \\ vcfutils.pl vcf2fq -d 10 -D 50 -Q 30 > variant_consensus.fq # generate PSMC input ./psmc/utils/fq2psmcfa variant_consensus.fq > psmc_input.psmcfa Running PSMC A basic PSMC run is outlined in the GitHub, but optimization is required to avoid overfitting problems. You can start with the default parameters and work from there. The first flag, -N, is the number of iterations to use. The second, -t, is the maximum coalescence time permitted. The third, -r, is the initial theta/rho ratio (effectively the per-base mutation rate divided by the per-base recombination rate). The final, -p, is the most complicated, but effectively delimits the number of time intervals per parameter. Using, say, '-p \"25*1\"' would mean 25 parameters each spanning one time interval. The example has 28 parameters spanning 64 intervals (one spanning 4, 25 spanning 2, one spanning 4, then one spanning 6). The more recent intervals are on the left side of the expression. You can start with a set-up like below: ./psmc/psmc -N25 -t15 -r5 -p \"4+25*2+4+6\" -o sample.psmc sample.psmcfa However, before we run bootstrapping and plot this, we want to make sure out runs are optimized. Optimization After our first run, we will view the end of the resulting .psmc file, which will look something like the table below (with many lines skipped for brevity). Each line represents a time interval, and the fifth column (i.e. 3651.038295 in the first row) the number of recombinations that occur. You want to make sure this number is at least 20 in all intervals, although the GitHub says 10 is sufficient. An example of a good result is below, keep repeating until you get a sufficient result: RS 0 0.000000 1.282590 3651.038295 0.004395 0.002875 RS 1 0.005884 0.570172 8631.229543 0.010389 0.010062 RS 2 0.012114 0.786536 6563.160557 0.007900 0.006705 ... RS 75 7.181803 0.765618 56.699641 0.000068 0.000064 RS 76 7.610256 0.765618 34.277873 0.000041 0.000038 RS 77 8.063918 0.765618 43.762351 0.000053 0.000049 An example of what worked well for me is below, with a notably different maximum coalescent time (biggest difference for me) and free parameters spanning shorter time intervals toward the present: ./psmc/psmc -N25 -t10 -r5 -p \"8*1+30*2+4+6\" -o sample.psmc sample.psmcfa Next, we want to bootstrap our results and plot them. Bootstrapping and plotting Bootstrapping is a step that can fortunately be run in parallel. There's no set number of bootstraps needed, but we'll do 50 here. We'll run this with GNU parallel. First, you'll need to load a parallel module. Then, you'll run what is essentially the same command as before, but with the -b flag: module load parallel/20190222-wsvg parallel '$SLURM_SUBMIT_DIR/psmc/psmc -N25 -t10 -r5 -b -p \"8*1+30*2+4+6\" \\ -o $SLURM_SUBMIT_DIR/boot/sample_r{}.psmc $SLURM_SUBMIT_DIR/sample.psmcfa' \\ ::: $(seq 50) cat sample.psmc boot/sample_r*.psmc > sample_combined.psmc Unfortunately, perl issues make it difficult to plot the output on CARC, so we suggest you transfer the 'sample_combined.psmc' file to your personal computer. Then, install psmc on your computer like above, and run the command as follows. The -g flag specifies your organism's generation time, and -u the per-generation mutation rate (no scientific notation sadly): /path/to/psmc/utils/psmc_plot.pl -p -g 2.2 -u 0.00000000506 bootstrapped_plot sample_combined.psmc Then you'll get an estimate of your focal population's detailed demographic history! Citations Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. Bioinformatics, 25(14), 1754\u20131760. https://doi.org/10.1093/bioinformatics/btp324 Li, H., & Durbin, R. (2011). Inference of human population history from individual whole-genome sequences. Nature, 475(7357), 493\u2013496. https://doi.org/10.1038/nature10231 Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., \u2026 Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. Bioinformatics, 25(16), 2078\u20132079. https://doi.org/10.1093/bioinformatics/btp352 Picard toolkit. (2019). Broad Institute, GitHub Repository. https://doi.org/http://broadinstitute.github.io/picard/ Tange, O. (2018). GNU Parallel 2018. https://doi.org/10.5281/ZENODO.1146014","title":"Single genome demographic history with PSMC"},{"location":"psmc_quickbyte/#pairwise-sequentially-markovian-coalescent-psmc","text":"The pairwise sequentially Markovian coalescent model is a popular method of leveraging single high-quality diploid genomes to infer the demographic history of a lineage over thousands to hundreds of thousands of years. It can be a great exploratory tool for genomic data, and can help you understand and generate biogeographic and evolutionary hypotheses. It leverages heterozygosity information to estimate local times of most recent common ancestor across the genome, which is then used to reconstruct demographic \"stairway plots\". It is implemented by the authors of the original paper on GitHub , but the documentation is difficult to understand and the method of calling variants is a bit outdated. Here I'll outline a simple pipeline for generating a consensus sequence using high coverage (>18x) reads and sites with a depth of at least 10 reads and a reference genome. Then, I'll go over how to run PSMC and perform bootstrapping. Note that nothing but the bootstrapping can work across different nodes, so if you find bootstrapping takes too long you can run it as a seperate job with more nodes. The runtime and resource requirements will vary based on genome, but the only step that can work across nodes is bootstrap generation. Wheeler will work for some samples, but nodes with more cores may be needed for others due to wall time limits.","title":"Pairwise Sequentially Markovian Coalescent (PSMC)"},{"location":"psmc_quickbyte/#installation-and-setup","text":"Due to the inavailibility of PSMC on conda, high number of included utilities, and ease of installing locally, we suggest you install PSMC as shown below. You can install it anywhere, but we'll assume it's in the working directory you're using to run everything: git clone https://github.com/lh3/psmc.git cd psmc make cd utils make cd ../.. Then we'll install some dependencies with conda as below. conda create -n psmc-env -c bioconda -c conda-forge picard bcftools samtools bwa At the top of any scripts used for this, change to your working directory and activate the environment like: cd $SLURM_SUBMIT_DIR # if using PBS, 'cd $PBS_O_WORKDIR' module load miniconda3/4.8.2-pilj eval \"$(conda shell.bash hook)\" conda activate psmc-env Then, we'll make a subdirectory for future bootstraps with \"mkdir boot\". If you are testing multiple references for consistency, I suggest putting everything for each reference in its own subdirectory.","title":"Installation and setup"},{"location":"psmc_quickbyte/#generating-input","text":"Generating input is essentially a simplified version of GATK's widely used pipeline using bcftools to leverage its simplicity and ability to generate a consensus FASTA file with heterozygosity. First, reads are alligned to the reference with BWA. Then, bcftools' mpileup and call are used to obtain variant calls, which are filtered using bcftools' view command. Finally, a consensus sequence is generated and converted to a \"PSMC FASTA\" for use in PSMC itself. Code for running these steps is below, assuming your reference is \"reference.fa\" and read files are called \"sample_R1.fastq.gz\" and \"sample_R2.fastq.gz\". # Align with BWA bwa index -p reference reference.fa # assuming you have 8 threads, not that this .sam file will be huge, and we'll remove it at the end bwa mem -t 8 reference sample_R1.fastq.gz sample_R2.fastq.gz > bwa_alignment.sam # convert .sam to .bam samtools view -S -b bwa_alignment.sam > unsorted_alignment.bam # sort the .bam file picard SortSam I=unsorted_alignment.bam O=sorted_alignment.bam SORT_ORDER=coordinate # remove old files, comment out if you want to keep them to troubleshoot rm bwa_alignment.sam rm unsorted_alignment.bam # pipeline combining bcftool's mpileup and call (consensus mode) using 8 threads, then samtools's vcfutils.pl # the latter filters variants with a depth less than 10 or greater than 50, and those with quality score under 30 bcftools mpileup -Q 30 -q 30 -Ovu -f reference.fa sorted_alignment.bam --threads 8 | \\ bcftools call -c --threads 8 | \\ vcfutils.pl vcf2fq -d 10 -D 50 -Q 30 > variant_consensus.fq # generate PSMC input ./psmc/utils/fq2psmcfa variant_consensus.fq > psmc_input.psmcfa","title":"Generating input"},{"location":"psmc_quickbyte/#running-psmc","text":"A basic PSMC run is outlined in the GitHub, but optimization is required to avoid overfitting problems. You can start with the default parameters and work from there. The first flag, -N, is the number of iterations to use. The second, -t, is the maximum coalescence time permitted. The third, -r, is the initial theta/rho ratio (effectively the per-base mutation rate divided by the per-base recombination rate). The final, -p, is the most complicated, but effectively delimits the number of time intervals per parameter. Using, say, '-p \"25*1\"' would mean 25 parameters each spanning one time interval. The example has 28 parameters spanning 64 intervals (one spanning 4, 25 spanning 2, one spanning 4, then one spanning 6). The more recent intervals are on the left side of the expression. You can start with a set-up like below: ./psmc/psmc -N25 -t15 -r5 -p \"4+25*2+4+6\" -o sample.psmc sample.psmcfa However, before we run bootstrapping and plot this, we want to make sure out runs are optimized.","title":"Running PSMC"},{"location":"psmc_quickbyte/#optimization","text":"After our first run, we will view the end of the resulting .psmc file, which will look something like the table below (with many lines skipped for brevity). Each line represents a time interval, and the fifth column (i.e. 3651.038295 in the first row) the number of recombinations that occur. You want to make sure this number is at least 20 in all intervals, although the GitHub says 10 is sufficient. An example of a good result is below, keep repeating until you get a sufficient result: RS 0 0.000000 1.282590 3651.038295 0.004395 0.002875 RS 1 0.005884 0.570172 8631.229543 0.010389 0.010062 RS 2 0.012114 0.786536 6563.160557 0.007900 0.006705 ... RS 75 7.181803 0.765618 56.699641 0.000068 0.000064 RS 76 7.610256 0.765618 34.277873 0.000041 0.000038 RS 77 8.063918 0.765618 43.762351 0.000053 0.000049 An example of what worked well for me is below, with a notably different maximum coalescent time (biggest difference for me) and free parameters spanning shorter time intervals toward the present: ./psmc/psmc -N25 -t10 -r5 -p \"8*1+30*2+4+6\" -o sample.psmc sample.psmcfa Next, we want to bootstrap our results and plot them.","title":"Optimization"},{"location":"psmc_quickbyte/#bootstrapping-and-plotting","text":"Bootstrapping is a step that can fortunately be run in parallel. There's no set number of bootstraps needed, but we'll do 50 here. We'll run this with GNU parallel. First, you'll need to load a parallel module. Then, you'll run what is essentially the same command as before, but with the -b flag: module load parallel/20190222-wsvg parallel '$SLURM_SUBMIT_DIR/psmc/psmc -N25 -t10 -r5 -b -p \"8*1+30*2+4+6\" \\ -o $SLURM_SUBMIT_DIR/boot/sample_r{}.psmc $SLURM_SUBMIT_DIR/sample.psmcfa' \\ ::: $(seq 50) cat sample.psmc boot/sample_r*.psmc > sample_combined.psmc Unfortunately, perl issues make it difficult to plot the output on CARC, so we suggest you transfer the 'sample_combined.psmc' file to your personal computer. Then, install psmc on your computer like above, and run the command as follows. The -g flag specifies your organism's generation time, and -u the per-generation mutation rate (no scientific notation sadly): /path/to/psmc/utils/psmc_plot.pl -p -g 2.2 -u 0.00000000506 bootstrapped_plot sample_combined.psmc Then you'll get an estimate of your focal population's detailed demographic history!","title":"Bootstrapping and plotting"},{"location":"psmc_quickbyte/#citations","text":"Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. Bioinformatics, 25(14), 1754\u20131760. https://doi.org/10.1093/bioinformatics/btp324 Li, H., & Durbin, R. (2011). Inference of human population history from individual whole-genome sequences. Nature, 475(7357), 493\u2013496. https://doi.org/10.1038/nature10231 Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., \u2026 Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. Bioinformatics, 25(16), 2078\u20132079. https://doi.org/10.1093/bioinformatics/btp352 Picard toolkit. (2019). Broad Institute, GitHub Repository. https://doi.org/http://broadinstitute.github.io/picard/ Tange, O. (2018). GNU Parallel 2018. https://doi.org/10.5281/ZENODO.1146014","title":"Citations"},{"location":"resource_limits/","text":"Storage Policy Home directories have a soft limit of 100GB and a hard limit of 200GB. Once you exceed the soft limit we will ask you to reduce your usage. You will not be able to write data beyond the hard limit. Project space is limited to 250 GB. Scratch storage is limited to 1 TB (2 TB on Xena). Center-wide project scratch space is limited to 1 TB and user scratch is limited to 100G (/carc/scratch). To purchase additional storage please see our pricing spreadsheet . The 'quotas' command shows your quota usage. Compute Usage Policy To ensure that all research and class projects get their fair share of the clusters and to prevent any one group from using a disproportionate amount of resources, we utilize Slurm\u2019s built-in job accounting and fairshare system. The cluster is a limited resource and Fairshare allows us to ensure everyone gets a fair opportunity to use it regardless of how big or small the group is. Your compute resource allocation is shared among everyone in the slurm account you select. For more on slurm accounts see this quickbyte: Slurm Accounting Note that your slurm account is not the same as your CARC login account. To see the predicted start time of your job based on your fairshare score, use the following command: squeue --start --job <job_id> Xena Configuration Queue GPU Bigmem Debug Number of Processors 192 128 8 Number of Nodes 12 (singleGPU) 4 (dualGPU) 1 2 Processors per Node 16 32 4 Walltime(H:M:S) 48:00:00 48:00:00 04:00:00 Memory Limit 60 Gb (singleGPU and dualGPU) 1 Tb (bigmem-1TB) 3 Tb (bigmem-3TB) 60 Gb Wheeler Configuration Queue: Default Debug Number of Processors 400 32 Number of Nodes 50 4 Processors per Node 8 8 Walltime(H:M:S) 48:00:00 04:00:00 Memory Limit 44 Gb 44 Gb Hopper Configuration Queue: General Debug Condo Private Number of Processors 64 8 192 Determined by queue owner Number of Nodes 2 2 6 Processors per Node 32 8 32 Walltime(H:M:S) 48:00:00 04:00:00 48:00:00 Memory Limits 90 Gb 90 Gb","title":"Resource Limits"},{"location":"resource_limits/#storage-policy","text":"Home directories have a soft limit of 100GB and a hard limit of 200GB. Once you exceed the soft limit we will ask you to reduce your usage. You will not be able to write data beyond the hard limit. Project space is limited to 250 GB. Scratch storage is limited to 1 TB (2 TB on Xena). Center-wide project scratch space is limited to 1 TB and user scratch is limited to 100G (/carc/scratch). To purchase additional storage please see our pricing spreadsheet . The 'quotas' command shows your quota usage.","title":"Storage Policy"},{"location":"resource_limits/#compute-usage-policy","text":"To ensure that all research and class projects get their fair share of the clusters and to prevent any one group from using a disproportionate amount of resources, we utilize Slurm\u2019s built-in job accounting and fairshare system. The cluster is a limited resource and Fairshare allows us to ensure everyone gets a fair opportunity to use it regardless of how big or small the group is. Your compute resource allocation is shared among everyone in the slurm account you select. For more on slurm accounts see this quickbyte: Slurm Accounting Note that your slurm account is not the same as your CARC login account. To see the predicted start time of your job based on your fairshare score, use the following command: squeue --start --job <job_id>","title":"Compute Usage Policy"},{"location":"resource_limits/#xena-configuration","text":"Queue GPU Bigmem Debug Number of Processors 192 128 8 Number of Nodes 12 (singleGPU) 4 (dualGPU) 1 2 Processors per Node 16 32 4 Walltime(H:M:S) 48:00:00 48:00:00 04:00:00 Memory Limit 60 Gb (singleGPU and dualGPU) 1 Tb (bigmem-1TB) 3 Tb (bigmem-3TB) 60 Gb","title":"Xena Configuration"},{"location":"resource_limits/#wheeler-configuration","text":"Queue: Default Debug Number of Processors 400 32 Number of Nodes 50 4 Processors per Node 8 8 Walltime(H:M:S) 48:00:00 04:00:00 Memory Limit 44 Gb 44 Gb","title":"Wheeler Configuration"},{"location":"resource_limits/#hopper-configuration","text":"Queue: General Debug Condo Private Number of Processors 64 8 192 Determined by queue owner Number of Nodes 2 2 6 Processors per Node 32 8 32 Walltime(H:M:S) 48:00:00 04:00:00 48:00:00 Memory Limits 90 Gb 90 Gb","title":"Hopper Configuration"},{"location":"running_matlab_jobs/","text":"Running MATLAB Jobs at CARC MATLAB MATLAB is a powerful software environment and language especially for matrix manipulation and calculations, whence the name __MAT__rix__LAB__oratory. if you are unfamiliar with MATLAB please visit there website for more information at this link . For a list of which versions of MATLAB are available on a certain system use the command module avail matlab on the head node. MATLAB is installed on all CARC systems, however, it can only be accessed on the compute nodes and is not available on the head node. In order to run MATLAB jobs at CARC it is necessary to call the MATLAB software in batch mode to run on a script/program supplied by the user. Batch mode Many users are likely familiar with using the MATLAB GUI to run their code either in the interactive console or by launching a script. However, it is often useful to run a Matlab program from the command line in batch mode, as though it were a shell script. MATLAB can be run non-interactively using a built in function called batch mode. All MATLAB jobs should be run using batch mode when using CARC systems. Submitting a job with batch mode Say you have a small program named my_program.m that generates a 3x3 matrix of random numbers and writes the results to a .csv file. Your code for such a program might look like this: %generate a matrix of random numbers of dimension 3x3 rmatrix=rand(3); fname='randnums.csv'; csvwrite(fname,rmatrix); quit This program can be submitted using batch mode with the command matlab -r my_program . In this command the -r flag is telling MATLAB to run your script my_program.m in batch mode. Notice that when calling a script with batch mode you leave off the extension .m . This command works if you are running on a local machine, however, when running jobs at CARC you need to submit your job to the job scheduler using a PBS script. For those unfamiliar with PBS scripts explanations and example scripts can be found here . To submit our MATLAB program to the Wheeler job scheduler we would put our batch command in a PBS script similar to the one below: #!/bin/bash # Commands specific to the job scheduler requesting resources, naming your job, and setting up email alerts regarding job status. #PBS -l walltime=1:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_matlab_job #PBS -m abe #PBS -M my_email.@unm.edu # Change to the directory that you submitted your PBS script from. cd $PBS_O_WORKDIR # Loading the MATLAB software module. The specific module name is system dependent. module load matlab/R2017a # Calling MATLAB in batch mode to run your program. matlab -nojvm -nodisplay -r my_program > /dev/null There are a couple of additions to our batch mode command when calling MATLAB on our program. The -nojvm flag turns off Java Virtual Machine since this is only necessary when running the MATLAB GUI. The -nodisplay turns off all graphical output from MATLAB since we do not support visual display at CARC. Remember that the -r flag is telling MATLAB to run in batch mode, it must immediately precede the name of your program, \"my_program\" in this case. At the end of our command we then redirect what would normally be written to stdout , or your MATLAB console when running interactively, to a special file called null . We do this because MATLAB normally writes all output to stdout which is stored in memory (RAM). If your program generates enough output to stdout this can overload local memory and crash the compute node that your program is running on. If you wish to keep what is printed to stdout you can redirect to a file instead using the following syntax and replacing the name with whatever you would like to call your file: matlab -nojvm -nodisplay -r my_program > my_program.output Now that you have your program and your PBS script you can submit your job to the job scheduler using the qsub command: qsub my_matlab_job.pbs","title":"Running MATLAB jobs at CARC"},{"location":"running_matlab_jobs/#running-matlab-jobs-at-carc","text":"","title":"Running MATLAB Jobs at CARC"},{"location":"running_matlab_jobs/#matlab","text":"MATLAB is a powerful software environment and language especially for matrix manipulation and calculations, whence the name __MAT__rix__LAB__oratory. if you are unfamiliar with MATLAB please visit there website for more information at this link . For a list of which versions of MATLAB are available on a certain system use the command module avail matlab on the head node. MATLAB is installed on all CARC systems, however, it can only be accessed on the compute nodes and is not available on the head node. In order to run MATLAB jobs at CARC it is necessary to call the MATLAB software in batch mode to run on a script/program supplied by the user.","title":"MATLAB"},{"location":"running_matlab_jobs/#batch-mode","text":"Many users are likely familiar with using the MATLAB GUI to run their code either in the interactive console or by launching a script. However, it is often useful to run a Matlab program from the command line in batch mode, as though it were a shell script. MATLAB can be run non-interactively using a built in function called batch mode. All MATLAB jobs should be run using batch mode when using CARC systems.","title":"Batch mode"},{"location":"running_matlab_jobs/#submitting-a-job-with-batch-mode","text":"Say you have a small program named my_program.m that generates a 3x3 matrix of random numbers and writes the results to a .csv file. Your code for such a program might look like this: %generate a matrix of random numbers of dimension 3x3 rmatrix=rand(3); fname='randnums.csv'; csvwrite(fname,rmatrix); quit This program can be submitted using batch mode with the command matlab -r my_program . In this command the -r flag is telling MATLAB to run your script my_program.m in batch mode. Notice that when calling a script with batch mode you leave off the extension .m . This command works if you are running on a local machine, however, when running jobs at CARC you need to submit your job to the job scheduler using a PBS script. For those unfamiliar with PBS scripts explanations and example scripts can be found here . To submit our MATLAB program to the Wheeler job scheduler we would put our batch command in a PBS script similar to the one below: #!/bin/bash # Commands specific to the job scheduler requesting resources, naming your job, and setting up email alerts regarding job status. #PBS -l walltime=1:00:00 #PBS -l nodes=1:ppn=8 #PBS -N my_matlab_job #PBS -m abe #PBS -M my_email.@unm.edu # Change to the directory that you submitted your PBS script from. cd $PBS_O_WORKDIR # Loading the MATLAB software module. The specific module name is system dependent. module load matlab/R2017a # Calling MATLAB in batch mode to run your program. matlab -nojvm -nodisplay -r my_program > /dev/null There are a couple of additions to our batch mode command when calling MATLAB on our program. The -nojvm flag turns off Java Virtual Machine since this is only necessary when running the MATLAB GUI. The -nodisplay turns off all graphical output from MATLAB since we do not support visual display at CARC. Remember that the -r flag is telling MATLAB to run in batch mode, it must immediately precede the name of your program, \"my_program\" in this case. At the end of our command we then redirect what would normally be written to stdout , or your MATLAB console when running interactively, to a special file called null . We do this because MATLAB normally writes all output to stdout which is stored in memory (RAM). If your program generates enough output to stdout this can overload local memory and crash the compute node that your program is running on. If you wish to keep what is printed to stdout you can redirect to a file instead using the following syntax and replacing the name with whatever you would like to call your file: matlab -nojvm -nodisplay -r my_program > my_program.output Now that you have your program and your PBS script you can submit your job to the job scheduler using the qsub command: qsub my_matlab_job.pbs","title":"Submitting a job with batch mode"},{"location":"singularity-markdown-version/","text":"Docker and Singularity Introduction Have you ever installed software on your own system to process data, but then find it is not so easy to setup the same environment on a CARC cluster when you want to process larger data sets? CARC staff will work with you to get your software running, but occasionally we run across fundamental incompatibilities. Docker allows us to circumvent those incompatibilities. Docker and Singularity are free tools that allow you to run software with dependencies on its environment that are difficult to satisfy natively on the CARC clusters. For example, your software might depend on a particular flavor of linux that differs from the one installed on the cluster you want to use. Perhaps your software needs to make global changes to the operating system that are incompatible with the needs of the center or other users. Docker allows us to deploy software with these requirements. In this guide you will learn how to setup your software to run in a docker container, and how to convert the container to singularity and run it at CARC. The R software package party. What is Docker? Docker allows you to setup a virtual environment for your program that you can configure however you like. You can choose the underlying operating system (so long as it is linux based), install any packages you need, and make any other changes the root user could make. The custom OS environment is stored in a docker image file that can be loaded by any docker installation. Docker has online repositories with many pre-built images available to download. When loaded, a docker image provides a container that allows the software to have complete control of its environment without effecting the host operating system. If you have used a virtual machine (such as virtualbox, or vmware) the description of docker images will sound familiar. The main difference is that docker just containerizes the environment but still uses the host operating system's kernel. This means there is very little performance impact. What is Singularity? Singularity is able to convert and run docker images into a secure form suitable for multiuser machines such as the CARC clusters. Installing Docker Docker runs on Microsoft Windows, Apple OS X, and Linux. All the systems at CARC are linux based so you will need to create and configure a linux based docker container. OS X and Linux will allow you to do this. Windows 10 can as well but you need to install the Windows Subsystem for Linux and a linux distribution. Docker can be downloaded from www.docker.com. Running Docker To start docker under Windows or OS X just launch the program like you would any other. To start the docker daemon under linux enter: sudo systemctl start docker Creating a Docker Image Pulling an existing base docker image For this example we are going to use an existing docker image that has the CentOS linux distribution preconfigured. We could just as easily use Ubuntu by replacing \"centos\" with \"ubuntu\" below. docker pull centos Using default tag: latest Trying to pull repository docker.io/library/centos ... latest: Pulling from docker.io/library/centos a02a4930cb5d: Pull complete Digest:sha256:184e5f35598e333bfa7de10d8fb1cebb5ee4df5bc0f970bf2b1e7c7345136426 Status: Downloaded newer image for docker.io/centos:latest We can now issue a command that runs inside the docker container and returns output. Below we run the ls -la command to get a file listing inside the container. docker run centos ls anaconda-post.log bin dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var Customizing the docker image Now we can install the program we need inside the docker container and any dependencies it needs. It is convenient to do this interactively inside the container. Configuring the container in interactive mode allows us to do everything we need to do before saving the changes to a docker image. If we issued commands one-by-one with run we would lose the state of the container after each command. docker run -it centos [root@a264d0f7b8c9 /]# The long string after @ is the name of the container we are running in. Notice it is not the same name as the docker image. In a new terminal we can list the running docker containers and see that our container is running. docker container list CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a264d0f7b8c9 centos \"/bin/bash\" About a minute ago Up About a minute striking_mahavira Example: Installing the \"party\" R package As an example, we will install party. Party is an R package for recursive partitioning. It produces regression trees for machine learning. But you can install whatever tools you wish. This is the beauty of docker, it gives you control over your environment. [root@a264d0f7b8c9 /]# yum update [root@a264d0f7b8c9 /]# yum -y install epel-release [root@a264d0f7b8c9 /]# yum install R Once R has finished installing: [root@a264d0f7b8c9 /]# R R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. >install.packages(\"party\",deps=YES) Commit customised container to image file Once party has finished installing, our Docker container is ready. Now we need to commit those changes to a docker image file so we can load the image that has R and party installed for future use. Exit R: [root@a264d0f7b8c9 /]# > quit() Exit the docker container: In general: [root@a264d0f7b8c9 /]# exit $ sudo docker commit For our example: [root@a264d0f7b8c9 /]# exit $ sudo docker commit a264d0f7b8c9 r_party Now we have a docker image saved. Running Commands using the Docker Image Now that we have a docker image we can run it on our own machines as before with: docker run r_party ls But now we can also execute R commands such as: docker run r_party Rscript For our party example we can write the following R script and save it to a file called test_party.R: library(\"party\") set.seed(290875) ### honest (i.e., out-of-bag) cross-classification of ### true vs. predicted classes data(\"mammoexp\", package = \"TH.data\") table(mammoexp$ME, predict(cforest(ME ~ ., data = mammoexp, control = cforest_unbiased(ntree = 50)), OOB = TRUE)) ### fit forest to censored response if (require(\"TH.data\") && require(\"survival\")) { data(\"GBSG2\", package = \"TH.data\") bst <- cforest(Surv(time, cens) ~ ., data = GBSG2, control = cforest_unbiased(ntree = 50)) ### estimate conditional Kaplan-Meier curves treeresponse(bst, newdata = GBSG2[1:2,], OOB = TRUE) party:::prettytree(bst@ensemble[[1]], names(bst@data@get(\"input\"))) And run it using the docker image with: $ docker run -v < path to test_party.R folder > :/mnt r_party Rscript /mnt/test_party.R Singularity Converting Docker Images to Singularty Images Once we are happy with the docker image we created we will convert it to a singularity image so we can use it on the CARC clusters. We do the conversion by using a docker image provided to us that contains the necessary tools: $ docker pull singularityware/docker2singularity $ docker run -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/output --privileged -t --rm singularityware/docker2singularity r_party This will produce a singularity image in the /tmp directory that we can upload to a CARC cluster using our favorite file transfer program. Running a Singularity Image at CARC First login to a CARC cluster head node. Next we will load the singularity module: $ module load singularity-2.4.1-intel-17.0.4-sjwoqj4 $ The syntax for executing Singularity images are similar to those we used for docker: $ singularity exec r_party.simg Rscript test_party.R Mapping Directories $ singularity -B $PBS_O_WORKDIR:/mnt exec r_party.simg Rscript test_party.R The Above command maps the directory that that PBS script was submitted from, $PBS_O_WORKDIR , to the /mnt location within the r_party.simg singularity image and then executes the test_party.R script. Version Issues If you recieve the following when you try to execute your singularity image there may be a mismatch between the version you used to create the image and the singularity version you loaded at CARC. ERROR : Failed to mount image in (read only): Invalid argument ABORT : Retval = 255 We have several modules with different singularity versions. Enter the following command to see them all: module avail singularity","title":"Docker and Singularity"},{"location":"singularity-markdown-version/#docker-and-singularity","text":"","title":"Docker and Singularity"},{"location":"singularity-markdown-version/#introduction","text":"Have you ever installed software on your own system to process data, but then find it is not so easy to setup the same environment on a CARC cluster when you want to process larger data sets? CARC staff will work with you to get your software running, but occasionally we run across fundamental incompatibilities. Docker allows us to circumvent those incompatibilities. Docker and Singularity are free tools that allow you to run software with dependencies on its environment that are difficult to satisfy natively on the CARC clusters. For example, your software might depend on a particular flavor of linux that differs from the one installed on the cluster you want to use. Perhaps your software needs to make global changes to the operating system that are incompatible with the needs of the center or other users. Docker allows us to deploy software with these requirements. In this guide you will learn how to setup your software to run in a docker container, and how to convert the container to singularity and run it at CARC. The R software package party.","title":"Introduction"},{"location":"singularity-markdown-version/#what-is-docker","text":"Docker allows you to setup a virtual environment for your program that you can configure however you like. You can choose the underlying operating system (so long as it is linux based), install any packages you need, and make any other changes the root user could make. The custom OS environment is stored in a docker image file that can be loaded by any docker installation. Docker has online repositories with many pre-built images available to download. When loaded, a docker image provides a container that allows the software to have complete control of its environment without effecting the host operating system. If you have used a virtual machine (such as virtualbox, or vmware) the description of docker images will sound familiar. The main difference is that docker just containerizes the environment but still uses the host operating system's kernel. This means there is very little performance impact.","title":"What is Docker?"},{"location":"singularity-markdown-version/#what-is-singularity","text":"Singularity is able to convert and run docker images into a secure form suitable for multiuser machines such as the CARC clusters.","title":"What is Singularity?"},{"location":"singularity-markdown-version/#installing-docker","text":"Docker runs on Microsoft Windows, Apple OS X, and Linux. All the systems at CARC are linux based so you will need to create and configure a linux based docker container. OS X and Linux will allow you to do this. Windows 10 can as well but you need to install the Windows Subsystem for Linux and a linux distribution. Docker can be downloaded from www.docker.com.","title":"Installing Docker"},{"location":"singularity-markdown-version/#running-docker","text":"To start docker under Windows or OS X just launch the program like you would any other. To start the docker daemon under linux enter: sudo systemctl start docker","title":"Running Docker"},{"location":"singularity-markdown-version/#creating-a-docker-image","text":"","title":"Creating a Docker Image"},{"location":"singularity-markdown-version/#pulling-an-existing-base-docker-image","text":"For this example we are going to use an existing docker image that has the CentOS linux distribution preconfigured. We could just as easily use Ubuntu by replacing \"centos\" with \"ubuntu\" below. docker pull centos Using default tag: latest Trying to pull repository docker.io/library/centos ... latest: Pulling from docker.io/library/centos a02a4930cb5d: Pull complete Digest:sha256:184e5f35598e333bfa7de10d8fb1cebb5ee4df5bc0f970bf2b1e7c7345136426 Status: Downloaded newer image for docker.io/centos:latest We can now issue a command that runs inside the docker container and returns output. Below we run the ls -la command to get a file listing inside the container. docker run centos ls anaconda-post.log bin dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var","title":"Pulling an existing base docker image"},{"location":"singularity-markdown-version/#customizing-the-docker-image","text":"Now we can install the program we need inside the docker container and any dependencies it needs. It is convenient to do this interactively inside the container. Configuring the container in interactive mode allows us to do everything we need to do before saving the changes to a docker image. If we issued commands one-by-one with run we would lose the state of the container after each command. docker run -it centos [root@a264d0f7b8c9 /]# The long string after @ is the name of the container we are running in. Notice it is not the same name as the docker image. In a new terminal we can list the running docker containers and see that our container is running. docker container list CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a264d0f7b8c9 centos \"/bin/bash\" About a minute ago Up About a minute striking_mahavira","title":"Customizing the docker image"},{"location":"singularity-markdown-version/#example-installing-the-party-r-package","text":"As an example, we will install party. Party is an R package for recursive partitioning. It produces regression trees for machine learning. But you can install whatever tools you wish. This is the beauty of docker, it gives you control over your environment. [root@a264d0f7b8c9 /]# yum update [root@a264d0f7b8c9 /]# yum -y install epel-release [root@a264d0f7b8c9 /]# yum install R Once R has finished installing: [root@a264d0f7b8c9 /]# R R version 3.5.0 (2018-04-23) -- \"Joy in Playing\" Copyright (C) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. >install.packages(\"party\",deps=YES)","title":"Example: Installing the \"party\" R package"},{"location":"singularity-markdown-version/#commit-customised-container-to-image-file","text":"Once party has finished installing, our Docker container is ready. Now we need to commit those changes to a docker image file so we can load the image that has R and party installed for future use. Exit R: [root@a264d0f7b8c9 /]# > quit() Exit the docker container: In general: [root@a264d0f7b8c9 /]# exit $ sudo docker commit For our example: [root@a264d0f7b8c9 /]# exit $ sudo docker commit a264d0f7b8c9 r_party Now we have a docker image saved.","title":"Commit customised container to image file"},{"location":"singularity-markdown-version/#running-commands-using-the-docker-image","text":"Now that we have a docker image we can run it on our own machines as before with: docker run r_party ls But now we can also execute R commands such as: docker run r_party Rscript For our party example we can write the following R script and save it to a file called test_party.R: library(\"party\") set.seed(290875) ### honest (i.e., out-of-bag) cross-classification of ### true vs. predicted classes data(\"mammoexp\", package = \"TH.data\") table(mammoexp$ME, predict(cforest(ME ~ ., data = mammoexp, control = cforest_unbiased(ntree = 50)), OOB = TRUE)) ### fit forest to censored response if (require(\"TH.data\") && require(\"survival\")) { data(\"GBSG2\", package = \"TH.data\") bst <- cforest(Surv(time, cens) ~ ., data = GBSG2, control = cforest_unbiased(ntree = 50)) ### estimate conditional Kaplan-Meier curves treeresponse(bst, newdata = GBSG2[1:2,], OOB = TRUE) party:::prettytree(bst@ensemble[[1]], names(bst@data@get(\"input\"))) And run it using the docker image with: $ docker run -v < path to test_party.R folder > :/mnt r_party Rscript /mnt/test_party.R","title":"Running Commands using the Docker Image"},{"location":"singularity-markdown-version/#singularity","text":"","title":"Singularity"},{"location":"singularity-markdown-version/#converting-docker-images-to-singularty-images","text":"Once we are happy with the docker image we created we will convert it to a singularity image so we can use it on the CARC clusters. We do the conversion by using a docker image provided to us that contains the necessary tools: $ docker pull singularityware/docker2singularity $ docker run -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/output --privileged -t --rm singularityware/docker2singularity r_party This will produce a singularity image in the /tmp directory that we can upload to a CARC cluster using our favorite file transfer program.","title":"Converting Docker Images to Singularty Images"},{"location":"singularity-markdown-version/#running-a-singularity-image-at-carc","text":"First login to a CARC cluster head node. Next we will load the singularity module: $ module load singularity-2.4.1-intel-17.0.4-sjwoqj4 $ The syntax for executing Singularity images are similar to those we used for docker: $ singularity exec r_party.simg Rscript test_party.R","title":"Running a Singularity Image at CARC"},{"location":"singularity-markdown-version/#mapping-directories","text":"$ singularity -B $PBS_O_WORKDIR:/mnt exec r_party.simg Rscript test_party.R The Above command maps the directory that that PBS script was submitted from, $PBS_O_WORKDIR , to the /mnt location within the r_party.simg singularity image and then executes the test_party.R script.","title":"Mapping Directories"},{"location":"singularity-markdown-version/#version-issues","text":"If you recieve the following when you try to execute your singularity image there may be a mismatch between the version you used to create the image and the singularity version you loaded at CARC. ERROR : Failed to mount image in (read only): Invalid argument ABORT : Retval = 255 We have several modules with different singularity versions. Enter the following command to see them all: module avail singularity","title":"Version Issues"},{"location":"slurm_accounting/","text":"Introduction to Slurm Accounting at CARC What is Slurm accounting? Slurm accounting allows us to track resource usage at CARC and use that information to make scheduling decisions. When a job is run, the resources (CPUs/RAM) used by that job are tracked and associated with an account. When a cluster is under heavy utilization, accounts with less historical resource usage will have their priority increased over accounts with higher historical resource usage. Accounting Commands At CARC, accounts correspond to CARC projects. The account name is the project ID. myaccounts will show you all of the accounts you are a member of. Example: [tredfear@xena]:~ $ myaccounts User Account ---------- ---------- tredfear systems tredfear 1000001 To view more information about an account, you can use the command sacctmgr show account <account_name> . Example: [tredfear@xena]:~ $ sacctmgr show account 1000001 Account Descr Org ---------- -------------------- -------------------- 1000001 hpc@unm sys admin download The Org column displays the username of the PI associated with the project. Choosing an account to associate with a job There are 3 ways that a job can be associated with an account and they are checked in the following order: 1. --account= specified in job submission script 2. ~/.default_slurm_account 3. Most recent account you are a part of If any of these are specified, the scheduler will not proceed further down the list. --account --account can be specified in either an srun command or in a sbatch submission script using the #SBATCH directive. This option takes precedence over every other method of account specification. ~/.default_slurm_account If there exists a home file in your home directory called .default_slurm_account containing a valid account name, that account will be used any time you do not specifically use --account . Most recent project If you do not specify --account in your submission script and there is no ~/.default_slurm_account (or the account specified therein is not valid), your resource usage will be attributed to the newest project you are a part of. You can find out which project this is with the command sacctmgr show user <username> . Example: [tredfear@xena]:~ $ sacctmgr show user tredfear User Def Acct Admin ---------- ---------- --------- tredfear 1000001 None You can then use sacctmgr show account as specified above to find more information about this account. More resources: http://www.physik.uni-leipzig.de/wiki/files/slurm_summary.pdf https://slurm.schedmd.com/fair_tree.html https://slurm.schedmd.com/sacctmgr.html https://slurm.schedmd.com/sacct.html","title":"Intro to Slurm accounting at CARC"},{"location":"slurm_accounting/#introduction-to-slurm-accounting-at-carc","text":"","title":"Introduction to Slurm Accounting at CARC"},{"location":"slurm_accounting/#what-is-slurm-accounting","text":"Slurm accounting allows us to track resource usage at CARC and use that information to make scheduling decisions. When a job is run, the resources (CPUs/RAM) used by that job are tracked and associated with an account. When a cluster is under heavy utilization, accounts with less historical resource usage will have their priority increased over accounts with higher historical resource usage.","title":"What is Slurm accounting?"},{"location":"slurm_accounting/#accounting-commands","text":"At CARC, accounts correspond to CARC projects. The account name is the project ID. myaccounts will show you all of the accounts you are a member of. Example: [tredfear@xena]:~ $ myaccounts User Account ---------- ---------- tredfear systems tredfear 1000001 To view more information about an account, you can use the command sacctmgr show account <account_name> . Example: [tredfear@xena]:~ $ sacctmgr show account 1000001 Account Descr Org ---------- -------------------- -------------------- 1000001 hpc@unm sys admin download The Org column displays the username of the PI associated with the project.","title":"Accounting Commands"},{"location":"slurm_accounting/#choosing-an-account-to-associate-with-a-job","text":"There are 3 ways that a job can be associated with an account and they are checked in the following order: 1. --account= specified in job submission script 2. ~/.default_slurm_account 3. Most recent account you are a part of If any of these are specified, the scheduler will not proceed further down the list.","title":"Choosing an account to associate with a job"},{"location":"slurm_accounting/#-account","text":"--account can be specified in either an srun command or in a sbatch submission script using the #SBATCH directive. This option takes precedence over every other method of account specification.","title":"--account"},{"location":"slurm_accounting/#default_slurm_account","text":"If there exists a home file in your home directory called .default_slurm_account containing a valid account name, that account will be used any time you do not specifically use --account .","title":"~/.default_slurm_account"},{"location":"slurm_accounting/#most-recent-project","text":"If you do not specify --account in your submission script and there is no ~/.default_slurm_account (or the account specified therein is not valid), your resource usage will be attributed to the newest project you are a part of. You can find out which project this is with the command sacctmgr show user <username> . Example: [tredfear@xena]:~ $ sacctmgr show user tredfear User Def Acct Admin ---------- ---------- --------- tredfear 1000001 None You can then use sacctmgr show account as specified above to find more information about this account. More resources: http://www.physik.uni-leipzig.de/wiki/files/slurm_summary.pdf https://slurm.schedmd.com/fair_tree.html https://slurm.schedmd.com/sacctmgr.html https://slurm.schedmd.com/sacct.html","title":"Most recent project"},{"location":"spark/","text":"Spark Apache Spark is a parallel/distributed computing environment designed for processing large, complex data sets. Compared to other large-scale parallel programming environments (e.g. MPI), it\u2019s generally easier to program, especially for data-centric workloads. You basically write a Java, Scala, or Python program that coordinates parallel data processing by a large number of worker processes. In addition, Spark has extensions analyzing streaming data sets (Spark Streaming) and using machine learning techniques for analyzing data (Spark ML), making it ideal for many modern data-oriented research questions. In this tutorial, we discuss how to run Spark on the UNM CARC cluster systems, using a Python-oriented example of analyzing Chicago crime data drawn from an excellent series of Spark Python examples provided by datascienceplus.com. Once you understand the basic Spark model, the main complications for running these examples on CARC are starting your personal Spark instance on which to run and then using that. The remainder of this page discusses on these issues, and shows how to set up a full-featured Spark environment on CARC systems using anaconda to run some of the examples from the tutorial linked above. All of the files from this tutorial are available at https://lobogit.unm.edu/CARC/tutorials/-/tree/master/spark. The Basic Spark Model Spark is what is referred to as a Single-Program-Multiple-Data (SPMD) parallel programming model. You write a single Spark program in the language of your choice that runs on a single master node (we\u2019ll be using Python in this example), and that master orchestrates multiple parallel workers, each of which work on a subset of the overall data. Key to this is understanding and managing which (and how much) data resides on the master and which data is split across the workers in what Spark refers to as Resilient Distributed Datasets (RDDs). Note that while RDDs are the main abstraction Spark uses under the covers to manage distributed data, you will more likely use and manipulate Spark DataFrames, a higher-level version of RDDs akin to a large distributed table or spreadsheet. The first thing to understand is that RDDs and DataFrames are spread among the parallel nodes and workers and they don\u2019t change. Instead, you create a sequence of transformations that Spark executes when necessary to generate new DataFrames and RDDs from existing data (e.g. files) and RDDs/DataFrames, and you execute actions on RDDs to extract data from the workers back to your master program. It is important to remember that transformations are run in parallel across the Spark cluster, while actions generate data housed only on the master! As a result, actions that generate large amounts of data can cause your program to run out of memory, and activities that work on the results of actions do not happen in parallel! The Canonical Simple Example: Word count in Python ```# Create my Spark connection in Python spark = SparkSession.builder.appName(\"WordCount\").getOrCreate() Create an RDD from a text file lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0]) Generate a new RDD (via the map transformations run in parallel on workers) that splits the lines into words words = lines.flatMap(lambda x: x.split(' ')) And then another transformation that counts up the number of times each word appears counts = words.map(lambda x: (x, 1)).reduceByKey(add) Execute an action that puts the resulting list of counts in an array of (word,count) pairs in the master program output = counts.collect() Run (serially on the master only, since this isn\u2019t Spark operations!) code to generate output. for (word, count) in output: print(\"%s: %i\" % (word, count)) ## Running Spark at CARC When you read Spark tutorials, you\u2019ll see that it is generally run on dedicated per-user spark clusters, with the \u201cspark-submit\u201d script or \u201cpyspark\u201d commands used to talk to your cluster. You can do the same thing at CARC, but you\u2019ll need to do a few things first: Have a CARC project/account, for which you can find directions on the CARC website: Log into a CARC shared cluster (we\u2019ll be using wheeler.carc.unm.edu) using ssh or a similar tool (XXX This should link to our documentation on how to do that) Use the PBS queueing system to request a set of nodes for your use (XXX This should link to our PBS information) Run pbs-spark-submit on those nodes to start your personal spark instance (see below) Talk to the spark workers using pyspark or spark-submit Basically, pbs-submit-spark starts your personal Spark cluster (and optionally one job) using batch queueing information from PBS queueing system, and after that\u2019s done you can use pyspark to talk to interactively or submit-spark to submit new Spark jobs to it. pbs-spark-submit is available at https://lobogit.unm.edu/CARC/tutorials/-/blob/master/spark/pbs-spark-submit. ## Simple CARC Spark Examples 1. Create an interactive 4-node cluster on Wheeler for 1 hour ```bridges@wheeler-sn[519]> ls big.txt conf/ pbs-spark-submit* small.txt bridges@wheeler-sn[520]> cd bridges@wheeler-sn[521]> qsub -I -l nodes=4:ppn=8 -l walltime=01:00:00 qsub: waiting for job 14319.wheeler-sn.alliance.unm.edu to start qsub: job 14319.wheeler-sn.alliance.unm.edu ready Job 14319.wheeler-sn.alliance.unm.edu running on nodes: wheeler240 wheeler235 wheeler234 wheeler233 Wheeler Portable Batch System Prologue Job Id: 14319.wheeler-sn.alliance.unm.edu Username: bridges Job 14319.wheeler-sn.alliance.unm.edu running on nodes: wheeler240 wheeler235 wheeler234 wheeler233 prologue running on host: wheeler240 Start the Spark cluster on your wheeler nodes ```bridges@wheeler240[500]> cd /wheeler/scratch/bridges/spark bridges@wheeler240[501]> module load spark-2.1.0-gcc-4.8.5-lifnga6 bridges@wheeler240[502]> export SPARK_HOME=$SPARK_DIR bridges@wheeler240[503]> ./pbs-spark-submit SPARK_MASTER_HOST=wheeler240 SPARK_MASTER_PORT=7077 bridges@wheeler240[504]> pyspark --master spark://wheeler240:7077 3. Run Spark interactively ```>>> rddread = sc.textFile(\"small.txt\") >>> rddread.takeSample(False, 10, 2) [u'a savage place as holy and enchanted ', u'huge fragments vaulted like rebounding hail ', u' floated midway on the waves ', u'and all should cry beware beware ', u'and from this chasm with ceaseless turmoil seething ', u'enfolding sunny spots of greenery ', u'as if this earth in fast thick pants were breathing ', u'and drunk the milk of paradise', u'five miles meandering with a mazy motion ', u'where blossomed many an incensebearing tree '] >>> exit() bridges@wheeler240[505]> exit qsub: job 14319.wheeler-sn.alliance.unm.edu completed Spark jobs can be run in batch as well \u2013 The following PBS batch script would run the simple wordcount example from above, using the Wheeler scratch file system to store the data while it is being processed: ```# wordcount.pbs PBS -N wordcount PBS -l nodes=2:ppn=8 PBS -l walltime=00:05:00 SCRATCHDIR=/wheeler/scratch/$USER/spark module load spark-2.1.0-gcc-4.8.5-lifnga6 export SPARK_HOME=$SPARK_DIR cd $PBS_O_WORKDIR cp wordcount.py $SCRATCHDIR cp big.txt $SCRATCHDIR cp -r conf $SCRATCHDIR cd $SCRATCHDIR $PBS_O_WORKDIR/pbs-spark-submit wordcount.py $SCRATCHDIR/big.txt > wordcount.log cp wordcount.log $PBS_O_WORKDIR ## A More Involved Example To make the most of Spark, most programmers will want to do two things: use the DataFrames API and construct a more full-features Python programming environment in which to work and analyze data. While RDDs are the core Spark abstraction, the DataFrame API (available since Spark 2.0) is much more programmer friendly. In particular, DataFrames have columns and column operations, can import CSV and JSON data files, while still being distributed across the memory of all of the nodes in your Spark cluster. 16 Wheeler nodes can, for example, handle a 200GB table! Similarly, setting up a full-featured programming environment by using Anaconda will allow you to analyze and visualize your data much more effectively. ## Setting up a Full Python/Spark environment using Anaconda As mentioned above and on other CARC web pages, we suggest that users use Anaconda to manage their python environments. This applies to Python+Spark as well with one exception \u2013 you want to get spark (and pyspark) from the Spark package so that it matches the version of Spark on your system instead of from Anaconda. To create this environment, simply run the following two commands: ```wheeler-sn> module load anaconda3-4.4.0-gcc-4.8.5-ubgzkkv wheeler-sn> conda create \u2013n spark python=27 numpy scipy pandas matplotlib Once this is done, we just activate that environment and can add new things to it as needed after we launch our Spark cluster, to interactively look at a lot of data. First, we bring up Spark with this customized environment: ```wheeler-sn> qsub \u2013I \u2013l nodes=1:ppn=8 -l walltime=01:00:00 wheeler263> module load anaconda3-4.4.0-gcc-4.8.5-ubgzkkv wheeler263> source activate spark (spark) wheeler263> module load spark spark-2.1.0-gcc-4.8.5-lifnga6 (spark) wheeler263> export SPARK_HOME=$(SPARK_DIR) (spark) wheeler263> ./pbs-spark-submit (spark) wheeler263> pyspark \u2013-master spark://wheeler263:7077 Once we have this running, we can simply load and analyze our data, first converting it into Spark Parquet format so that we can load and save it more quickly later: ```>>> crimes_schema = ... # available in source code files >>> format = \"MM/dd/yyyy hh:mm:ss a\" >>> crimes = spark.read.format(\"csv\").options(header = 'true',dateFormat = format,timestampFormat = format).load(\"crimes.csv\", schema = crimes_schema) >>> count = crimes.count() >>> print(\"Imported CSV with {} crime records.\".format(count)) Imported CSV with 6614877 crime records >>> crimes.write.parquet(\"crimes.parquet\") From there, you can basically think of the resulting DataFrame as a SQL database or Excel sheet, turning the full power of Python and SQL loose on it, including computings averages, sorting or grouping it, adding columns, and doing proper SQL operations on it (selects, joins, pivots, etc.). For example, if you wanted to normalize the Latitudes in the table loaded above, you could the the following: ```>>> lat_mean = crimes.agg({\"Latitude\" : \"mean\"}).collect()[0][0] print(\"The mean latitude values is {}\".format(lat_mean)) The mean latitude values is 41.8419399743 df = crimes.withColumn(\"Lat(Norm)\",lat_mean - crimes[\"Latitude\"]) df.select([\"Latitude\", \"Lat(Norm)\"]).show(3) +------------+--------------------+ | Latitude| Lat(Norm)| +------------+--------------------+ |41.987319201|-0.14537922670000114| |41.868473957|-0.02653398270000...| |41.742667249| 0.09927272529999698| +------------+--------------------+ ## Visualization of Data from Python You can also use Pandas and Matplotlib and everything else on processed data, for example the following code generates a file which graphs Chicago crimes by month: ```import matplotlib as mpl mpl.use('PDF\u2019) # Live rendering over X11 requires ssh tunneling import matplotlib.pyplot as plt import pandas as pd # And then do things like compute domestic crimes by month from pyspark.sql.functions import month monthdf = df.withColumn(\"Month\",month(\"Date\")) monthCounts = monthdf.select(\"Month\").groupBy(\"Month\").count().collect() months = [item[0] for item in monthCounts] count = [item[1] for item in monthCounts] crimes_per_month = pd.DataFrame({\"month\":months, \"crime_count\": crimes_per_month = crimes_per_month.sort_values(by = \"month\") crimes_per_month.plot(figsize = (20,10), kind = \"line\", x = \"month\", y = \"crime_count\", color = \"red\", linewidth = 8, legend = False) plt.xlabel(\"Month\", fontsize = 18) plt.ylabel(\"Number of Crimes\", fontsize = 18) plt.title(\"Number of Crimes Per Month\", fontsize = 28) plt.xticks(size = 18) plt.yticks(size = 18) plt.savefig(\"crimes-by-month.pdf\") Similarly, this code generates a file which charts Chicago crime by location type: crime_location = crimes.groupBy(\"LocationDescription\").count().collect() location = [item[0] for item in crime_location] count = [item[1] for item in crime_location] crime_location = {\"location\" : location, \"count\": count} crime_location = pd.DataFrame(crime_location) crime_location = crime_location.sort_values(by = \"count\", ascending = False) crime_location = crime_location.iloc[:20] myplot = crime_location.plot(figsize =(20,20), kind=\"barh\", color=\"#b35900\", width = 0.8, x = \"location\", y = \"count\", legend = False) myplot.invert_yaxis() plt.xlabel(\"Number of crimes\", fontsize = 28) plt.ylabel(\"Crime Location\", fontsize = 28) plt.title(\"Number of Crimes By Location\", fontsize = 36) plt.xticks(size = 24) plt.yticks(size = 24) plt.savefig(\"crimes-by-location.pdf\") More Information In addition to all of this, there are a wide range of other things you can do with Spark. Spark Streaming (add link) augments RDDs with Dstreams, data batched into N second chunks with one RDD per batch. Spark can automatically monitor data dumped into files or directories or message queues and fill them into Dstreams. Twitter sentiment analysis is a common use case (e.g. combined with a Kafka queue that Spark Streaming pulls from) Spark ML (add link) provides a wide range of scalable regression, inference, clustering, and learning algorithms from Spark. Finally, we\u2019re working on an array of additional improvements on this at CARC in the long term, including making this accessible from Jupyter notebooks, and making it easier to interactively render data to your desktop (as opposed to into files like was done above). Keep an eye out on the CARC website for additional information as these features develop.","title":"Spark"},{"location":"spark/#spark","text":"Apache Spark is a parallel/distributed computing environment designed for processing large, complex data sets. Compared to other large-scale parallel programming environments (e.g. MPI), it\u2019s generally easier to program, especially for data-centric workloads. You basically write a Java, Scala, or Python program that coordinates parallel data processing by a large number of worker processes. In addition, Spark has extensions analyzing streaming data sets (Spark Streaming) and using machine learning techniques for analyzing data (Spark ML), making it ideal for many modern data-oriented research questions. In this tutorial, we discuss how to run Spark on the UNM CARC cluster systems, using a Python-oriented example of analyzing Chicago crime data drawn from an excellent series of Spark Python examples provided by datascienceplus.com. Once you understand the basic Spark model, the main complications for running these examples on CARC are starting your personal Spark instance on which to run and then using that. The remainder of this page discusses on these issues, and shows how to set up a full-featured Spark environment on CARC systems using anaconda to run some of the examples from the tutorial linked above. All of the files from this tutorial are available at https://lobogit.unm.edu/CARC/tutorials/-/tree/master/spark.","title":"Spark"},{"location":"spark/#the-basic-spark-model","text":"Spark is what is referred to as a Single-Program-Multiple-Data (SPMD) parallel programming model. You write a single Spark program in the language of your choice that runs on a single master node (we\u2019ll be using Python in this example), and that master orchestrates multiple parallel workers, each of which work on a subset of the overall data. Key to this is understanding and managing which (and how much) data resides on the master and which data is split across the workers in what Spark refers to as Resilient Distributed Datasets (RDDs). Note that while RDDs are the main abstraction Spark uses under the covers to manage distributed data, you will more likely use and manipulate Spark DataFrames, a higher-level version of RDDs akin to a large distributed table or spreadsheet. The first thing to understand is that RDDs and DataFrames are spread among the parallel nodes and workers and they don\u2019t change. Instead, you create a sequence of transformations that Spark executes when necessary to generate new DataFrames and RDDs from existing data (e.g. files) and RDDs/DataFrames, and you execute actions on RDDs to extract data from the workers back to your master program. It is important to remember that transformations are run in parallel across the Spark cluster, while actions generate data housed only on the master! As a result, actions that generate large amounts of data can cause your program to run out of memory, and activities that work on the results of actions do not happen in parallel!","title":"The Basic Spark Model"},{"location":"spark/#the-canonical-simple-example-word-count-in-python","text":"```# Create my Spark connection in Python spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()","title":"The Canonical Simple Example: Word count in Python"},{"location":"spark/#create-an-rdd-from-a-text-file","text":"lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])","title":"Create an RDD from a text file"},{"location":"spark/#generate-a-new-rdd-via-the-map-transformations-run-in-parallel-on-workers-that-splits-the-lines-into-words","text":"words = lines.flatMap(lambda x: x.split(' '))","title":"Generate a new RDD (via the map transformations run in parallel on workers) that splits the lines into words"},{"location":"spark/#and-then-another-transformation-that-counts-up-the-number-of-times-each-word-appears","text":"counts = words.map(lambda x: (x, 1)).reduceByKey(add)","title":"And then another transformation that counts up the number of times each word appears"},{"location":"spark/#execute-an-action-that-puts-the-resulting-list-of-counts-in-an-array-of-wordcount-pairs-in-the-master-program","text":"output = counts.collect()","title":"Execute an action that puts the resulting list of counts in an array of (word,count) pairs in the master program"},{"location":"spark/#run-serially-on-the-master-only-since-this-isnt-spark-operations-code-to-generate-output","text":"for (word, count) in output: print(\"%s: %i\" % (word, count)) ## Running Spark at CARC When you read Spark tutorials, you\u2019ll see that it is generally run on dedicated per-user spark clusters, with the \u201cspark-submit\u201d script or \u201cpyspark\u201d commands used to talk to your cluster. You can do the same thing at CARC, but you\u2019ll need to do a few things first: Have a CARC project/account, for which you can find directions on the CARC website: Log into a CARC shared cluster (we\u2019ll be using wheeler.carc.unm.edu) using ssh or a similar tool (XXX This should link to our documentation on how to do that) Use the PBS queueing system to request a set of nodes for your use (XXX This should link to our PBS information) Run pbs-spark-submit on those nodes to start your personal spark instance (see below) Talk to the spark workers using pyspark or spark-submit Basically, pbs-submit-spark starts your personal Spark cluster (and optionally one job) using batch queueing information from PBS queueing system, and after that\u2019s done you can use pyspark to talk to interactively or submit-spark to submit new Spark jobs to it. pbs-spark-submit is available at https://lobogit.unm.edu/CARC/tutorials/-/blob/master/spark/pbs-spark-submit. ## Simple CARC Spark Examples 1. Create an interactive 4-node cluster on Wheeler for 1 hour ```bridges@wheeler-sn[519]> ls big.txt conf/ pbs-spark-submit* small.txt bridges@wheeler-sn[520]> cd bridges@wheeler-sn[521]> qsub -I -l nodes=4:ppn=8 -l walltime=01:00:00 qsub: waiting for job 14319.wheeler-sn.alliance.unm.edu to start qsub: job 14319.wheeler-sn.alliance.unm.edu ready Job 14319.wheeler-sn.alliance.unm.edu running on nodes: wheeler240 wheeler235 wheeler234 wheeler233 Wheeler Portable Batch System Prologue Job Id: 14319.wheeler-sn.alliance.unm.edu Username: bridges Job 14319.wheeler-sn.alliance.unm.edu running on nodes: wheeler240 wheeler235 wheeler234 wheeler233 prologue running on host: wheeler240 Start the Spark cluster on your wheeler nodes ```bridges@wheeler240[500]> cd /wheeler/scratch/bridges/spark bridges@wheeler240[501]> module load spark-2.1.0-gcc-4.8.5-lifnga6 bridges@wheeler240[502]> export SPARK_HOME=$SPARK_DIR bridges@wheeler240[503]> ./pbs-spark-submit SPARK_MASTER_HOST=wheeler240 SPARK_MASTER_PORT=7077 bridges@wheeler240[504]> pyspark --master spark://wheeler240:7077 3. Run Spark interactively ```>>> rddread = sc.textFile(\"small.txt\") >>> rddread.takeSample(False, 10, 2) [u'a savage place as holy and enchanted ', u'huge fragments vaulted like rebounding hail ', u' floated midway on the waves ', u'and all should cry beware beware ', u'and from this chasm with ceaseless turmoil seething ', u'enfolding sunny spots of greenery ', u'as if this earth in fast thick pants were breathing ', u'and drunk the milk of paradise', u'five miles meandering with a mazy motion ', u'where blossomed many an incensebearing tree '] >>> exit() bridges@wheeler240[505]> exit qsub: job 14319.wheeler-sn.alliance.unm.edu completed Spark jobs can be run in batch as well \u2013 The following PBS batch script would run the simple wordcount example from above, using the Wheeler scratch file system to store the data while it is being processed: ```# wordcount.pbs","title":"Run (serially on the master only, since this isn\u2019t Spark operations!) code to generate output."},{"location":"spark/#pbs-n-wordcount","text":"","title":"PBS -N wordcount"},{"location":"spark/#pbs-l-nodes2ppn8","text":"","title":"PBS -l nodes=2:ppn=8"},{"location":"spark/#pbs-l-walltime000500","text":"SCRATCHDIR=/wheeler/scratch/$USER/spark module load spark-2.1.0-gcc-4.8.5-lifnga6 export SPARK_HOME=$SPARK_DIR cd $PBS_O_WORKDIR cp wordcount.py $SCRATCHDIR cp big.txt $SCRATCHDIR cp -r conf $SCRATCHDIR cd $SCRATCHDIR $PBS_O_WORKDIR/pbs-spark-submit wordcount.py $SCRATCHDIR/big.txt > wordcount.log cp wordcount.log $PBS_O_WORKDIR ## A More Involved Example To make the most of Spark, most programmers will want to do two things: use the DataFrames API and construct a more full-features Python programming environment in which to work and analyze data. While RDDs are the core Spark abstraction, the DataFrame API (available since Spark 2.0) is much more programmer friendly. In particular, DataFrames have columns and column operations, can import CSV and JSON data files, while still being distributed across the memory of all of the nodes in your Spark cluster. 16 Wheeler nodes can, for example, handle a 200GB table! Similarly, setting up a full-featured programming environment by using Anaconda will allow you to analyze and visualize your data much more effectively. ## Setting up a Full Python/Spark environment using Anaconda As mentioned above and on other CARC web pages, we suggest that users use Anaconda to manage their python environments. This applies to Python+Spark as well with one exception \u2013 you want to get spark (and pyspark) from the Spark package so that it matches the version of Spark on your system instead of from Anaconda. To create this environment, simply run the following two commands: ```wheeler-sn> module load anaconda3-4.4.0-gcc-4.8.5-ubgzkkv wheeler-sn> conda create \u2013n spark python=27 numpy scipy pandas matplotlib Once this is done, we just activate that environment and can add new things to it as needed after we launch our Spark cluster, to interactively look at a lot of data. First, we bring up Spark with this customized environment: ```wheeler-sn> qsub \u2013I \u2013l nodes=1:ppn=8 -l walltime=01:00:00 wheeler263> module load anaconda3-4.4.0-gcc-4.8.5-ubgzkkv wheeler263> source activate spark (spark) wheeler263> module load spark spark-2.1.0-gcc-4.8.5-lifnga6 (spark) wheeler263> export SPARK_HOME=$(SPARK_DIR) (spark) wheeler263> ./pbs-spark-submit (spark) wheeler263> pyspark \u2013-master spark://wheeler263:7077 Once we have this running, we can simply load and analyze our data, first converting it into Spark Parquet format so that we can load and save it more quickly later: ```>>> crimes_schema = ... # available in source code files >>> format = \"MM/dd/yyyy hh:mm:ss a\" >>> crimes = spark.read.format(\"csv\").options(header = 'true',dateFormat = format,timestampFormat = format).load(\"crimes.csv\", schema = crimes_schema) >>> count = crimes.count() >>> print(\"Imported CSV with {} crime records.\".format(count)) Imported CSV with 6614877 crime records >>> crimes.write.parquet(\"crimes.parquet\") From there, you can basically think of the resulting DataFrame as a SQL database or Excel sheet, turning the full power of Python and SQL loose on it, including computings averages, sorting or grouping it, adding columns, and doing proper SQL operations on it (selects, joins, pivots, etc.). For example, if you wanted to normalize the Latitudes in the table loaded above, you could the the following: ```>>> lat_mean = crimes.agg({\"Latitude\" : \"mean\"}).collect()[0][0] print(\"The mean latitude values is {}\".format(lat_mean)) The mean latitude values is 41.8419399743 df = crimes.withColumn(\"Lat(Norm)\",lat_mean - crimes[\"Latitude\"]) df.select([\"Latitude\", \"Lat(Norm)\"]).show(3) +------------+--------------------+ | Latitude| Lat(Norm)| +------------+--------------------+ |41.987319201|-0.14537922670000114| |41.868473957|-0.02653398270000...| |41.742667249| 0.09927272529999698| +------------+--------------------+ ## Visualization of Data from Python You can also use Pandas and Matplotlib and everything else on processed data, for example the following code generates a file which graphs Chicago crimes by month: ```import matplotlib as mpl mpl.use('PDF\u2019) # Live rendering over X11 requires ssh tunneling import matplotlib.pyplot as plt import pandas as pd # And then do things like compute domestic crimes by month from pyspark.sql.functions import month monthdf = df.withColumn(\"Month\",month(\"Date\")) monthCounts = monthdf.select(\"Month\").groupBy(\"Month\").count().collect() months = [item[0] for item in monthCounts] count = [item[1] for item in monthCounts] crimes_per_month = pd.DataFrame({\"month\":months, \"crime_count\": crimes_per_month = crimes_per_month.sort_values(by = \"month\") crimes_per_month.plot(figsize = (20,10), kind = \"line\", x = \"month\", y = \"crime_count\", color = \"red\", linewidth = 8, legend = False) plt.xlabel(\"Month\", fontsize = 18) plt.ylabel(\"Number of Crimes\", fontsize = 18) plt.title(\"Number of Crimes Per Month\", fontsize = 28) plt.xticks(size = 18) plt.yticks(size = 18) plt.savefig(\"crimes-by-month.pdf\") Similarly, this code generates a file which charts Chicago crime by location type: crime_location = crimes.groupBy(\"LocationDescription\").count().collect() location = [item[0] for item in crime_location] count = [item[1] for item in crime_location] crime_location = {\"location\" : location, \"count\": count} crime_location = pd.DataFrame(crime_location) crime_location = crime_location.sort_values(by = \"count\", ascending = False) crime_location = crime_location.iloc[:20] myplot = crime_location.plot(figsize =(20,20), kind=\"barh\", color=\"#b35900\", width = 0.8, x = \"location\", y = \"count\", legend = False) myplot.invert_yaxis() plt.xlabel(\"Number of crimes\", fontsize = 28) plt.ylabel(\"Crime Location\", fontsize = 28) plt.title(\"Number of Crimes By Location\", fontsize = 36) plt.xticks(size = 24) plt.yticks(size = 24) plt.savefig(\"crimes-by-location.pdf\")","title":"PBS -l walltime=00:05:00"},{"location":"spark/#more-information","text":"In addition to all of this, there are a wide range of other things you can do with Spark. Spark Streaming (add link) augments RDDs with Dstreams, data batched into N second chunks with one RDD per batch. Spark can automatically monitor data dumped into files or directories or message queues and fill them into Dstreams. Twitter sentiment analysis is a common use case (e.g. combined with a Kafka queue that Spark Streaming pulls from) Spark ML (add link) provides a wide range of scalable regression, inference, clustering, and learning algorithms from Spark. Finally, we\u2019re working on an array of additional improvements on this at CARC in the long term, including making this accessible from Jupyter notebooks, and making it easier to interactively render data to your desktop (as opposed to into files like was done above). Keep an eye out on the CARC website for additional information as these features develop.","title":"More Information"},{"location":"ssh_keygen_config/","text":"SSH keys and config file Once you start computing you will be logging in to the CARC systems fairly often and having to type your username at the machine address will become tedious. In order to alleviate this tedium it is beneficial to generate ssh keys and a ssh config file. The ssh keys bypass the need to enter your password each time you log in, and the config file stores the addresses of all the machines you are logging in to. SSH key generation First, set up your ssh key. To do this type in the terminal prompt: ssh-keygen Just keep hitting enter until the program finishes. We recommend that you decline setting up a passphrase as this defeats the convenience of having a SSH key. Now that your SSH key has been generated you need to copy it to your home directory at CARC. To do, open terminal and type: ssh-copy-id yourusername@machinename.alliance.unm.edu Since your home directory is shared across all machines at CARC you only need to do this step once to enable ssh key access across all CARC machines. SSH config file To make logging in to CARC even easier we also recommend setting up a ssh config file which allows you to simply type ssh machinename instead of your username at the machine address. To set up this file simply copy the example below and save it to a text document in your ssh folder, which is found at ~/.ssh/ . Change the user to your CARC username and you are set to log in quickly and efficiently. You can add machines based on which ones you have access to. Host wheeler hostname wheeler.alliance.unm.edu user CHANGEME port 22 Host hopper hostname hopper.alliance.unm.edu user CHANGEME port 22 Host xena hostname xena.alliance.unm.edu user CHANGEME ForwardX11 yes port 22","title":"SSH keys and Config file"},{"location":"ssh_keygen_config/#ssh-keys-and-config-file","text":"Once you start computing you will be logging in to the CARC systems fairly often and having to type your username at the machine address will become tedious. In order to alleviate this tedium it is beneficial to generate ssh keys and a ssh config file. The ssh keys bypass the need to enter your password each time you log in, and the config file stores the addresses of all the machines you are logging in to.","title":"SSH keys and config file"},{"location":"ssh_keygen_config/#ssh-key-generation","text":"First, set up your ssh key. To do this type in the terminal prompt: ssh-keygen Just keep hitting enter until the program finishes. We recommend that you decline setting up a passphrase as this defeats the convenience of having a SSH key. Now that your SSH key has been generated you need to copy it to your home directory at CARC. To do, open terminal and type: ssh-copy-id yourusername@machinename.alliance.unm.edu Since your home directory is shared across all machines at CARC you only need to do this step once to enable ssh key access across all CARC machines.","title":"SSH key generation"},{"location":"ssh_keygen_config/#ssh-config-file","text":"To make logging in to CARC even easier we also recommend setting up a ssh config file which allows you to simply type ssh machinename instead of your username at the machine address. To set up this file simply copy the example below and save it to a text document in your ssh folder, which is found at ~/.ssh/ . Change the user to your CARC username and you are set to log in quickly and efficiently. You can add machines based on which ones you have access to. Host wheeler hostname wheeler.alliance.unm.edu user CHANGEME port 22 Host hopper hostname hopper.alliance.unm.edu user CHANGEME port 22 Host xena hostname xena.alliance.unm.edu user CHANGEME ForwardX11 yes port 22","title":"SSH config file"},{"location":"storage_and_backup/","text":"Storage and backup CARC provides home, project, and scratch directories on all compute systems. User home directories are subdirectories of /users. User project directories are subdirectories of /project. Scratch directories are subdirectories of the directory /scratch. Storage at CARC CARC supports a number of different storage devices for users to read, write, and store their data and results on. The different devices vary in size, I/O speed (Figure 1), distance to the reading/writing process using them, whether they have quotas to limit their use, and whether they are backed up or not. Certain storage devices, like the home directories and machine-scratch\u2019s, are shared resources, and serve multiple users simultaneously. This means that problems caused by poorly chosen storage locations could interrupt the work of other users as well as your own. Don\u2019t hesitate to open a help ticket if you have any questions about, for example, where to store a large dataset, how to read it quickly, how to write a PBS script that first moves data onto the compute node in order to take advantage of the fast I/O, or any other issue related to storage at CARC. For storage limits see the resource policy page . Types of Storage At CARC, there are four different types of storage: Home directory - /users/username - Upon logging into any CARC machine, you will find yourself in your home directory, in /user/username, replacing \u201cusername\u201d with your actual username (note, the home directory also goes by \u201c~\u201d and \u201c$HOME\u201d). You may notice that regardless of which machine you login to, the contents of your home directory are identical. This is because they are not part of any specific machine, but rather they are stored on separate computer entirely, and are then mounted by every head and compute node at CARC. Machine-wide scratch disk - ~/machine-scratch -> /machine/scratch/username - The most common place where data is stored after it is generated by running calculations, and before it is further analyses and then either downloaded, deleted, or moved when a long term data storage or archival plan is implemented. These are not backed up and CARC reserves the right to delete this data without advanced warning. Hard drive (only on the compute nodes) - /tmp - On the machines that support these , compute nodes have their own hard drives installed and they can be accessed simply by creating a directory in /tmp and then placing data there (see sample PBS script below). Since the hard drive is dedicated to that compute node, this is one of the fastest places for I/O. Files stores here will be cleared at the end of the job. Shared memory (only on the compute nodes) - /dev/shm - This is actually direct access to the machine's memory for use in storage. The directory at /dev/shm appears to the user as a normal read/write accessible directory just like /tmp, but files written or read from any directory within are simply being stored into memory as if they were on a disk. This provides extremely fast I/O speeds, and is very useful if small temporary files are written and read often. Be careful with the amount of data you write here, however, because this competes directly with all processes using the compute nodes RAM, including yours. Like /tmp, /dev/shm is also cleared at the end of a PBS job, so you must move any data you want to keep off the compute node before the end of the calculation or walltime. Figure 1. The various storage locations plotted by their relative size and I/O speed. Choosing a storage type In order to determine which storage type to use, it may be helpful to consider which of the following broad categories your data falls into: Data that is hard to produce and long-lived - Eg. source code, scripts, documents, results. This data is either used to produce or is the product of other calculations and work Results and temporary data - This is data that is produced by your calculations such as simulation logs and are typically further analyzed or have some data of interest extracted and summarized, perhaps even reported in a publication. This data can be regenerated relatively easily, by rerunning the calculation that produced it, and is often deleted once further processed or at most, at the end of a project. We recommend that the first type of data be stored in your home directory, which is available from everywhere inside the CARC network, and is backed up each week. The second kind of data is best produced/stored on the machine-scratch drives. In cases of very high I/O, that data can temporarily be moved to the compute node and onto either the hard drive (if the machine in use has an internal hard drive) or onto shared memory (if the files are very small). An obvious benifit to using devices that are very close to the CPU and are typically only used by a single user at a time, is the high read and write speed availible (Figure 1). The cost of this speed is the fact that the data must be moved to the compute node, read in by the calculation, and anything that is intended to be saved must be moved back off the compute node before the end of walltime or before the job ends, when the compute node is returned to the general resource pool. Example use of a node's hard drive This PBS script that will first copy an input file to the compute node (large_input_data.dat), perform calculations (\"run_my_program\"), and then copy all results of the calculation back into the directory where it was first taken: #PBS -l nodes=1:ppn=8 #PBS -l walltime=1:00:00 #PBS -N Local_storage # First define a directory location as a variable, create the directory once the job has started, move data there, then cd to it and run TEMP_DIR=/tmp/$USER/$PBS_JOBID mkdir -p \u201c$TEMP_DIR\u201d cp -r ${PBS_O_WORKDIR}/large_input_data.dat \u201c$TEMP_DIR\u201d cd $TEMP_DIR # Now run my program run_my_program # Now job has finished, so mv data back to where it came cp -r $TEMP_DIR/* $PBS_O_WORKDIR # Finally clean up the temporary directory rm -r $TEMP_DIR","title":"Storage and backup"},{"location":"storage_and_backup/#storage-and-backup","text":"CARC provides home, project, and scratch directories on all compute systems. User home directories are subdirectories of /users. User project directories are subdirectories of /project. Scratch directories are subdirectories of the directory /scratch.","title":"Storage and backup"},{"location":"storage_and_backup/#storage-at-carc","text":"CARC supports a number of different storage devices for users to read, write, and store their data and results on. The different devices vary in size, I/O speed (Figure 1), distance to the reading/writing process using them, whether they have quotas to limit their use, and whether they are backed up or not. Certain storage devices, like the home directories and machine-scratch\u2019s, are shared resources, and serve multiple users simultaneously. This means that problems caused by poorly chosen storage locations could interrupt the work of other users as well as your own. Don\u2019t hesitate to open a help ticket if you have any questions about, for example, where to store a large dataset, how to read it quickly, how to write a PBS script that first moves data onto the compute node in order to take advantage of the fast I/O, or any other issue related to storage at CARC. For storage limits see the resource policy page .","title":"Storage at CARC"},{"location":"storage_and_backup/#types-of-storage","text":"At CARC, there are four different types of storage: Home directory - /users/username - Upon logging into any CARC machine, you will find yourself in your home directory, in /user/username, replacing \u201cusername\u201d with your actual username (note, the home directory also goes by \u201c~\u201d and \u201c$HOME\u201d). You may notice that regardless of which machine you login to, the contents of your home directory are identical. This is because they are not part of any specific machine, but rather they are stored on separate computer entirely, and are then mounted by every head and compute node at CARC. Machine-wide scratch disk - ~/machine-scratch -> /machine/scratch/username - The most common place where data is stored after it is generated by running calculations, and before it is further analyses and then either downloaded, deleted, or moved when a long term data storage or archival plan is implemented. These are not backed up and CARC reserves the right to delete this data without advanced warning. Hard drive (only on the compute nodes) - /tmp - On the machines that support these , compute nodes have their own hard drives installed and they can be accessed simply by creating a directory in /tmp and then placing data there (see sample PBS script below). Since the hard drive is dedicated to that compute node, this is one of the fastest places for I/O. Files stores here will be cleared at the end of the job. Shared memory (only on the compute nodes) - /dev/shm - This is actually direct access to the machine's memory for use in storage. The directory at /dev/shm appears to the user as a normal read/write accessible directory just like /tmp, but files written or read from any directory within are simply being stored into memory as if they were on a disk. This provides extremely fast I/O speeds, and is very useful if small temporary files are written and read often. Be careful with the amount of data you write here, however, because this competes directly with all processes using the compute nodes RAM, including yours. Like /tmp, /dev/shm is also cleared at the end of a PBS job, so you must move any data you want to keep off the compute node before the end of the calculation or walltime. Figure 1. The various storage locations plotted by their relative size and I/O speed.","title":"Types of Storage"},{"location":"storage_and_backup/#choosing-a-storage-type","text":"In order to determine which storage type to use, it may be helpful to consider which of the following broad categories your data falls into: Data that is hard to produce and long-lived - Eg. source code, scripts, documents, results. This data is either used to produce or is the product of other calculations and work Results and temporary data - This is data that is produced by your calculations such as simulation logs and are typically further analyzed or have some data of interest extracted and summarized, perhaps even reported in a publication. This data can be regenerated relatively easily, by rerunning the calculation that produced it, and is often deleted once further processed or at most, at the end of a project. We recommend that the first type of data be stored in your home directory, which is available from everywhere inside the CARC network, and is backed up each week. The second kind of data is best produced/stored on the machine-scratch drives. In cases of very high I/O, that data can temporarily be moved to the compute node and onto either the hard drive (if the machine in use has an internal hard drive) or onto shared memory (if the files are very small). An obvious benifit to using devices that are very close to the CPU and are typically only used by a single user at a time, is the high read and write speed availible (Figure 1). The cost of this speed is the fact that the data must be moved to the compute node, read in by the calculation, and anything that is intended to be saved must be moved back off the compute node before the end of walltime or before the job ends, when the compute node is returned to the general resource pool.","title":"Choosing a storage type"},{"location":"storage_and_backup/#example-use-of-a-nodes-hard-drive","text":"This PBS script that will first copy an input file to the compute node (large_input_data.dat), perform calculations (\"run_my_program\"), and then copy all results of the calculation back into the directory where it was first taken: #PBS -l nodes=1:ppn=8 #PBS -l walltime=1:00:00 #PBS -N Local_storage # First define a directory location as a variable, create the directory once the job has started, move data there, then cd to it and run TEMP_DIR=/tmp/$USER/$PBS_JOBID mkdir -p \u201c$TEMP_DIR\u201d cp -r ${PBS_O_WORKDIR}/large_input_data.dat \u201c$TEMP_DIR\u201d cd $TEMP_DIR # Now run my program run_my_program # Now job has finished, so mv data back to where it came cp -r $TEMP_DIR/* $PBS_O_WORKDIR # Finally clean up the temporary directory rm -r $TEMP_DIR","title":"Example use of a node's hard drive"},{"location":"submitting_jobs/","text":"Submitting jobs There are two ways you can run your jobs, namely submitting a PBS script and running a job interactively. Either way, jobs are submitted to CARC by the command qsub . For more information on available options type man qsub Submitting the PBS Script to the Batch Scheduler In order to run our simple PBS script, we will need to submit it to the batch scheduler using the command qsub followed by the name of the script we would like to run. For more information please see our page on writing a PBS batch script . In the following example, we submit our simple hello.pbs script to the batch scheduler using qsub . Note that it returns the job identifier when the job is successfully submitted. You can use this job identifier to query the status of your job from your shell. For example: qsub hello.pbs 64152.wheeler-sn.alliance.unm.edu Interactive PBS Jobs Normally a job is submitted for execution on a cluster or supercomputer using the command qsub script.pbs . CARC recommends that all jobs are submitted this way as job submission fails if there are errors in resources requested. However, at times, such as when debugging, it can be useful to run a job interactively. To run a job in this way type qsub -I followed by resources requested, and the batch manager will log you into a node where you can directly run your code. For example, here is the output from an interactive session running our simple helloworld_paralell.pbs script: qsub -I -lnodes=1:ppn=8 -lwalltime=00:05:00 qsub: waiting for job 64143.wheeler-sn.alliance.unm.edu to start qsub: job 64143.wheeler-sn.alliance.unm.edu ready Wheeler Portable Batch System Prologue Job Id: 64143.wheeler-sn.alliance.unm.edu Username: user Job 64143.wheeler-sn.alliance.unm.edu running on nodes: wheeler274 prologue running on host: wheeler274 bash helloworld.pbs Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Three commands were executed here. The first, qsub -I -lnodes=1:ppn=4 -lwalltime=00:05:00 asked the batch manager to provide one node of wheeler with all 8 of that node\u2019s cores for use. It is good practice to request all available processors on a node to avoid multiple users being assigned to the same node. The walltime was specified as 5 minutes, since this was a simple code that would execute quickly. The second command, module load module load openmpi-3.1.1-intel-18.0.2-hlc45mq loaded the openMPI software module to parallelize our script across all 8 processors; this ensures that the necessary MPI libraries would be available during execution. The third command, bash helloworld_parallel.pbs ran the commands found within our helloworld_paralell.pbs script.","title":"Submitting jobs"},{"location":"submitting_jobs/#submitting-jobs","text":"There are two ways you can run your jobs, namely submitting a PBS script and running a job interactively. Either way, jobs are submitted to CARC by the command qsub . For more information on available options type man qsub","title":"Submitting jobs"},{"location":"submitting_jobs/#submitting-the-pbs-script-to-the-batch-scheduler","text":"In order to run our simple PBS script, we will need to submit it to the batch scheduler using the command qsub followed by the name of the script we would like to run. For more information please see our page on writing a PBS batch script . In the following example, we submit our simple hello.pbs script to the batch scheduler using qsub . Note that it returns the job identifier when the job is successfully submitted. You can use this job identifier to query the status of your job from your shell. For example: qsub hello.pbs 64152.wheeler-sn.alliance.unm.edu","title":"Submitting the PBS Script to the Batch Scheduler"},{"location":"submitting_jobs/#interactive-pbs-jobs","text":"Normally a job is submitted for execution on a cluster or supercomputer using the command qsub script.pbs . CARC recommends that all jobs are submitted this way as job submission fails if there are errors in resources requested. However, at times, such as when debugging, it can be useful to run a job interactively. To run a job in this way type qsub -I followed by resources requested, and the batch manager will log you into a node where you can directly run your code. For example, here is the output from an interactive session running our simple helloworld_paralell.pbs script: qsub -I -lnodes=1:ppn=8 -lwalltime=00:05:00 qsub: waiting for job 64143.wheeler-sn.alliance.unm.edu to start qsub: job 64143.wheeler-sn.alliance.unm.edu ready Wheeler Portable Batch System Prologue Job Id: 64143.wheeler-sn.alliance.unm.edu Username: user Job 64143.wheeler-sn.alliance.unm.edu running on nodes: wheeler274 prologue running on host: wheeler274 bash helloworld.pbs Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Hello World from host wheeler274 Three commands were executed here. The first, qsub -I -lnodes=1:ppn=4 -lwalltime=00:05:00 asked the batch manager to provide one node of wheeler with all 8 of that node\u2019s cores for use. It is good practice to request all available processors on a node to avoid multiple users being assigned to the same node. The walltime was specified as 5 minutes, since this was a simple code that would execute quickly. The second command, module load module load openmpi-3.1.1-intel-18.0.2-hlc45mq loaded the openMPI software module to parallelize our script across all 8 processors; this ensures that the necessary MPI libraries would be available during execution. The third command, bash helloworld_parallel.pbs ran the commands found within our helloworld_paralell.pbs script.","title":"Interactive PBS Jobs"},{"location":"systems_information/","text":"CARC Systems Information CARC Supercomputer and Cluster Resources Machine Name Wheeler Taos Gibbs Xena Hopper Model/Type SGI AltixXE Xeon X5550 2.67 GHz Dell PowerEdge R630 Xeon E5-2698 V4 2.20 GHz Dell PowerEdge R620 Intel Xeon E5-2670 2.6 GHz Dell PowerEdge R730 Intel Xeon E5-2640 2.6 GHz and PowerEdge R930 Intel Xeon E7-4809 2.0 Ghz Dell PowerEdge R640 Intel Xeon Gold 6226R 2.9 GHz and Dell PowerEdge R740 Intel Xeon Gold 6242 2.8 GHz Linux Operating System CentOS 7 CentOS 7 Scientific Linux CentOS 7 Rocky Linux Interconnect Mellanox IS5600 InfiniScale IV ConnectX-2 IB QDR (MT26428) Mellanox SX6000 ConnectX-3 IB FDR (MT4099) InfiniBand QDR InfiniBand FDR InfiniBand HDR Nodes 304 9 24 32 61 Cores/Node 8 variable 16 16, 32 32 Total Cores 2432 180 384 576 2176 RAM/Core 6GB variable 4 GB 4 GB, 32 GB, 96 GB variable Local disk/node Diskless 1 TB 1 TB 1 TB 448 GB Peak FLOPS (theoretical), in TFLOPS 25 (TBD) 3.996 18 (TBD) Processor Architecture Intel Xeon Nehalem EP Intel Xeon Broadwell Intel SandyBridge Intel Xeon E7-2640 Intel Xeon E7-4809 (Haswell) Intel Xeon Gold 6226R Intel Xeon Gold 6242 (Cascade Lake) Local Scratch Space (TB) 40 27 6.3 73 (TBD) Xena Cluster Specs Queue Name bigmem-1TB bigmem-3TB dualGPU singleGPU Nodes 2 2 4 24 Cores/Node 32 32 16 16 Memory/Node 1 TB 3 TB 64 GB 64 GB Total Cores 64 64 64 384 Processor Architecture Intel Xeon CPU E7-4809 Intel Xeon CPU E7-4809 Intel Xeon CPU E5-2640 Intel Xeon CPU E5-2640 CPU GHz 2.00 2.00 2.60 2.60 GPU N/A N/A 2 x Nvidia Tesla K40M per node 1 x Nvidia Tesla K40M per node","title":"Systems Information"},{"location":"systems_information/#carc-systems-information","text":"","title":"CARC Systems Information"},{"location":"systems_information/#carc-supercomputer-and-cluster-resources","text":"Machine Name Wheeler Taos Gibbs Xena Hopper Model/Type SGI AltixXE Xeon X5550 2.67 GHz Dell PowerEdge R630 Xeon E5-2698 V4 2.20 GHz Dell PowerEdge R620 Intel Xeon E5-2670 2.6 GHz Dell PowerEdge R730 Intel Xeon E5-2640 2.6 GHz and PowerEdge R930 Intel Xeon E7-4809 2.0 Ghz Dell PowerEdge R640 Intel Xeon Gold 6226R 2.9 GHz and Dell PowerEdge R740 Intel Xeon Gold 6242 2.8 GHz Linux Operating System CentOS 7 CentOS 7 Scientific Linux CentOS 7 Rocky Linux Interconnect Mellanox IS5600 InfiniScale IV ConnectX-2 IB QDR (MT26428) Mellanox SX6000 ConnectX-3 IB FDR (MT4099) InfiniBand QDR InfiniBand FDR InfiniBand HDR Nodes 304 9 24 32 61 Cores/Node 8 variable 16 16, 32 32 Total Cores 2432 180 384 576 2176 RAM/Core 6GB variable 4 GB 4 GB, 32 GB, 96 GB variable Local disk/node Diskless 1 TB 1 TB 1 TB 448 GB Peak FLOPS (theoretical), in TFLOPS 25 (TBD) 3.996 18 (TBD) Processor Architecture Intel Xeon Nehalem EP Intel Xeon Broadwell Intel SandyBridge Intel Xeon E7-2640 Intel Xeon E7-4809 (Haswell) Intel Xeon Gold 6226R Intel Xeon Gold 6242 (Cascade Lake) Local Scratch Space (TB) 40 27 6.3 73 (TBD)","title":"CARC Supercomputer and Cluster Resources"},{"location":"systems_information/#xena-cluster-specs","text":"Queue Name bigmem-1TB bigmem-3TB dualGPU singleGPU Nodes 2 2 4 24 Cores/Node 32 32 16 16 Memory/Node 1 TB 3 TB 64 GB 64 GB Total Cores 64 64 64 384 Processor Architecture Intel Xeon CPU E7-4809 Intel Xeon CPU E7-4809 Intel Xeon CPU E5-2640 Intel Xeon CPU E5-2640 CPU GHz 2.00 2.00 2.60 2.60 GPU N/A N/A 2 x Nvidia Tesla K40M per node 1 x Nvidia Tesla K40M per node","title":"Xena Cluster Specs"},{"location":"transfer_data/","text":"Transferring data Where is your data? Your home directory, /users/your-user-name/ , is shared across all CARC machines, meaning that once your data has been uploaded to your home directory it is accessible regardless of which machine you are logged in to. Refer to our documentation page on CARC for details on CARC systems. Graphical User Interface (GUI) options There are several options available for data transfer that employ a GUI for ease of use. Several options are listed below and are linked to the homepage for the software with documentation on how to use it. FileZilla WinSCP Fetch CyberDuck FileZilla is available for both Windows and Unix systems, whereas WinSCP is Windows only, and Fetch is MacOS only. GUI-based programs are very user friendly and well-suited to those that are less comfortable in Linux Command-line interface. Unfortunately, the programs listed above, and other GUI-based programs, employ File Transfer Protocol (FTP), which has a relatively low transfer speed and is suitable for smaller file sizes. Command-line interface (CLI) options For larger files it is recommended you use one of several programs implemented in a command-line interface. These programs have several benefits over their GUI-based counterparts, including higher transfer speeds and the ability to continue a transfer if it is interrupted without having to restart from the beginning. Below are several popular options with example commands and links for more advanced usage. Secure Copy (SCP) Transfer from local machine to CARC: scp /your-file your-username@wheeler.alliance.unm.edu:target-directory/ Transfer from CARC to local machine: scp your-username@wheeler.alliance.unm.edu:your-file /target-directory/ Remote Sync (RSYNC) Transfer from local machine to CARC rsync -vhatP /your-file your-username@wheeler.alliance.unm.edu:target-directory Transfer from CARC to local machine rsync -vhatP your-username@wheeler.alliance.unm.edu:your-file /target-directory/ The -vhatP flag are instructions to rsync print out the progress of the transfer verbosely and human-readable. BaBar Copy (BBCP) Transfer from local machine to CARC bbcp /your-file your-username@wheeler.alliance.unm.edu:target-directory/ Transfer from CARC to local machine bbcp your-username@wheeler.alliance.unm.edu:your-file /target-directory/ As you can see, the syntax for using the various programs is very similar, however, the options for advanced usage are unique to each program. The examples provided above are for very basic data transfers, but you should refer to the links provided, or for the CLI options use the command man programname , in order to optimize each for maximum data transfer efficiency and speed. The necessity for transfer optimization increases as file size increases.","title":"Transferring data"},{"location":"transfer_data/#transferring-data","text":"","title":"Transferring data"},{"location":"transfer_data/#where-is-your-data","text":"Your home directory, /users/your-user-name/ , is shared across all CARC machines, meaning that once your data has been uploaded to your home directory it is accessible regardless of which machine you are logged in to. Refer to our documentation page on CARC for details on CARC systems.","title":"Where is your data?"},{"location":"transfer_data/#graphical-user-interface-gui-options","text":"There are several options available for data transfer that employ a GUI for ease of use. Several options are listed below and are linked to the homepage for the software with documentation on how to use it. FileZilla WinSCP Fetch CyberDuck FileZilla is available for both Windows and Unix systems, whereas WinSCP is Windows only, and Fetch is MacOS only. GUI-based programs are very user friendly and well-suited to those that are less comfortable in Linux Command-line interface. Unfortunately, the programs listed above, and other GUI-based programs, employ File Transfer Protocol (FTP), which has a relatively low transfer speed and is suitable for smaller file sizes.","title":"Graphical User Interface (GUI) options"},{"location":"transfer_data/#command-line-interface-cli-options","text":"For larger files it is recommended you use one of several programs implemented in a command-line interface. These programs have several benefits over their GUI-based counterparts, including higher transfer speeds and the ability to continue a transfer if it is interrupted without having to restart from the beginning. Below are several popular options with example commands and links for more advanced usage.","title":"Command-line interface (CLI) options"},{"location":"transfer_data/#secure-copy-scp","text":"Transfer from local machine to CARC: scp /your-file your-username@wheeler.alliance.unm.edu:target-directory/ Transfer from CARC to local machine: scp your-username@wheeler.alliance.unm.edu:your-file /target-directory/","title":"Secure Copy (SCP)"},{"location":"transfer_data/#remote-sync-rsync","text":"Transfer from local machine to CARC rsync -vhatP /your-file your-username@wheeler.alliance.unm.edu:target-directory Transfer from CARC to local machine rsync -vhatP your-username@wheeler.alliance.unm.edu:your-file /target-directory/ The -vhatP flag are instructions to rsync print out the progress of the transfer verbosely and human-readable.","title":"Remote Sync (RSYNC)"},{"location":"transfer_data/#babar-copy-bbcp","text":"Transfer from local machine to CARC bbcp /your-file your-username@wheeler.alliance.unm.edu:target-directory/ Transfer from CARC to local machine bbcp your-username@wheeler.alliance.unm.edu:your-file /target-directory/ As you can see, the syntax for using the various programs is very similar, however, the options for advanced usage are unique to each program. The examples provided above are for very basic data transfers, but you should refer to the links provided, or for the CLI options use the command man programname , in order to optimize each for maximum data transfer efficiency and speed. The necessity for transfer optimization increases as file size increases.","title":"BaBar Copy (BBCP)"},{"location":"workshop_slides/","text":"Workshop Slides These slides were used in CARC workshops. They are provided for your convenience. Current workshop schedule Beginner Introduction: Basic Linux and Interactive SLURM Intermediate Introduction: High Performance Computing and Batch SLURM /w Python Intermediate Introduction: High Performance Computing and Batch SLURM /w FORTRAN Biology 4/519: Biodiversity Informatics Chemistry 567/Physics 581/Electrical and Computer Engineering 595: Quantum Computing for Quantum Chemistry Computer Science 4/542: Introduction to Parallel Processing Computer Science 523: Complex Adaptive Systems Computer Science/Math 471: Introduction to Scientific Computing Environmental Sciences 352: Global Climate Change Earth and Planetary Sciences 400/522: Computational Methods for Geoscience CARC Annual Meeting: Machine Learning PBS, Modules, GNU Parallel, Conda, and MPI Intermediate Intro to SLURM with NAMD ESCAPE Workshop with Jupyterhub Parallel Processing with MPI and SLURM 50 min version SLURM and Gaussian 1 hr version Intermediate SLURM and StarCCM Intermediate SLURM and Machine Learning","title":"Workshop Slides"},{"location":"workshop_slides/#workshop-slides","text":"These slides were used in CARC workshops. They are provided for your convenience. Current workshop schedule Beginner Introduction: Basic Linux and Interactive SLURM Intermediate Introduction: High Performance Computing and Batch SLURM /w Python Intermediate Introduction: High Performance Computing and Batch SLURM /w FORTRAN Biology 4/519: Biodiversity Informatics Chemistry 567/Physics 581/Electrical and Computer Engineering 595: Quantum Computing for Quantum Chemistry Computer Science 4/542: Introduction to Parallel Processing Computer Science 523: Complex Adaptive Systems Computer Science/Math 471: Introduction to Scientific Computing Environmental Sciences 352: Global Climate Change Earth and Planetary Sciences 400/522: Computational Methods for Geoscience CARC Annual Meeting: Machine Learning PBS, Modules, GNU Parallel, Conda, and MPI Intermediate Intro to SLURM with NAMD ESCAPE Workshop with Jupyterhub Parallel Processing with MPI and SLURM 50 min version SLURM and Gaussian 1 hr version Intermediate SLURM and StarCCM Intermediate SLURM and Machine Learning","title":"Workshop Slides"},{"location":"R_at_CARC/","text":"Runing R at CARC intro The following markdown files contain the introductory workshop material for running R on CARC systems. The \"tutorials\" go basically in this order: Getting R software Installing packages PBS job submission A GNU parallel tutorial The following files demonstrate parallellization of R code on a CARC system: * sequential_r.pbs is a job submission script to submit the super simple sequential.R job * parallel_r.pbs is a job submission script to submit the parallel.r * parameters","title":"Runing R at CARC intro"},{"location":"R_at_CARC/#runing-r-at-carc-intro","text":"The following markdown files contain the introductory workshop material for running R on CARC systems. The \"tutorials\" go basically in this order: Getting R software Installing packages PBS job submission A GNU parallel tutorial The following files demonstrate parallellization of R code on a CARC system: * sequential_r.pbs is a job submission script to submit the super simple sequential.R job * parallel_r.pbs is a job submission script to submit the parallel.r * parameters","title":"Runing R at CARC intro"},{"location":"R_at_CARC/PBS_job_submission/","text":"Writing and submitting the PBS script The PBS script In order to actually submit a job to the cluster you need a PBS script that lists all of the resources you are requesting, the software you want to use, and the commands to that software. So pretend we have an R script called \"my_script.R\" that you want to run on Wheeler, the PBS script would look something like this: #!/bin/bash #PBS -N my_r_job #PBS -l walltime=48:00:00 #PBS -l nodes=1:ppn=8 #PBS -j oe #PBS -V #PBS -m abe #PBS -M my_name@unm.edu cd $PBS_O_WORKDIR module load r-3.6.0-gcc-7.3.0-python2-7akol5t Rscript my_script.R This PBS script would then be submitted to the job queue with yourusername@wheeler-sn$ qsub my_pbs_script.pbs A PBS script is just a bash script that combines flags for the job scheduler (PBS), and bash commands. We can break it down by sections. PBS directives If you are unfamiliar with bash scripting the first line #!/bin/bash/ is called the \"hashbang\" or \"shebang\" and is directing your shell on what should be used to interpret the following code. This isn't actually necessary for a PBS script but is just common practice. The next chunk of lines are the flags to qsub asking for specific resources. The # here are important because they represent comments to bash but are interpreted by qsub . ## The -N flag specifies the name of your job and will be what shows on the queue and the prefix for all ## PBS specific output. #PBS -N my_r_job ## The -l flag specifies the actual computational resources you want and can take many arguments. In this case we are ## asking for 48 hours of walltime on the cluster with one node and all cpu's on that node, which for Wheeler is 8. #PBS -l walltime=48:00:00 #PBS -l nodes=1:ppn=8 ## The -j flag joins your standard out (stdout) and standard error (stderr) into one file. I do this just so there is ## less output and clutter resulting from a job. #PBS -j oe ## The -V flag exports user environmental variables from the head node to the compute node. Not always necessary. #PBS -V ## The -m and -M flags specifiy how you want the scheduler to control mailing information about your job. The -m abe ## is saying that you want emails for Abort, Begin, and End, and the -M flag is a comma separated list of who ## to send mail to. #PBS -m abe #PBS -M my_name@unm.edu At the very least you should always specify the walltime, nodes, and processors per node for each job. For an exhaustive list of qsub commands you can type man qsub on the head node of any CARC system. Calling your code The rest of the PBS script are usually bash commands (or whatever shell you prefer to use, which can be specified with the -S flag) that tells the compute node where your data is, loads the software you want, and then executes your job. ## There are many job specific variables that are created when PBS starts a job, including $PBS_O_WORKDIR. The ## following line moves to the directory where the qsub command was executed. cd $PBS_O_WORKDIR ## This loads the R software environment. module load r-3.6.0-gcc-7.3.0-python2-7akol5t ## If you are using a conda environment instead of an R module installed by CARC you would have the following module load anaconda3 source activate my_r_env ## This line is where you are actually running your R script. The older way of using R CMD BATCH is deprecated ## and Rscript is the preferred way for launching an R batch job. Rscript my_script.R","title":"Writing and submitting the PBS script"},{"location":"R_at_CARC/PBS_job_submission/#writing-and-submitting-the-pbs-script","text":"","title":"Writing and submitting the PBS script"},{"location":"R_at_CARC/PBS_job_submission/#the-pbs-script","text":"In order to actually submit a job to the cluster you need a PBS script that lists all of the resources you are requesting, the software you want to use, and the commands to that software. So pretend we have an R script called \"my_script.R\" that you want to run on Wheeler, the PBS script would look something like this: #!/bin/bash #PBS -N my_r_job #PBS -l walltime=48:00:00 #PBS -l nodes=1:ppn=8 #PBS -j oe #PBS -V #PBS -m abe #PBS -M my_name@unm.edu cd $PBS_O_WORKDIR module load r-3.6.0-gcc-7.3.0-python2-7akol5t Rscript my_script.R This PBS script would then be submitted to the job queue with yourusername@wheeler-sn$ qsub my_pbs_script.pbs A PBS script is just a bash script that combines flags for the job scheduler (PBS), and bash commands. We can break it down by sections.","title":"The PBS script"},{"location":"R_at_CARC/PBS_job_submission/#pbs-directives","text":"If you are unfamiliar with bash scripting the first line #!/bin/bash/ is called the \"hashbang\" or \"shebang\" and is directing your shell on what should be used to interpret the following code. This isn't actually necessary for a PBS script but is just common practice. The next chunk of lines are the flags to qsub asking for specific resources. The # here are important because they represent comments to bash but are interpreted by qsub . ## The -N flag specifies the name of your job and will be what shows on the queue and the prefix for all ## PBS specific output. #PBS -N my_r_job ## The -l flag specifies the actual computational resources you want and can take many arguments. In this case we are ## asking for 48 hours of walltime on the cluster with one node and all cpu's on that node, which for Wheeler is 8. #PBS -l walltime=48:00:00 #PBS -l nodes=1:ppn=8 ## The -j flag joins your standard out (stdout) and standard error (stderr) into one file. I do this just so there is ## less output and clutter resulting from a job. #PBS -j oe ## The -V flag exports user environmental variables from the head node to the compute node. Not always necessary. #PBS -V ## The -m and -M flags specifiy how you want the scheduler to control mailing information about your job. The -m abe ## is saying that you want emails for Abort, Begin, and End, and the -M flag is a comma separated list of who ## to send mail to. #PBS -m abe #PBS -M my_name@unm.edu At the very least you should always specify the walltime, nodes, and processors per node for each job. For an exhaustive list of qsub commands you can type man qsub on the head node of any CARC system.","title":"PBS directives"},{"location":"R_at_CARC/PBS_job_submission/#calling-your-code","text":"The rest of the PBS script are usually bash commands (or whatever shell you prefer to use, which can be specified with the -S flag) that tells the compute node where your data is, loads the software you want, and then executes your job. ## There are many job specific variables that are created when PBS starts a job, including $PBS_O_WORKDIR. The ## following line moves to the directory where the qsub command was executed. cd $PBS_O_WORKDIR ## This loads the R software environment. module load r-3.6.0-gcc-7.3.0-python2-7akol5t ## If you are using a conda environment instead of an R module installed by CARC you would have the following module load anaconda3 source activate my_r_env ## This line is where you are actually running your R script. The older way of using R CMD BATCH is deprecated ## and Rscript is the preferred way for launching an R batch job. Rscript my_script.R","title":"Calling your code"},{"location":"R_at_CARC/getting_R_software/","text":"Running R on HPC systems (well, on CARC systems) Getting R in the first place There are three options for accessing R at Carc and I will run through both approaches since there are pros and cons to each. Option 1 The first option is to activate an installed R module. When logged in to a CARC system you can use the module avail command to see which R versions are available. If you have a CARC account open a terminal and log in to follow along. yourusername@wheeler-sn$ module avail r- which will print out the following (although I have truncated the output): ----------------------- /opt/spack/share/spack/modules/linux-centos7-x86_64 ------------------------ ... r-3.4.1-gcc-4.8.5-python2-gzeg24m r-3.4.1-gcc-4.8.5-python2-zpkgqap r-3.4.1-intel-17.0.4-mkl-python2-67zsm3b r-3.4.1-intel-17.0.4-mkl-python2-gygkoab r-3.4.2-intel-18.0.2-python2-xsxuxwx r-3.4.3-gcc-4.8.5-python2-gk66fni r-3.4.3-gcc-4.8.5-python2-qv6gwz6 r-3.4.3-gcc-6.1.0-python2-lyqiytq r-3.4.3-gcc-7.3.0-python2-zhxbajj r-3.4.3-intel-18.0.1-python2-3l4dkgz r-3.4.3-intel-18.0.1-python2-lr24ix6 r-3.4.3-intel-18.0.2-python2-q3covk7 r-3.5.0-gcc-4.8.5-python2-khqxja7 r-3.5.0-gcc-7.3.0-python2-rvq3qk5 r-3.5.0-intel-18.0.2-python2-mkl-r6lx6yy r-3.5.3-gcc-7.3.0-python2-ziiolp5 r-3.6.0-gcc-4.8.5-python2-i4uimtp r-3.6.0-gcc-7.3.0-python2-7akol5t ... Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". These are all of the currently available R modules installed on Wheeler. In order to activate a R software module you use the module load command. For example: yourusername@wheeler-sn$ module load r-3.6.0-gcc-7.3.0-python2-7akol5t yourusername@wheeler-sn$ R R version 3.6.0 (2019-04-26) -- \"Planting of a Tree\" Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > Will load R-3.6.0 that has been compiled with GCC-7.3.0. Normally you will be running R jobs in batch mode as oppposed to interactively, which means you will have the module load command in your PBS script, but we will get to that later. Option 2 The second option is to create a custom local Anaconda environment with the version of R that would like to run. In order to do this you need to first load an Anaconda software module and then create a new environment according to your specifications. For example, the following commands will create an Anaconda environment with R-3.4.3: yourusername@wheeler-sn$ module load anaconda3 yourusername@wheeler-sn$ conda create --yes --name my_r_env r=3.4.3 Solving environment: done ## Package Plan ## environment location: /users/yourusername/.conda/envs/my_r_env added / updated specs: - r=3.4.3 The following packages will be downloaded: package | build ---------------------------|----------------- gfortran_linux-64-7.3.0 | h553295d_8 148 KB r-cluster-2.0.6 | r343h4829c52_0 519 KB r-lattice-0.20_35 | r343h086d26f_0 713 KB r-base-3.4.3 | h9bb98a2_5 38.3 MB r-recommended-3.4.3 | r343_0 3 KB r-3.4.3 | r343_0 3 KB r-mgcv-1.8_22 | r343h086d26f_0 2.4 MB r-matrix-1.2_12 | r343h086d26f_0 2.5 MB r-kernsmooth-2.23_15 | r343h4829c52_4 101 KB r-class-7.3_14 | r343h086d26f_4 93 KB r-nlme-3.1_131 | r343h4829c52_0 2.2 MB r-foreign-0.8_69 | r343h086d26f_0 256 KB r-survival-2.41_3 | r343h086d26f_0 5.1 MB gcc_linux-64-7.3.0 | h553295d_8 149 KB r-spatial-7.3_11 | r343h086d26f_4 140 KB gcc_impl_linux-64-7.3.0 | habb00fd_1 73.2 MB gxx_impl_linux-64-7.3.0 | hdf63c60_1 18.7 MB gxx_linux-64-7.3.0 | h553295d_8 148 KB r-boot-1.3_20 | r343h889e2dd_0 625 KB binutils_linux-64-2.31.1 | h6176602_8 148 KB r-mass-7.3_48 | r343h086d26f_0 1.1 MB bzip2-1.0.8 | h7b6447c_0 105 KB ca-certificates-2019.5.15 | 1 134 KB r-codetools-0.2_15 | r343h889e2dd_0 49 KB r-rpart-4.1_11 | r343h086d26f_0 899 KB r-nnet-7.3_12 | r343h086d26f_0 118 KB libxml2-2.9.9 | hea5a465_1 2.0 MB ------------------------------------------------------------ Total: 149.8 MB The following NEW packages will be INSTALLED: _libgcc_mutex: 0.1-main _r-mutex: 1.0.0-anacondar_1 binutils_impl_linux-64: 2.31.1-h6176602_1 binutils_linux-64: 2.31.1-h6176602_8 bwidget: 1.9.11-1 bzip2: 1.0.8-h7b6447c_0 ca-certificates: 2019.5.15-1 cairo: 1.14.12-h8948797_3 curl: 7.65.2-hbc83047_0 fontconfig: 2.13.0-h9420a91_0 freetype: 2.9.1-h8a8886c_1 fribidi: 1.0.5-h7b6447c_0 gcc_impl_linux-64: 7.3.0-habb00fd_1 gcc_linux-64: 7.3.0-h553295d_8 gfortran_impl_linux-64: 7.3.0-hdf63c60_1 gfortran_linux-64: 7.3.0-h553295d_8 glib: 2.56.2-hd408876_0 graphite2: 1.3.13-h23475e2_0 gxx_impl_linux-64: 7.3.0-hdf63c60_1 gxx_linux-64: 7.3.0-h553295d_8 harfbuzz: 1.8.8-hffaf4a1_0 icu: 58.2-h9c2bf20_1 jpeg: 9b-h024ee3a_2 krb5: 1.16.1-h173b8e3_7 libcurl: 7.65.2-h20c2e04_0 libedit: 3.1.20181209-hc058e9b_0 libffi: 3.2.1-hd88cf55_4 libgcc-ng: 9.1.0-hdf63c60_0 libgfortran-ng: 7.3.0-hdf63c60_0 libpng: 1.6.37-hbc83047_0 libssh2: 1.8.2-h1ba5d50_0 libstdcxx-ng: 9.1.0-hdf63c60_0 libtiff: 4.0.10-h2733197_2 libuuid: 1.0.3-h1bed415_2 libxcb: 1.13-h1bed415_1 libxml2: 2.9.9-hea5a465_1 ncurses: 6.1-he6710b0_1 openssl: 1.1.1c-h7b6447c_1 pango: 1.42.4-h049681c_0 pcre: 8.43-he6710b0_0 pixman: 0.38.0-h7b6447c_0 r: 3.4.3-r343_0 r-base: 3.4.3-h9bb98a2_5 r-boot: 1.3_20-r343h889e2dd_0 r-class: 7.3_14-r343h086d26f_4 r-cluster: 2.0.6-r343h4829c52_0 r-codetools: 0.2_15-r343h889e2dd_0 r-foreign: 0.8_69-r343h086d26f_0 r-kernsmooth: 2.23_15-r343h4829c52_4 r-lattice: 0.20_35-r343h086d26f_0 r-mass: 7.3_48-r343h086d26f_0 r-matrix: 1.2_12-r343h086d26f_0 r-mgcv: 1.8_22-r343h086d26f_0 r-nlme: 3.1_131-r343h4829c52_0 r-nnet: 7.3_12-r343h086d26f_0 r-recommended: 3.4.3-r343_0 r-rpart: 4.1_11-r343h086d26f_0 r-spatial: 7.3_11-r343h086d26f_4 r-survival: 2.41_3-r343h086d26f_0 readline: 7.0-h7b6447c_5 tk: 8.6.8-hbc83047_0 tktable: 2.10-h14c3975_0 xz: 5.2.4-h14c3975_4 zlib: 1.2.11-h7b6447c_3 zstd: 1.3.7-h0b5b093_0 Downloading and Extracting Packages gfortran_linux-64-7. | 148 KB | ########################################################### | 100% r-cluster-2.0.6 | 519 KB | ########################################################### | 100% r-lattice-0.20_35 | 713 KB | ########################################################### | 100% r-base-3.4.3 | 38.3 MB | ########################################################### | 100% r-recommended-3.4.3 | 3 KB | ########################################################### | 100% r-3.4.3 | 3 KB | ########################################################### | 100% r-mgcv-1.8_22 | 2.4 MB | ########################################################### | 100% r-matrix-1.2_12 | 2.5 MB | ########################################################### | 100% r-kernsmooth-2.23_15 | 101 KB | ########################################################### | 100% r-class-7.3_14 | 93 KB | ########################################################### | 100% r-nlme-3.1_131 | 2.2 MB | ########################################################### | 100% r-foreign-0.8_69 | 256 KB | ########################################################### | 100% r-survival-2.41_3 | 5.1 MB | ########################################################### | 100% gcc_linux-64-7.3.0 | 149 KB | ########################################################### | 100% r-spatial-7.3_11 | 140 KB | ########################################################### | 100% gcc_impl_linux-64-7. | 73.2 MB | ########################################################### | 100% gxx_impl_linux-64-7. | 18.7 MB | ########################################################### | 100% gxx_linux-64-7.3.0 | 148 KB | ########################################################### | 100% r-boot-1.3_20 | 625 KB | ########################################################### | 100% binutils_linux-64-2. | 148 KB | ########################################################### | 100% r-mass-7.3_48 | 1.1 MB | ########################################################### | 100% bzip2-1.0.8 | 105 KB | ########################################################### | 100% ca-certificates-2019 | 134 KB | ########################################################### | 100% r-codetools-0.2_15 | 49 KB | ########################################################### | 100% r-rpart-4.1_11 | 899 KB | ########################################################### | 100% r-nnet-7.3_12 | 118 KB | ########################################################### | 100% libxml2-2.9.9 | 2.0 MB | ########################################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use: # > source activate my_r_env # # To deactivate an active environment, use: # > source deactivate # Then to use your newly created R environment you need to 1) make sure you have the Anaconda software module loaded, and 2), activate your conda envioronment. yourusername@wheeler-sn$ module load anaconda3 yourusername@wheeler-sn$ source activate my_r_env yourusername@wheeler-sn$ R R version 3.4.3 (2017-11-30) -- \"Kite-Eating Tree\" Copyright (C) 2017 The R Foundation for Statistical Computing Platform: x86_64-conda_cos6-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > Option 3 The third option is to user JupyterHub. To do so direct your browser to https://wheeler.alliance.unm.edu:8000 and log in with your CARC credentials. Click on the \"New\" drop down menu and select \"R\". Now you have a R session running on Wheeler through JupyterHub.","title":"Running R on HPC systems (well, on CARC systems)"},{"location":"R_at_CARC/getting_R_software/#running-r-on-hpc-systems-well-on-carc-systems","text":"","title":"Running R on HPC systems (well, on CARC systems)"},{"location":"R_at_CARC/getting_R_software/#getting-r-in-the-first-place","text":"There are three options for accessing R at Carc and I will run through both approaches since there are pros and cons to each.","title":"Getting R in the first place"},{"location":"R_at_CARC/getting_R_software/#option-1","text":"The first option is to activate an installed R module. When logged in to a CARC system you can use the module avail command to see which R versions are available. If you have a CARC account open a terminal and log in to follow along. yourusername@wheeler-sn$ module avail r- which will print out the following (although I have truncated the output): ----------------------- /opt/spack/share/spack/modules/linux-centos7-x86_64 ------------------------ ... r-3.4.1-gcc-4.8.5-python2-gzeg24m r-3.4.1-gcc-4.8.5-python2-zpkgqap r-3.4.1-intel-17.0.4-mkl-python2-67zsm3b r-3.4.1-intel-17.0.4-mkl-python2-gygkoab r-3.4.2-intel-18.0.2-python2-xsxuxwx r-3.4.3-gcc-4.8.5-python2-gk66fni r-3.4.3-gcc-4.8.5-python2-qv6gwz6 r-3.4.3-gcc-6.1.0-python2-lyqiytq r-3.4.3-gcc-7.3.0-python2-zhxbajj r-3.4.3-intel-18.0.1-python2-3l4dkgz r-3.4.3-intel-18.0.1-python2-lr24ix6 r-3.4.3-intel-18.0.2-python2-q3covk7 r-3.5.0-gcc-4.8.5-python2-khqxja7 r-3.5.0-gcc-7.3.0-python2-rvq3qk5 r-3.5.0-intel-18.0.2-python2-mkl-r6lx6yy r-3.5.3-gcc-7.3.0-python2-ziiolp5 r-3.6.0-gcc-4.8.5-python2-i4uimtp r-3.6.0-gcc-7.3.0-python2-7akol5t ... Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\". These are all of the currently available R modules installed on Wheeler. In order to activate a R software module you use the module load command. For example: yourusername@wheeler-sn$ module load r-3.6.0-gcc-7.3.0-python2-7akol5t yourusername@wheeler-sn$ R R version 3.6.0 (2019-04-26) -- \"Planting of a Tree\" Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > Will load R-3.6.0 that has been compiled with GCC-7.3.0. Normally you will be running R jobs in batch mode as oppposed to interactively, which means you will have the module load command in your PBS script, but we will get to that later.","title":"Option 1"},{"location":"R_at_CARC/getting_R_software/#option-2","text":"The second option is to create a custom local Anaconda environment with the version of R that would like to run. In order to do this you need to first load an Anaconda software module and then create a new environment according to your specifications. For example, the following commands will create an Anaconda environment with R-3.4.3: yourusername@wheeler-sn$ module load anaconda3 yourusername@wheeler-sn$ conda create --yes --name my_r_env r=3.4.3 Solving environment: done ## Package Plan ## environment location: /users/yourusername/.conda/envs/my_r_env added / updated specs: - r=3.4.3 The following packages will be downloaded: package | build ---------------------------|----------------- gfortran_linux-64-7.3.0 | h553295d_8 148 KB r-cluster-2.0.6 | r343h4829c52_0 519 KB r-lattice-0.20_35 | r343h086d26f_0 713 KB r-base-3.4.3 | h9bb98a2_5 38.3 MB r-recommended-3.4.3 | r343_0 3 KB r-3.4.3 | r343_0 3 KB r-mgcv-1.8_22 | r343h086d26f_0 2.4 MB r-matrix-1.2_12 | r343h086d26f_0 2.5 MB r-kernsmooth-2.23_15 | r343h4829c52_4 101 KB r-class-7.3_14 | r343h086d26f_4 93 KB r-nlme-3.1_131 | r343h4829c52_0 2.2 MB r-foreign-0.8_69 | r343h086d26f_0 256 KB r-survival-2.41_3 | r343h086d26f_0 5.1 MB gcc_linux-64-7.3.0 | h553295d_8 149 KB r-spatial-7.3_11 | r343h086d26f_4 140 KB gcc_impl_linux-64-7.3.0 | habb00fd_1 73.2 MB gxx_impl_linux-64-7.3.0 | hdf63c60_1 18.7 MB gxx_linux-64-7.3.0 | h553295d_8 148 KB r-boot-1.3_20 | r343h889e2dd_0 625 KB binutils_linux-64-2.31.1 | h6176602_8 148 KB r-mass-7.3_48 | r343h086d26f_0 1.1 MB bzip2-1.0.8 | h7b6447c_0 105 KB ca-certificates-2019.5.15 | 1 134 KB r-codetools-0.2_15 | r343h889e2dd_0 49 KB r-rpart-4.1_11 | r343h086d26f_0 899 KB r-nnet-7.3_12 | r343h086d26f_0 118 KB libxml2-2.9.9 | hea5a465_1 2.0 MB ------------------------------------------------------------ Total: 149.8 MB The following NEW packages will be INSTALLED: _libgcc_mutex: 0.1-main _r-mutex: 1.0.0-anacondar_1 binutils_impl_linux-64: 2.31.1-h6176602_1 binutils_linux-64: 2.31.1-h6176602_8 bwidget: 1.9.11-1 bzip2: 1.0.8-h7b6447c_0 ca-certificates: 2019.5.15-1 cairo: 1.14.12-h8948797_3 curl: 7.65.2-hbc83047_0 fontconfig: 2.13.0-h9420a91_0 freetype: 2.9.1-h8a8886c_1 fribidi: 1.0.5-h7b6447c_0 gcc_impl_linux-64: 7.3.0-habb00fd_1 gcc_linux-64: 7.3.0-h553295d_8 gfortran_impl_linux-64: 7.3.0-hdf63c60_1 gfortran_linux-64: 7.3.0-h553295d_8 glib: 2.56.2-hd408876_0 graphite2: 1.3.13-h23475e2_0 gxx_impl_linux-64: 7.3.0-hdf63c60_1 gxx_linux-64: 7.3.0-h553295d_8 harfbuzz: 1.8.8-hffaf4a1_0 icu: 58.2-h9c2bf20_1 jpeg: 9b-h024ee3a_2 krb5: 1.16.1-h173b8e3_7 libcurl: 7.65.2-h20c2e04_0 libedit: 3.1.20181209-hc058e9b_0 libffi: 3.2.1-hd88cf55_4 libgcc-ng: 9.1.0-hdf63c60_0 libgfortran-ng: 7.3.0-hdf63c60_0 libpng: 1.6.37-hbc83047_0 libssh2: 1.8.2-h1ba5d50_0 libstdcxx-ng: 9.1.0-hdf63c60_0 libtiff: 4.0.10-h2733197_2 libuuid: 1.0.3-h1bed415_2 libxcb: 1.13-h1bed415_1 libxml2: 2.9.9-hea5a465_1 ncurses: 6.1-he6710b0_1 openssl: 1.1.1c-h7b6447c_1 pango: 1.42.4-h049681c_0 pcre: 8.43-he6710b0_0 pixman: 0.38.0-h7b6447c_0 r: 3.4.3-r343_0 r-base: 3.4.3-h9bb98a2_5 r-boot: 1.3_20-r343h889e2dd_0 r-class: 7.3_14-r343h086d26f_4 r-cluster: 2.0.6-r343h4829c52_0 r-codetools: 0.2_15-r343h889e2dd_0 r-foreign: 0.8_69-r343h086d26f_0 r-kernsmooth: 2.23_15-r343h4829c52_4 r-lattice: 0.20_35-r343h086d26f_0 r-mass: 7.3_48-r343h086d26f_0 r-matrix: 1.2_12-r343h086d26f_0 r-mgcv: 1.8_22-r343h086d26f_0 r-nlme: 3.1_131-r343h4829c52_0 r-nnet: 7.3_12-r343h086d26f_0 r-recommended: 3.4.3-r343_0 r-rpart: 4.1_11-r343h086d26f_0 r-spatial: 7.3_11-r343h086d26f_4 r-survival: 2.41_3-r343h086d26f_0 readline: 7.0-h7b6447c_5 tk: 8.6.8-hbc83047_0 tktable: 2.10-h14c3975_0 xz: 5.2.4-h14c3975_4 zlib: 1.2.11-h7b6447c_3 zstd: 1.3.7-h0b5b093_0 Downloading and Extracting Packages gfortran_linux-64-7. | 148 KB | ########################################################### | 100% r-cluster-2.0.6 | 519 KB | ########################################################### | 100% r-lattice-0.20_35 | 713 KB | ########################################################### | 100% r-base-3.4.3 | 38.3 MB | ########################################################### | 100% r-recommended-3.4.3 | 3 KB | ########################################################### | 100% r-3.4.3 | 3 KB | ########################################################### | 100% r-mgcv-1.8_22 | 2.4 MB | ########################################################### | 100% r-matrix-1.2_12 | 2.5 MB | ########################################################### | 100% r-kernsmooth-2.23_15 | 101 KB | ########################################################### | 100% r-class-7.3_14 | 93 KB | ########################################################### | 100% r-nlme-3.1_131 | 2.2 MB | ########################################################### | 100% r-foreign-0.8_69 | 256 KB | ########################################################### | 100% r-survival-2.41_3 | 5.1 MB | ########################################################### | 100% gcc_linux-64-7.3.0 | 149 KB | ########################################################### | 100% r-spatial-7.3_11 | 140 KB | ########################################################### | 100% gcc_impl_linux-64-7. | 73.2 MB | ########################################################### | 100% gxx_impl_linux-64-7. | 18.7 MB | ########################################################### | 100% gxx_linux-64-7.3.0 | 148 KB | ########################################################### | 100% r-boot-1.3_20 | 625 KB | ########################################################### | 100% binutils_linux-64-2. | 148 KB | ########################################################### | 100% r-mass-7.3_48 | 1.1 MB | ########################################################### | 100% bzip2-1.0.8 | 105 KB | ########################################################### | 100% ca-certificates-2019 | 134 KB | ########################################################### | 100% r-codetools-0.2_15 | 49 KB | ########################################################### | 100% r-rpart-4.1_11 | 899 KB | ########################################################### | 100% r-nnet-7.3_12 | 118 KB | ########################################################### | 100% libxml2-2.9.9 | 2.0 MB | ########################################################### | 100% Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use: # > source activate my_r_env # # To deactivate an active environment, use: # > source deactivate # Then to use your newly created R environment you need to 1) make sure you have the Anaconda software module loaded, and 2), activate your conda envioronment. yourusername@wheeler-sn$ module load anaconda3 yourusername@wheeler-sn$ source activate my_r_env yourusername@wheeler-sn$ R R version 3.4.3 (2017-11-30) -- \"Kite-Eating Tree\" Copyright (C) 2017 The R Foundation for Statistical Computing Platform: x86_64-conda_cos6-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. >","title":"Option 2"},{"location":"R_at_CARC/getting_R_software/#option-3","text":"The third option is to user JupyterHub. To do so direct your browser to https://wheeler.alliance.unm.edu:8000 and log in with your CARC credentials. Click on the \"New\" drop down menu and select \"R\". Now you have a R session running on Wheeler through JupyterHub.","title":"Option 3"},{"location":"R_at_CARC/installing_packages/","text":"Installing R packages Installing interactively This is the way that most of us are probably comfortable with installing packages with R since it is exactly the same way you install packages on your laptop. However, because the head node of CARC systems is a shared resource it is best practice to not compile binaries for R for an extended period of time because it can result in overhead on the head node. To avoid this we can request a compute node for interactive use, or even better, one of our debug nodes. The actual call to Torque, our job scheduler, will be explained in more depth later, but for now to request an interactive node type the following at the command prompt on Wheeler: yourusername@wheeler-sn$ qsub -I -l walltime=01:00:00 -l nodes=1:ppn=8 qsub: waiting for job 201775.wheeler-sn.alliance.unm.edu to start qsub: job 201775.wheeler-sn.alliance.unm.edu ready Wheeler Portable Batch System Prologue Job Id: 201775.wheeler-sn.alliance.unm.edu Username: liphardt Job 201775.wheeler-sn.alliance.unm.edu running on nodes: wheeler272 prologue running on host: wheeler272 Which will request a compute node and log you in once it is ready. Load a R software module with your preferred method and start a R session. If this is your first time installing a R package for one of the major versions you will be prompted to use a personal library. R version 3.6.0 (2019-04-26) -- \"Planting of a Tree\" Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > install.packages(\"ape\", dependencies=T, repos=\"http://cran.r-project.org\") Warning in install.packages(\"ape\", dependencies = T, repos = \"http://cran.r-project.org\", : 'lib = \"/opt/spack/opt/spack/linux-centos7-x86_64/gcc-7.3.0/r-3.6.0-7akol5ts7ebhhvoqz2tf6ghmc32hng7g/rlib/R/library\"' is not writable Would you like to use a personal library instead? (yes/No/cancel) yes Would you like to create a personal library '~/R/x86_64-pc-linux-gnu-library/3.6' to install packages into? (yes/No/cancel) yes Go ahead and say yes to both questions and install packages normally. You only need to specify a personal library the first time you use a new major version. Installing packages using a script Installing packages in a script is a little more complicated so it might be easier to just do it interactively. However, it is possible. If you haven't created a personal library yet either interactively or through a script you first need to do that. The following commands in an R script will take care of this for you: # First create the directory, .libPaths() will not append your library list unless the directory exists. dir.create(Sys.getenv(\"R_LIBS_USER\", recursive=T, mode=\"0777\")) #Now append your library path with your newly created local library .libPaths(c(Sys.getenv(\"R_LIBS_USER\"), .libPaths())) #The above steps are only necessary the first time you are installing packages. Remove or comment out if you have already created a persional library. #Now install packages normally > install.packages(\"ape\", dependencies=T, lib=Sys.getenv(\"R_LIBS_USER\"), repos=\"http://cran.r-project.org\") The first two lines are only necessary when you have not created a personal library for that major version of R yet, otherwise you just need to specify the repos you are downloading packages from and specify your personal library as the install location. You shouldn't need to specify the library since you have appended your library path, but it doesn't hurt to be explicit.","title":"Installing R packages"},{"location":"R_at_CARC/installing_packages/#installing-r-packages","text":"","title":"Installing R packages"},{"location":"R_at_CARC/installing_packages/#installing-interactively","text":"This is the way that most of us are probably comfortable with installing packages with R since it is exactly the same way you install packages on your laptop. However, because the head node of CARC systems is a shared resource it is best practice to not compile binaries for R for an extended period of time because it can result in overhead on the head node. To avoid this we can request a compute node for interactive use, or even better, one of our debug nodes. The actual call to Torque, our job scheduler, will be explained in more depth later, but for now to request an interactive node type the following at the command prompt on Wheeler: yourusername@wheeler-sn$ qsub -I -l walltime=01:00:00 -l nodes=1:ppn=8 qsub: waiting for job 201775.wheeler-sn.alliance.unm.edu to start qsub: job 201775.wheeler-sn.alliance.unm.edu ready Wheeler Portable Batch System Prologue Job Id: 201775.wheeler-sn.alliance.unm.edu Username: liphardt Job 201775.wheeler-sn.alliance.unm.edu running on nodes: wheeler272 prologue running on host: wheeler272 Which will request a compute node and log you in once it is ready. Load a R software module with your preferred method and start a R session. If this is your first time installing a R package for one of the major versions you will be prompted to use a personal library. R version 3.6.0 (2019-04-26) -- \"Planting of a Tree\" Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type 'license()' or 'licence()' for distribution details. R is a collaborative project with many contributors. Type 'contributors()' for more information and 'citation()' on how to cite R or R packages in publications. Type 'demo()' for some demos, 'help()' for on-line help, or 'help.start()' for an HTML browser interface to help. Type 'q()' to quit R. > install.packages(\"ape\", dependencies=T, repos=\"http://cran.r-project.org\") Warning in install.packages(\"ape\", dependencies = T, repos = \"http://cran.r-project.org\", : 'lib = \"/opt/spack/opt/spack/linux-centos7-x86_64/gcc-7.3.0/r-3.6.0-7akol5ts7ebhhvoqz2tf6ghmc32hng7g/rlib/R/library\"' is not writable Would you like to use a personal library instead? (yes/No/cancel) yes Would you like to create a personal library '~/R/x86_64-pc-linux-gnu-library/3.6' to install packages into? (yes/No/cancel) yes Go ahead and say yes to both questions and install packages normally. You only need to specify a personal library the first time you use a new major version.","title":"Installing interactively"},{"location":"R_at_CARC/installing_packages/#installing-packages-using-a-script","text":"Installing packages in a script is a little more complicated so it might be easier to just do it interactively. However, it is possible. If you haven't created a personal library yet either interactively or through a script you first need to do that. The following commands in an R script will take care of this for you: # First create the directory, .libPaths() will not append your library list unless the directory exists. dir.create(Sys.getenv(\"R_LIBS_USER\", recursive=T, mode=\"0777\")) #Now append your library path with your newly created local library .libPaths(c(Sys.getenv(\"R_LIBS_USER\"), .libPaths())) #The above steps are only necessary the first time you are installing packages. Remove or comment out if you have already created a persional library. #Now install packages normally > install.packages(\"ape\", dependencies=T, lib=Sys.getenv(\"R_LIBS_USER\"), repos=\"http://cran.r-project.org\") The first two lines are only necessary when you have not created a personal library for that major version of R yet, otherwise you just need to specify the repos you are downloading packages from and specify your personal library as the install location. You shouldn't need to specify the library since you have appended your library path, but it doesn't hurt to be explicit.","title":"Installing packages using a script"}]}