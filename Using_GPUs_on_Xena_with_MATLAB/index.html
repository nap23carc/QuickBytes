<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Using GPUs with MATLAB - CARC QuickBytes</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Using GPUs with MATLAB";
        var mkdocs_page_input_path = "Using_GPUs_on_Xena_with_MATLAB.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> CARC QuickBytes
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../workshop_slides/">Workshop Slides</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Linux & HPC Introduction</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../linux_intro/">Linux Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../logging_in/">Logging in</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ssh_keygen_config/">SSH keys and Config file</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../transfer_data/">Transferring data</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../Intro_to_slurm/">Intro to Slurm</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../pbs2slurm/">Converting PBS to Slurm</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../checking_on_running_jobs/">Check running jobs</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../module_management/">Managing modules</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../slurm_accounting/">Intro to Slurm accounting at CARC</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../GNU_Parallel/">GNU Parallel</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../X11_forwarding/">X11 Forwarding</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Applications Tutorials</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" >MATLAB</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../running_matlab_jobs/">Running MATLAB jobs at CARC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../ParallelMatlabServer/">Parallel MATLAB Server</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Parallel_MATLAB_profile_setup_and_batch_submission/">Parallel MATLAB batch submission</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="#">Using GPUs with MATLAB</a>
    <ul class="current">
    <li class="toctree-l3"><a class="reference internal" href="#using-a-single-gpu-on-xena">Using a single GPU on Xena</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#use-gpu-in-interactive-session">Use GPU in Interactive Session</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#schedule-a-job">Schedule a job</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-multiple-gpus-on-a-single-xena-node">Using Multiple GPUs on a single Xena node</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#matlab-script_1">MATLAB Script</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#slurm-script_1">Slurm Script</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#submit-job-to-queue_1">Submit Job to Queue</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-multiple-nodes-with-their-own-gpus">Using Multiple Nodes with their own GPUs</a>
    </li>
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../MATLAB_Deep_Learning_on_Xena/">MATLAB Deep Learning on Xena</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >JupyterHub</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../parallelization_with_Jupyterhub_using_mpi/">JupyterHub Parallel Processing with MPI</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Conda_JupyterHub/">Conda python environments for JupyterHub</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../julia_with_jupyterhub/">Using Julia in JupyterHub</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Conda</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../anaconda_general_intro/">General intro to Conda</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../anaconda_intro/">Intro to Conda with example</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../anaconda_pip_channels/">Anaconda pip channels</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >R</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../R_usage/">R Programming in HPC</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://github.com/UNM-CARC/QuickBytes/tree/master/R_at_CARC">R at CARC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Parallel_R_with_Future.ipynb">Running R in Parallel with Future</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Gurobi_optimizer_with_R/">Gurobi optimizer with R</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Machine Learning</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../Tensorflow_documentation/">Tensorflow</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../PyTorch_1.9_Xena/">Installing PyTorch on Xena</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../PyTorch_Classifier_Xena.ipynb">Example PyTorch Image Classification on Xena</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../multiGPU_tensorflow_tutorial.ipynb">Tensorflow with multiple GPUs</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../parallel_jupyterhub_with_dask_and_scikit-learn/">Parallelization with JupyterHub using Dask and SciKit-learn</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Bioinformatics</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../GATK_QuickByte/">Genomic variant calling with GATK</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../genome_evaluation/">Genome evaluation with QUAST and BUSCO</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../psmc_quickbyte/">Single genome demographic history with PSMC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Stacks_quickbyte/">Stacks for RAD-Seq Data</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Metabarcoding/">Metabarcoding with QIIME2, Mothur, and USEARCH</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Beast_at_CARC/">BEAST at CARC</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../msprime_quickbyte/">Population genetic simulations with msprime (backwards time</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Computational Chemistry</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../orca_wheeler_taos/">Orca on Wheeler and Taos</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../alphafold/">Alphafold</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Computational Immunology</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../SimCov/">SimCov on Wheeler</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Astronomy</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../mpiCASA/">CASA Radio Astronomy</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Paraview</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../paraview/">Paraview Wheeler</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../Paraview_Hopper/">Paraview Hopper</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../singularity-markdown-version/">Docker and Singularity</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../haskell/">Haskell at CARC</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../spark/">Spark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../install_perl_libraries/">Installing Perl Libraries to Your Home Directory</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Systems</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../resource_limits/">Resource Limits</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../export_control/">Export Control</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >JupyterHub cluster links</a>
    <ul>
                <li class="toctree-l2"><a class="" href="https://hopper.alliance.unm.edu:8000/">Hopper</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://xena.alliance.unm.edu:8000/">Xena</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://wheeler.alliance.unm.edu:8000/">Wheeler</a>
                </li>
                <li class="toctree-l2"><a class="" href="https://taos.alliance.unm.edu:8000/">Taos</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">CARC QuickBytes</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Applications Tutorials</li>
          <li class="breadcrumb-item">MATLAB</li>
      <li class="breadcrumb-item active">Using GPUs with MATLAB</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="using-gpus-with-matlab">Using GPUs with MATLAB</h1>
<ol>
<li><a href="#1">Using a single GPU on Xena</a><ol>
<li><a href="#1.1">Use GPU in Interactive Session</a><ol>
<li><a href="#1.1.1">Identify and Select GPU</a></li>
<li><a href="#1.1.2">Using Arrays on GPU</a></li>
<li><a href="#1.1.3">Initialize Array</a></li>
<li><a href="#1.1.4">Test if array is on GPU</a></li>
<li><a href="#1.1.5">Retrieve Array from GPU</a></li>
<li><a href="#1.1.6">Use functions on GPU Arrays</a></li>
</ol>
</li>
<li><a href="#1.2">Schedule a job</a><ol>
<li><a href="#1.2.1">MATLAB Script</a></li>
<li><a href="#1.2.2">PBS Script</a></li>
<li><a href="#1.2.3">Slurm Script</a></li>
<li><a href="#1.2.4">Submit Job to Queue</a></li>
</ol>
</li>
</ol>
</li>
<li><a href="#2">Using Multiple GPUs on a single Xena node</a><ol>
<li><a href="#2.2">MATLAB Script</a></li>
<li><a href="#2.2">Slurm Script</a></li>
<li><a href="#2.3">Submit Job to Queue</a> </li>
</ol>
</li>
<li><a href="#3">Using Multiple Nodes with their own GPUs</a></li>
</ol>
<h2 id="using-a-single-gpu-on-xena">Using a single GPU on Xena <a name="1"></a></h2>
<p>MATLAB allows the utilization of a single GPU that is part of a machine.
The following sections show how to access and utilize a GPU on xena.</p>
<h3 id="use-gpu-in-interactive-session">Use GPU in Interactive Session <a name="1.1"></a></h3>
<p>First, we will open MATLAB in an interactive session on a xena compute node.</p>
<h4 id="identify-and-select-gpu">Identify and Select GPU <a name="1.1.1"></a></h4>
<p>Start by requesting an interactive session:</p>
<pre><code class="language-bash">xena:~$ srun -G 1 --pty bash
</code></pre>
<p>Once you have a node allocated to you, load the MATLAB module and start a MATLAB session:</p>
<pre><code class="language-bash">xena01:~$ module load matlab
xena01:~$ matlab

To get started, type doc.
For product information, visit www.mathworks.com.
&gt;&gt;
</code></pre>
<p>Now you can check to see the number of GPUs available:</p>
<pre><code class="language-bash">&gt;&gt; gpuDeviceCount(&quot;available&quot;)
</code></pre>
<p>You should see the following:</p>
<pre><code class="language-bash">ans = 
     1
</code></pre>
<p>This means that you have access to a single GPU.</p>
<p>To get information about the available gpus, use this function:</p>
<pre><code class="language-bash">&gt;&gt; gpuDeviceTable
</code></pre>
<p>That will print something that looks like this on Xena:</p>
<pre><code class="language-bash">ans =

  1x5 table

    Index        Name        ComputeCapability    DeviceAvailable    DeviceSelected
    _____    ____________    _________________    _______________    ______________

      1      &quot;Tesla K40m&quot;          &quot;3.5&quot;               true              false
</code></pre>
<p>Next, you can tell MATLAB which GPU to use (pass in the desired index from the above table).
If you do not do this, MATLAB will automatically grab the lowest index GPU when you try to use one.</p>
<pre><code class="language-bash">&gt;&gt; gpuDevice(1)
</code></pre>
<p>Running the <code>gpuDeviceTable</code> command again shows this change:</p>
<pre><code class="language-bash">&gt;&gt; gpuDeviceTable

ans =

  1x5 table

    Index        Name        ComputeCapability    DeviceAvailable    DeviceSelected
    _____    ____________    _________________    _______________    ______________

      1      &quot;Tesla K40m&quot;          &quot;3.5&quot;               true              true

</code></pre>
<h4 id="using-arrays-on-gpu">Using Arrays on GPU <a name="1.1.2"></a></h4>
<p>In order to utilize the GPU, data must be loaded into a <code>gpuArray</code> object.
For a full description of the <code>gpuArray</code> object, please visit the official MathWorks Documentation at <a href="https://www.mathworks.com/help/parallel-computing/gpuarray.html">https://www.mathworks.com/help/parallel-computing/gpuarray.html</a></p>
<h5 id="initialize-array">Initialize Array <a name="1.1.3"></a></h5>
<p>First, create a normal array using any method you like.
In this example we will use the <code>magic(8)</code> function to create a magic square matrix that is 8x8.</p>
<pre><code class="language-bash">&gt;&gt; A = magic(8)
</code></pre>
<p>Next, pass that into a <code>gpuArray</code> object.
This will copy the contents of a normal array into an array on the GPU.</p>
<pre><code class="language-bash">&gt;&gt; B = gpuArray(A)
</code></pre>
<h5 id="test-if-array-is-on-gpu">Test if array is on GPU <a name="1.1.4"></a></h5>
<p>The <code>isgpuarray</code> function tests if an array is on a GPU:</p>
<pre><code class="language-bash">&gt;&gt; isgpuarray(A)

ans =

  logical

   0

&gt;&gt; isgpuarray(B)

ans =

  logical

   1
</code></pre>
<p>This confirms that array A is not on the GPU, but array B is.</p>
<h5 id="retrieve-array-from-gpu">Retrieve Array from GPU <a name="1.1.5"></a></h5>
<p>In order to retrieve an array from the GPU and put it back in the MATLAB workspace, use the <code>gather</code> function.
It will copy the contents of an array on the GPU into a normal array.
This is neccesary if you want to to perform non-GPU actions on your data after using the GPU.</p>
<pre><code class="language-bash">&gt;&gt; C = gather(B)
</code></pre>
<p>Now, we can test to see if C is stored on the gpu:</p>
<pre><code class="language-bash">&gt;&gt; isgpuarray(C)

ans =

  logical

   0
</code></pre>
<h4 id="use-functions-on-gpu-arrays">Use functions on GPU Arrays <a name="1.1.6"></a></h4>
<p>To perform functions on <code>gpuArray</code> objects, use the <code>arrayfun</code> function.
In this example, we will apply the MATLAB <code>sqrt</code> function to the array (B) that we created in the previous step:</p>
<pre><code class="language-bash">result = arrayfun(@sqrt,B)
</code></pre>
<p>This will apply the <code>sqrt</code> function to every element in the GPU array.
<code>result</code> is also a GPU array:</p>
<pre><code class="language-bash">&gt;&gt; isgpuarray(result)

ans =

  logical

   1

</code></pre>
<p>To see a list of MATLAB functions that are supported using gpus, visit <a href="https://www.mathworks.com/help/parallel-computing/gpuarray.html">https://www.mathworks.com/help/parallel-computing/gpuarray.html</a></p>
<p>You can also create your own functions to pass into <code>arrayfun</code>.</p>
<h3 id="schedule-a-job">Schedule a job <a name="1.2"></a></h3>
<p>It is good idea to do everything using a batch script and avoid the mistakes associated with interactive computing.
To get an idea of why performing functions on <code>gpuArray</code> objects is a good idea, let's create a simple MATLAB script that displays the amount of time it takes to perform the same computation on a cpu and on a gpu.
We will then create a PBS script that schedules a job with a GPU to run the MATLAB script for us.</p>
<h4 id="matlab-script">MATLAB Script <a name="1.2.1"></a></h4>
<p>We will perform the <code>sqrt</code> function on a 5000x5000 array.
The use of <code>tic</code> and <code>toc</code> allow us to time the seperate applications of <code>sqrt</code>.</p>
<p>Create the following script with the name <code>gpu_matlab.m</code>:</p>
<pre><code class="language-bash">gpuDevice(1);

A = magic(5000);
disp(&quot;sqrt of 5000x5000 matrix on cpu:&quot;)
tic
B = arrayfun(@sqrt, A);
toc
disp(&quot;sqrt of 5000x5000 matrix on gpu:&quot;)
C = gpuArray(magic(5000));
tic
D = arrayfun(@sqrt,C);
toc
</code></pre>
<h4 id="pbs-script">PBS Script <a name="1.2.2"></a></h4>
<p>Now, let's create a PBS script called <code>gpu_matlab.pbs</code>. 
Replace the <code>&lt;DIR&gt;</code> with the path to the directory containing the MATLAB script created above. 
This script will request the desired resrouces, load the MATLAB module, then run the script.
The output of the script will be sent to the file: <code>gpu_matlab.out</code></p>
<pre><code class="language-bash">#!/bin/bash

#PBS -N gpu_test
#PBS -l walltime=00:05:00
#PBS -l nodes=1:ppn=1:gpus=1
#PBS -j oe

cd &lt;DIR&gt;

module load matlab

matlab -nodisplay -r gpu_matlab &gt; gpu_matlab.out

</code></pre>
<h4 id="slurm-script">Slurm Script <a name="1.2.3"></a></h4>
<p>Now, let's create a Slurm script called <code>gpu_matlab.sh</code>. 
Replace the <code>&lt;DIR&gt;</code> with the path to the directory containing the MATLAB script created above. 
This script will request the desired resrouces, load the MATLAB module, then run the script.
The output of the script will be sent to the file: <code>gpu_matlab.out</code></p>
<pre><code class="language-bash">#!/bin/bash

#SBATCH --job-name gpu_matlab_job
#SBATCH --output gpu_matlab_job.out
#SBATCH --error gpu_matlab_job.err
#SBATCH --time 00:05:00
#SBATCH --ntasks 1
#SBATCH -G 1

cd &lt;DIR&gt;

module load matlab
matlab -nodisplay -r gpu_matlab &gt; gpu_matlab.out
</code></pre>
<h4 id="submit-job-to-queue">Submit Job to Queue <a name="1.2.4"></a></h4>
<p>Now we can submit the job to the scheduler from the xena head node:</p>
<p>PBS script version:</p>
<pre><code class="language-bash">xena:~$ qsub gpu_matlab.pbs
</code></pre>
<p>Slurm script version:</p>
<pre><code class="language-bash">xena:~$ sbatch gpu_matlab.sh
</code></pre>
<p>View the results:</p>
<pre><code class="language-bash">xena:~$ cat gpu_matlab.out
</code></pre>
<h2 id="using-multiple-gpus-on-a-single-xena-node">Using Multiple GPUs on a single Xena node <a name="2"></a></h2>
<p>Xena contains some nodes with two GPUs.
MATLAB allows for the utilization of multiple GPUs on a single node in the same way you use multiple CPUs.
To show how this works, below is an example MATLAB script that will create a logistic map using all available GPU's on the assigned node.</p>
<p>The <code>parpool</code> object is used to create workers to parallelize the execution.
Each worker will grab it's own GPU when performing actions with <code>gpuArray</code> objects.
For this to work properly, ensure that you have been allocated an equal number of CPUs as GPUs on the machine.
An example slurm script is included below to give an idea of how to ask for the proper resources to be allocated.</p>
<h3 id="matlab-script_1">MATLAB Script <a name="2.1"></a></h3>
<p>Create the following MATLAB script called <code>gpu_logistic_map.m</code>
This simple MATLAB script creates a Logistic Map by iterating the logistic equation on a set of random populations.
A worker is created for each available GPU.
They will then split up the work performed in the <code>parfor</code> loop.
The result is a logistic map figure saved as 'logistic_map.jpg'
It also contains calls to time the execution of the <code>parfor</code> loop.</p>
<pre><code class="language-bash">
N = 1000;
r = gpuArray.linspace(0,4,N);

numIterations = 1000;

numGPUs = gpuDeviceCount(&quot;available&quot;);
parpool(numGPUs);

numSimulations = 100;
X = zeros(numSimulations,N,'gpuArray');

disp(&quot;Timing execution of parfor loop:&quot;)

tic
parfor i=1:numSimulations
  X(i,:) = rand(1,N,'gpuArray')
  for n=1:numIterations
    X(i,:) = r.*X(i,:).*(1-X(i,:));
  end
end
toc

f = figure('visible','off');
plot(r,X,'.');
saveas(f,'logistic_map','jpg')

return
</code></pre>
<h3 id="slurm-script_1">Slurm Script <a name="2.2"></a></h3>
<p>When using the <code>--partition dualGPU</code> flag on xena, you must also set <code>--cpus-per-task 2</code> and <code>-G 2</code> for MATLAB to correctly find and utilize the available GPUs.
These numbers should match, as MATLAB will use a CPU to access each GPU.
For this partition, we ask for two CPUs and two GPUs.</p>
<p>Create the following slurm scrpt called <code>gpu_logistic_map.sh</code>.
Replace the <code>&lt;DIR&gt;</code> with the path to the directory containing the MATLAB script created above. 
This script will ask the scheduler for the proper resources.
Once the resrouces are allocated, the script will run the MATLAB script from the above step.
The MATLAB script will create a .jpg image once it has finished.
Any output of the MATLAB script is redirected to <code>gpu_logistic_map.out</code>.</p>
<pre><code class="language-bash">#!/bin/bash

#SBATCH --job-name gpu_logistic_map_job
#SBATCH --output gpu_logistic_map_job.out
#SBATCH --error gpu_logistic_map_job.err
#SBATCH --time 00:10:00
#SBATCH --partition dualGPU
#SBATCH --ntasks 1
#SBATCH --cpus-per-task 2
#SBATCH -G 2

cd &lt;DIR&gt;

module load matlab
matlab -nodisplay -r gpu_logistic_map &gt; gpu_logistic_map.out

</code></pre>
<h3 id="submit-job-to-queue_1">Submit Job to Queue <a name="2.3"></a></h3>
<p>Now we can submit the job to the scheduler from the xena head node:</p>
<pre><code class="language-bash">xena:~$ sbatch gpu_logistic_map.sh
</code></pre>
<p>View the results:</p>
<pre><code class="language-bash">xena:~$ cat gpu_logistic_map.out
</code></pre>
<p>You can also view the <code>logistic_map.jpg</code> image using your preferred method.</p>
<h2 id="using-multiple-nodes-with-their-own-gpus">Using Multiple Nodes with their own GPUs <a name="3"></a></h2>
<p>Coming Soon! (Maybe)</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../Parallel_MATLAB_profile_setup_and_batch_submission/" class="btn btn-neutral float-left" title="Parallel MATLAB batch submission"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../MATLAB_Deep_Learning_on_Xena/" class="btn btn-neutral float-right" title="MATLAB Deep Learning on Xena">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../Parallel_MATLAB_profile_setup_and_batch_submission/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../MATLAB_Deep_Learning_on_Xena/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
